{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7935ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Environment Variables:\n",
      "DATABASE_URL: None\n",
      "API_KEY: None\n",
      "\n",
      "âœ… Available Matplotlib styles: ['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'petroff10', 'seaborn-v0_8', 'seaborn-v0_8-bright', 'seaborn-v0_8-colorblind', 'seaborn-v0_8-dark', 'seaborn-v0_8-dark-palette', 'seaborn-v0_8-darkgrid', 'seaborn-v0_8-deep', 'seaborn-v0_8-muted', 'seaborn-v0_8-notebook', 'seaborn-v0_8-paper', 'seaborn-v0_8-pastel', 'seaborn-v0_8-poster', 'seaborn-v0_8-talk', 'seaborn-v0_8-ticks', 'seaborn-v0_8-white', 'seaborn-v0_8-whitegrid', 'tableau-colorblind10']\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "'seaborn-white' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\matplotlib\\style\\core.py:129\u001b[0m, in \u001b[0;36muse\u001b[1;34m(style)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m     style \u001b[38;5;241m=\u001b[39m \u001b[43m_rc_params_in_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\matplotlib\\__init__.py:903\u001b[0m, in \u001b[0;36m_rc_params_in_file\u001b[1;34m(fname, transform, fail_on_error)\u001b[0m\n\u001b[0;32m    902\u001b[0m rc_temp \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 903\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_or_url(fname) \u001b[38;5;28;01mas\u001b[39;00m fd:\n\u001b[0;32m    904\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\matplotlib\\__init__.py:880\u001b[0m, in \u001b[0;36m_open_file_or_url\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m    879\u001b[0m fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(fname)\n\u001b[1;32m--> 880\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'seaborn-white'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Set styling for visualizations\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ… Available Matplotlib styles:\u001b[39m\u001b[38;5;124m\"\u001b[39m, plt\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39mavailable)\n\u001b[1;32m---> 34\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstyle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseaborn-white\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Alternative: 'ggplot', 'classic'\u001b[39;00m\n\u001b[0;32m     35\u001b[0m sns\u001b[38;5;241m.\u001b[39mset_style(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhitegrid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m plt\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure.figsize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m8\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\matplotlib\\style\\core.py:131\u001b[0m, in \u001b[0;36muse\u001b[1;34m(style)\u001b[0m\n\u001b[0;32m    129\u001b[0m         style \u001b[38;5;241m=\u001b[39m _rc_params_in_file(style)\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 131\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    132\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstyle\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not a valid package style, path of style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile, URL of style file, or library style name (library \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyles are listed in `style.available`)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    135\u001b[0m filtered \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m style:  \u001b[38;5;66;03m# don't trigger RcParams.__getitem__('backend')\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: 'seaborn-white' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from scipy import stats\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import json\n",
    "import warnings\n",
    "import dotenv\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Set styling for visualizations\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\", color_codes=True)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "\n",
    "# Add project root to path to import custom modules\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "# Import PesaGuru custom modules\n",
    "try:\n",
    "    from utils.jupyter_helpers.data_loaders import load_test_data\n",
    "    from utils.jupyter_helpers.visualization import plot_ab_test_results\n",
    "    print(\"Custom PesaGuru modules imported successfully!\")\n",
    "except ImportError:\n",
    "    print(\"Warning: Custom modules not found. Using fallback implementations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b20a86",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Fallback implementations\n",
    "def load_test_data(source, start_date=None, end_date=None):\n",
    "        \"\"\"Load test data from various sources.\"\"\"\n",
    "        if source == 'firebase':\n",
    "            # Mock implementation\n",
    "            return pd.DataFrame({\n",
    "                'user_id': [f'user_{i}' for i in range(1000)],\n",
    "                'group': np.random.choice(['A', 'B'], size=1000),\n",
    "                'engagement_time': np.random.normal(120, 60, 1000),\n",
    "                'messages_sent': np.random.poisson(8, 1000),\n",
    "                'task_completed': np.random.choice([0, 1], size=1000, p=[0.3, 0.7]),\n",
    "                'satisfaction_rating': np.random.choice([1, 2, 3, 4, 5], size=1000),\n",
    "                'language': np.random.choice(['English', 'Swahili'], size=1000, p=[0.7, 0.3]),\n",
    "                'device_type': np.random.choice(['mobile', 'desktop', 'tablet'], size=1000, p=[0.6, 0.3, 0.1]),\n",
    "                'age_group': np.random.choice(['18-24', '25-34', '35-44', '45+'], size=1000),\n",
    "                'feature_used': np.random.choice(['investment_recommendation', 'loan_comparison', 'budget_planning', 'market_analysis'], size=1000)\n",
    "            })\n",
    "        elif source == 'csv':\n",
    "            # Mock implementation - in real scenario, would load from filepath\n",
    "            return pd.DataFrame({\n",
    "                'user_id': [f'user_{i}' for i in range(500)],\n",
    "                'group': np.random.choice(['A', 'B'], size=500),\n",
    "                'engagement_time': np.random.normal(120, 60, 500),\n",
    "                'messages_sent': np.random.poisson(8, 500),\n",
    "                'task_completed': np.random.choice([0, 1], size=500, p=[0.3, 0.7]),\n",
    "                'satisfaction_rating': np.random.choice([1, 2, 3, 4, 5], size=500),\n",
    "            })\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown data source: {source}\")\n",
    "    \n",
    "def plot_ab_test_results(df, metric, group_col='group', title=None):\n",
    "        \"\"\"Simple plotting function for A/B test results.\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=group_col, y=metric, data=df)\n",
    "        plt.title(title or f'A/B Test Results: {metric} by {group_col}')\n",
    "        plt.ylabel(metric)\n",
    "        plt.xlabel(group_col)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fba000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PesaGuru chatbot features we want to test\n",
    "ab_test_configs = [\n",
    "    {\n",
    "        'test_name': 'language_preference',\n",
    "        'description': 'Testing user engagement with English vs Swahili interface',\n",
    "        'variant_a': 'English only interface',\n",
    "        'variant_b': 'Bilingual interface with language toggle',\n",
    "        'primary_metric': 'engagement_time',\n",
    "        'secondary_metrics': ['messages_sent', 'task_completed', 'satisfaction_rating'],\n",
    "        'test_duration_days': 14,\n",
    "        'min_sample_size': 500\n",
    "    },\n",
    "    {\n",
    "        'test_name': 'recommendation_algorithm',\n",
    "        'description': 'Testing rule-based vs ML-based investment recommendations',\n",
    "        'variant_a': 'Rule-based recommendations',\n",
    "        'variant_b': 'ML-based personalized recommendations',\n",
    "        'primary_metric': 'conversion_rate',\n",
    "        'secondary_metrics': ['satisfaction_rating', 'recommendation_clicks'],\n",
    "        'test_duration_days': 21,\n",
    "        'min_sample_size': 800\n",
    "    },\n",
    "    {\n",
    "        'test_name': 'response_style',\n",
    "        'description': 'Testing formal vs conversational response styles',\n",
    "        'variant_a': 'Formal, professional responses',\n",
    "        'variant_b': 'Conversational, friendly responses',\n",
    "        'primary_metric': 'satisfaction_rating',\n",
    "        'secondary_metrics': ['engagement_time', 'messages_sent'],\n",
    "        'test_duration_days': 14,\n",
    "        'min_sample_size': 600\n",
    "    },\n",
    "    {\n",
    "        'test_name': 'ui_design',\n",
    "        'description': 'Testing different UI layouts for the chatbot',\n",
    "        'variant_a': 'Text-focused UI with minimal graphics',\n",
    "        'variant_b': 'Visual UI with charts and financial graphics',\n",
    "        'primary_metric': 'task_completed',\n",
    "        'secondary_metrics': ['time_to_completion', 'satisfaction_rating'],\n",
    "        'test_duration_days': 14,\n",
    "        'min_sample_size': 700\n",
    "    }\n",
    "]\n",
    "\n",
    "# Print the test configurations\n",
    "print(\"Defined A/B Test Configurations:\")\n",
    "for i, config in enumerate(ab_test_configs):\n",
    "    print(f\"\\nTest {i+1}: {config['test_name']}\")\n",
    "    print(f\"Description: {config['description']}\")\n",
    "    print(f\"Variant A: {config['variant_a']}\")\n",
    "    print(f\"Variant B: {config['variant_b']}\")\n",
    "    print(f\"Primary Metric: {config['primary_metric']}\")\n",
    "    print(f\"Test Duration: {config['test_duration_days']} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b2bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(baseline_conversion=0.10, minimum_detectable_effect=0.05, \n",
    "                          alpha=0.05, power=0.8):\n",
    "    \"\"\"\n",
    "    Calculate the required sample size for an A/B test\n",
    "    \n",
    "    Parameters:\n",
    "    - baseline_conversion: Expected conversion rate for control group\n",
    "    - minimum_detectable_effect: Smallest meaningful difference to detect\n",
    "    - alpha: Significance level (Type I error probability)\n",
    "    - power: 1 - Type II error probability\n",
    "    \n",
    "    Returns:\n",
    "    - sample_size_per_group: Required sample size per variant\n",
    "    \"\"\"\n",
    "    # Standard normal critical values for alpha and beta\n",
    "    z_alpha = stats.norm.ppf(1 - alpha/2)\n",
    "    z_beta = stats.norm.ppf(power)\n",
    "    \n",
    "    # Standard deviations under null and alternative\n",
    "    sd1 = np.sqrt(2 * baseline_conversion * (1 - baseline_conversion))\n",
    "    sd2 = np.sqrt(baseline_conversion * (1 - baseline_conversion) + \n",
    "                 (baseline_conversion + minimum_detectable_effect) * \n",
    "                 (1 - (baseline_conversion + minimum_detectable_effect)))\n",
    "    \n",
    "    # Calculate sample size\n",
    "    sample_size_per_group = ((z_alpha * sd1 + z_beta * sd2) / minimum_detectable_effect) ** 2\n",
    "    \n",
    "    return np.ceil(sample_size_per_group)\n",
    "\n",
    "# Calculate and display sample sizes for each test\n",
    "print(\"\\nSample Size Calculations:\")\n",
    "for config in ab_test_configs:\n",
    "    # Different baseline and MDE for different metrics\n",
    "    if config['primary_metric'] == 'conversion_rate':\n",
    "        baseline = 0.10\n",
    "        mde = 0.05\n",
    "    elif config['primary_metric'] == 'satisfaction_rating':\n",
    "        baseline = 0.40  # 40% give 4 or 5 stars\n",
    "        mde = 0.10\n",
    "    elif config['primary_metric'] == 'engagement_time':\n",
    "        baseline = 0.30  # 30% engage longer than 3 minutes\n",
    "        mde = 0.08\n",
    "    else:\n",
    "        baseline = 0.20\n",
    "        mde = 0.07\n",
    "    \n",
    "    sample_size = calculate_sample_size(baseline, mde)\n",
    "    print(f\"Test: {config['test_name']}\")\n",
    "    print(f\"Required sample size per group: {int(sample_size)}\")\n",
    "    print(f\"Minimum sample size set in config: {config['min_sample_size']}\")\n",
    "    \n",
    "    if sample_size > config['min_sample_size']:\n",
    "        print(f\"WARNING: Configured sample size may be too small for desired statistical power!\\n\")\n",
    "    else:\n",
    "        print(f\"Configured sample size is sufficient.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b63a9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_ab_test_data(config, n_users=1000, seed=42):\n",
    "    \"\"\"\n",
    "    Simulate data for an A/B test based on configuration\n",
    "    \n",
    "    Parameters:\n",
    "    - config: Dictionary with test configuration\n",
    "    - n_users: Number of users to simulate\n",
    "    - seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with simulated test data\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Create base dataframe\n",
    "    data = {\n",
    "        'user_id': [f'user_{i}' for i in range(n_users)],\n",
    "        'group': np.random.choice(['A', 'B'], size=n_users),\n",
    "        'test_name': config['test_name'],\n",
    "        'timestamp': [datetime.now() - timedelta(days=random.randint(0, config['test_duration_days'])) \n",
    "                      for _ in range(n_users)]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add demographic information\n",
    "    df['age_group'] = np.random.choice(['18-24', '25-34', '35-44', '45+'], size=n_users)\n",
    "    df['location'] = np.random.choice(['Nairobi', 'Mombasa', 'Kisumu', 'Nakuru', 'Other'], \n",
    "                                     size=n_users, p=[0.4, 0.2, 0.1, 0.1, 0.2])\n",
    "    df['device'] = np.random.choice(['mobile', 'desktop', 'tablet'], \n",
    "                                   size=n_users, p=[0.7, 0.2, 0.1])\n",
    "    \n",
    "    # Simulate effect based on test type\n",
    "    if config['test_name'] == 'language_preference':\n",
    "        # Group A: English only - lower engagement for Swahili speakers\n",
    "        # Group B: Bilingual - higher engagement overall\n",
    "        \n",
    "        # First determine language preference\n",
    "        df['preferred_language'] = np.random.choice(['English', 'Swahili'], size=n_users, p=[0.65, 0.35])\n",
    "        \n",
    "        # Engagement time (minutes)\n",
    "        df['engagement_time'] = np.where(\n",
    "            df['group'] == 'A',\n",
    "            # Group A: Lower engagement time for Swahili speakers\n",
    "            np.where(df['preferred_language'] == 'English',\n",
    "                    np.random.normal(5.2, 2.0, n_users),  # English speakers\n",
    "                    np.random.normal(2.8, 1.5, n_users)),  # Swahili speakers\n",
    "            # Group B: Higher engagement overall, especially for Swahili speakers\n",
    "            np.where(df['preferred_language'] == 'English',\n",
    "                    np.random.normal(5.5, 2.0, n_users),  # English speakers\n",
    "                    np.random.normal(5.0, 2.0, n_users))   # Swahili speakers\n",
    "        )\n",
    "        \n",
    "        # Number of messages\n",
    "        df['messages_sent'] = np.where(\n",
    "            df['group'] == 'A',\n",
    "            np.where(df['preferred_language'] == 'English',\n",
    "                    np.random.poisson(8, n_users),\n",
    "                    np.random.poisson(5, n_users)),\n",
    "            np.where(df['preferred_language'] == 'English',\n",
    "                    np.random.poisson(9, n_users),\n",
    "                    np.random.poisson(8, n_users))\n",
    "        )\n",
    "        \n",
    "        # Task completion (1 = completed, 0 = not completed)\n",
    "        p_complete_a_english = 0.70\n",
    "        p_complete_a_swahili = 0.40\n",
    "        p_complete_b_english = 0.72\n",
    "        p_complete_b_swahili = 0.65\n",
    "        \n",
    "        df['task_completed'] = np.where(\n",
    "            df['group'] == 'A',\n",
    "            np.where(df['preferred_language'] == 'English',\n",
    "                    np.random.binomial(1, p_complete_a_english, n_users),\n",
    "                    np.random.binomial(1, p_complete_a_swahili, n_users)),\n",
    "            np.where(df['preferred_language'] == 'English',\n",
    "                    np.random.binomial(1, p_complete_b_english, n_users),\n",
    "                    np.random.binomial(1, p_complete_b_swahili, n_users))\n",
    "        )\n",
    "        \n",
    "        # Satisfaction rating (1-5)\n",
    "        satisfaction_a_english = np.random.choice([1, 2, 3, 4, 5], n_users, \n",
    "                                                p=[0.05, 0.10, 0.25, 0.40, 0.20])\n",
    "        satisfaction_a_swahili = np.random.choice([1, 2, 3, 4, 5], n_users, \n",
    "                                                p=[0.15, 0.25, 0.35, 0.20, 0.05])\n",
    "        satisfaction_b_english = np.random.choice([1, 2, 3, 4, 5], n_users, \n",
    "                                                p=[0.03, 0.07, 0.20, 0.45, 0.25])\n",
    "        satisfaction_b_swahili = np.random.choice([1, 2, 3, 4, 5], n_users, \n",
    "                                                p=[0.05, 0.10, 0.25, 0.40, 0.20])\n",
    "        \n",
    "        df['satisfaction_rating'] = np.where(\n",
    "            df['group'] == 'A',\n",
    "            np.where(df['preferred_language'] == 'English',\n",
    "                    satisfaction_a_english,\n",
    "                    satisfaction_a_swahili),\n",
    "            np.where(df['preferred_language'] == 'English',\n",
    "                    satisfaction_b_english,\n",
    "                    satisfaction_b_swahili)\n",
    "        )\n",
    "        \n",
    "    elif config['test_name'] == 'recommendation_algorithm':\n",
    "        # Group A: Rule-based recommendations\n",
    "        # Group B: ML-based personalized recommendations\n",
    "        \n",
    "        # Conversion rate (1 = converted, 0 = not converted)\n",
    "        p_convert_a = 0.10  # Rule-based\n",
    "        p_convert_b = 0.15  # ML-based\n",
    "        \n",
    "        df['conversion_rate'] = np.where(\n",
    "            df['group'] == 'A',\n",
    "            np.random.binomial(1, p_convert_a, n_users),\n",
    "            np.random.binomial(1, p_convert_b, n_users)\n",
    "        )\n",
    "        \n",
    "        # Recommendation clicks\n",
    "        df['recommendation_clicks'] = np.where(\n",
    "            df['group'] == 'A',\n",
    "            np.random.poisson(2, n_users),\n",
    "            np.random.poisson(3, n_users)\n",
    "        )\n",
    "        \n",
    "        # Satisfaction rating (1-5)\n",
    "        df['satisfaction_rating'] = np.where(\n",
    "            df['group'] == 'A',\n",
    "            np.random.choice([1, 2, 3, 4, 5], n_users, p=[0.05, 0.15, 0.30, 0.35, 0.15]),\n",
    "            np.random.choice([1, 2, 3, 4, 5], n_users, p=[0.03, 0.12, 0.25, 0.40, 0.20])\n",
    "        )\n",
    "        \n",
    "        # Amount invested (KES)\n",
    "        df['amount_invested'] = np.where(\n",
    "            (df['conversion_rate'] == 1) & (df['group'] == 'A'),\n",
    "            np.random.normal(15000, 5000, n_users),\n",
    "            np.where(\n",
    "                (df['conversion_rate'] == 1) & (df['group'] == 'B'),\n",
    "                np.random.normal(18000, 6000, n_users),\n",
    "                0\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    elif config['test_name'] == 'response_style':\n",
    "        # Group A: Formal responses\n",
    "        # Group B: Conversational responses\n",
    "        \n",
    "        # Satisfaction rating (1-5)\n",
    "        df['satisfaction_rating'] = np.where(\n",
    "            df['group'] == 'A',\n",
    "            np.random.choice([1, 2, 3, 4, 5], n_users, p=[0.05, 0.15, 0.40, 0.30, 0.10]),\n",
    "            np.random.choice([1, 2, 3, 4, 5], n_users, p=[0.03, 0.10, 0.27, 0.40, 0.20])\n",
    "        )\n",
    "        \n",
    "        # Engagement time (minutes)\n",
    "        df['engagement_time'] = np.where(\n",
    "            df['group'] == 'A',\n",
    "            np.random.normal(4.5, 2.0, n_users),\n",
    "            np.random.normal(6.2, 2.5, n_users)\n",
    "        )\n",
    "        \n",
    "        # Number of messages\n",
    "        df['messages_sent'] = np.where(\n",
    "            df['group'] == 'A',\n",
    "            np.random.poisson(6, n_users),\n",
    "            np.random.poisson(9, n_users)\n",
    "        )\n",
    "        \n",
    "        # Return visit within 7 days (1 = yes, 0 = no)\n",
    "        df['return_visit'] = np.where(\n",
    "            df['group'] == 'A',\n",
    "            np.random.binomial(1, 0.30, n_users),\n",
    "            np.random.binomial(1, 0.45, n_users)\n",
    "        )\n",
    "    \n",
    "    elif config['test_name'] == 'ui_design':\n",
    "        # Group A: Text-focused UI\n",
    "        # Group B: Visual UI with charts and graphics\n",
    "        \n",
    "        # Task completion (1 = completed, 0 = not completed)\n",
    "        df['task_completed'] = np.where(\n",
    "            df['group'] == 'A',\n",
    "            np.random.binomial(1, 0.65, n_users),\n",
    "            np.random.binomial(1, 0.75, n_users)\n",
    "        )\n",
    "        \n",
    "        # Time to completion (minutes) - only for those who completed the task\n",
    "        completion_time_a = np.random.gamma(6, 0.5, n_users)  # Shape, scale\n",
    "        completion_time_b = np.random.gamma(4, 0.6, n_users)  # Shape, scale\n",
    "        \n",
    "        df['time_to_completion'] = np.where(\n",
    "            df['task_completed'] == 1,\n",
    "            np.where(df['group'] == 'A', completion_time_a, completion_time_b),\n",
    "            np.nan\n",
    "        )\n",
    "        \n",
    "        # Satisfaction rating (1-5)\n",
    "        df['satisfaction_rating'] = np.where(\n",
    "            df['group'] == 'A',\n",
    "            np.random.choice([1, 2, 3, 4, 5], n_users, p=[0.05, 0.15, 0.35, 0.35, 0.10]),\n",
    "            np.random.choice([1, 2, 3, 4, 5], n_users, p=[0.03, 0.10, 0.25, 0.37, 0.25])\n",
    "        )\n",
    "        \n",
    "        # Information retention (percentage of info retained in follow-up quiz)\n",
    "        df['info_retention'] = np.where(\n",
    "            df['group'] == 'A',\n",
    "            np.random.beta(5, 3, n_users) * 100,  # More cognitive processing with text\n",
    "            np.random.beta(7, 3, n_users) * 100   # Better retention with visuals\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Simulate data for the language preference test\n",
    "language_test_data = simulate_ab_test_data(ab_test_configs[0], n_users=1500)\n",
    "print(f\"Simulated data for {ab_test_configs[0]['test_name']} test:\")\n",
    "print(language_test_data.head())\n",
    "print(f\"Shape: {language_test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08b156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ab_test(df, metric, group_col='group', alpha=0.05):\n",
    "    \"\"\"\n",
    "    Analyze A/B test results for a specific metric\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with test data\n",
    "    - metric: Column name of the metric to analyze\n",
    "    - group_col: Column name for the group assignment (default: 'group')\n",
    "    - alpha: Significance level (default: 0.05)\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    # Get data for each group\n",
    "    group_a = df[df[group_col] == 'A'][metric].dropna()\n",
    "    group_b = df[df[group_col] == 'B'][metric].dropna()\n",
    "    \n",
    "    # Basic statistics\n",
    "    mean_a = group_a.mean()\n",
    "    mean_b = group_b.mean()\n",
    "    median_a = group_a.median()\n",
    "    median_b = group_b.median()\n",
    "    std_a = group_a.std()\n",
    "    std_b = group_b.std()\n",
    "    \n",
    "    # Absolute difference and relative lift\n",
    "    abs_diff = mean_b - mean_a\n",
    "    rel_lift = (mean_b - mean_a) / mean_a * 100 if mean_a != 0 else float('inf')\n",
    "    \n",
    "    # Statistical significance test\n",
    "    if len(group_a) > 30 and len(group_b) > 30:  # Large sample condition for t-test\n",
    "        # Use t-test for continuous variables\n",
    "        t_stat, p_value = stats.ttest_ind(group_a, group_b, equal_var=False)\n",
    "        test_name = \"Welch's t-test\"\n",
    "    else:\n",
    "        # Use Mann-Whitney U test for non-parametric test\n",
    "        u_stat, p_value = stats.mannwhitneyu(group_a, group_b)\n",
    "        test_name = \"Mann-Whitney U test\"\n",
    "    \n",
    "    # Confidence interval for the difference\n",
    "    ci_low, ci_high = sms.DescrStatsW(group_b).tconfint_mean() - sms.DescrStatsW(group_a).tconfint_mean()\n",
    "    \n",
    "    # Effect size - Cohen's d for continuous variables\n",
    "    pooled_std = np.sqrt((std_a**2 + std_b**2) / 2)\n",
    "    cohens_d = abs_diff / pooled_std if pooled_std != 0 else float('inf')\n",
    "    \n",
    "    # Results dictionary\n",
    "    results = {\n",
    "        'metric': metric,\n",
    "        'mean_a': mean_a,\n",
    "        'mean_b': mean_b,\n",
    "        'median_a': median_a,\n",
    "        'median_b': median_b,\n",
    "        'std_a': std_a,\n",
    "        'std_b': std_b,\n",
    "        'abs_diff': abs_diff,\n",
    "        'rel_lift': rel_lift,\n",
    "        'p_value': p_value,\n",
    "        'significant': p_value < alpha,\n",
    "        'test_name': test_name,\n",
    "        'ci_low': ci_low,\n",
    "        'ci_high': ci_high,\n",
    "        'cohens_d': cohens_d,\n",
    "        'sample_size_a': len(group_a),\n",
    "        'sample_size_b': len(group_b)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze language preference test results\n",
    "language_analysis = {}\n",
    "for metric in ['engagement_time', 'messages_sent', 'task_completed', 'satisfaction_rating']:\n",
    "    language_analysis[metric] = analyze_ab_test(language_test_data, metric)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nLanguage Preference Test Analysis Results:\")\n",
    "for metric, results in language_analysis.items():\n",
    "    print(f\"\\nMetric: {metric}\")\n",
    "    print(f\"Group A (English only): Mean = {results['mean_a']:.2f}, Median = {results['median_a']:.2f}, Std = {results['std_a']:.2f}\")\n",
    "    print(f\"Group B (Bilingual): Mean = {results['mean_b']:.2f}, Median = {results['median_b']:.2f}, Std = {results['std_b']:.2f}\")\n",
    "    print(f\"Absolute Difference: {results['abs_diff']:.2f}\")\n",
    "    print(f\"Relative Lift: {results['rel_lift']:.2f}%\")\n",
    "    print(f\"p-value ({results['test_name']}): {results['p_value']:.4f}\")\n",
    "    print(f\"Statistically Significant: {'Yes' if results['significant'] else 'No'}\")\n",
    "    print(f\"95% Confidence Interval: [{results['ci_low']:.2f}, {results['ci_high']:.2f}]\")\n",
    "    print(f\"Effect Size (Cohen's d): {results['cohens_d']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_analysis(df, metric, segment_col, group_col='group'):\n",
    "    \"\"\"\n",
    "    Analyze A/B test results segmented by a specific column\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with test data\n",
    "    - metric: Column name of the metric to analyze\n",
    "    - segment_col: Column name for segmentation\n",
    "    - group_col: Column name for the group assignment (default: 'group')\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with segment analysis results\n",
    "    \"\"\"\n",
    "    segments = df[segment_col].unique()\n",
    "    results = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        segment_df = df[df[segment_col] == segment]\n",
    "        \n",
    "        # Skip segments with too few samples\n",
    "        if len(segment_df) < 30:\n",
    "            continue\n",
    "            \n",
    "        group_a = segment_df[segment_df[group_col] == 'A'][metric].dropna()\n",
    "        group_b = segment_df[segment_df[group_col] == 'B'][metric].dropna()\n",
    "        \n",
    "        # Skip if either group has too few samples\n",
    "        if len(group_a) < 15 or len(group_b) < 15:\n",
    "            continue\n",
    "        \n",
    "        mean_a = group_a.mean()\n",
    "        mean_b = group_b.mean()\n",
    "        abs_diff = mean_b - mean_a\n",
    "        rel_lift = (mean_b - mean_a) / mean_a * 100 if mean_a != 0 else float('inf')\n",
    "        \n",
    "        # Statistical test\n",
    "        t_stat, p_value = stats.ttest_ind(group_a, group_b, equal_var=False)\n",
    "        \n",
    "        results.append({\n",
    "            'segment': segment,\n",
    "            'segment_size': len(segment_df),\n",
    "            'mean_a': mean_a,\n",
    "            'mean_b': mean_b,\n",
    "            'abs_diff': abs_diff,\n",
    "            'rel_lift': rel_lift,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207bc454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation by preferred language\n",
    "language_segments = segment_analysis(language_test_data, 'engagement_time', 'preferred_language')\n",
    "print(\"\\nSegmentation Analysis for Engagement Time by Language Preference:\")\n",
    "print(language_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff84bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of primary metrics\n",
    "metrics_to_plot = ['engagement_time', 'satisfaction_rating', 'messages_sent']\n",
    "\n",
    "fig, axes = plt.subplots(1, len(metrics_to_plot), figsize=(18, 6))\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    sns.boxplot(x='group', y=metric, data=language_test_data, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {metric}')\n",
    "    axes[i].set_xlabel('Test Group')\n",
    "    axes[i].set_ylabel(metric)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Language preference interaction plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='preferred_language', y='engagement_time', hue='group', \n",
    "            data=language_test_data, ci=68)\n",
    "plt.title('Engagement Time by Language Preference and Test Group')\n",
    "plt.xlabel('Preferred Language')\n",
    "plt.ylabel('Engagement Time (minutes)')\n",
    "plt.legend(title='Test Group', loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Task completion rates by group and language\n",
    "task_completion = language_test_data.groupby(['group', 'preferred_language'])['task_completed'].mean().reset_index()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='preferred_language', y='task_completed', hue='group', data=task_completion)\n",
    "plt.title('Task Completion Rate by Language Preference and Test Group')\n",
    "plt.xlabel('Preferred Language')\n",
    "plt.ylabel('Task Completion Rate')\n",
    "plt.legend(title='Test Group', loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Satisfaction rating distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "satisfaction_counts = language_test_data.groupby(['group', 'satisfaction_rating']).size().reset_index(name='count')\n",
    "satisfaction_pct = satisfaction_counts.copy()\n",
    "\n",
    "# Calculate percentages within each group\n",
    "for group in ['A', 'B']:\n",
    "    group_total = satisfaction_counts[satisfaction_counts['group'] == group]['count'].sum()\n",
    "    satisfaction_pct.loc[satisfaction_pct['group'] == group, 'percentage'] = (\n",
    "        satisfaction_pct[satisfaction_pct['group'] == group]['count'] / group_total * 100\n",
    "    )\n",
    "\n",
    "sns.barplot(x='satisfaction_rating', y='percentage', hue='group', data=satisfaction_pct)\n",
    "plt.title('Satisfaction Rating Distribution by Test Group')\n",
    "plt.xlabel('Satisfaction Rating (1-5)')\n",
    "plt.ylabel('Percentage of Users')\n",
    "plt.legend(title='Test Group', loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93acec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive engagement time by language preference\n",
    "fig = px.box(language_test_data, x='preferred_language', y='engagement_time', color='group',\n",
    "            title='Engagement Time by Language Preference',\n",
    "            labels={'preferred_language': 'Preferred Language', \n",
    "                   'engagement_time': 'Engagement Time (minutes)', \n",
    "                   'group': 'Test Group'},\n",
    "            category_orders={'group': ['A', 'B']},\n",
    "            color_discrete_map={'A': '#636EFA', 'B': '#EF553B'})\n",
    "\n",
    "fig.update_layout(legend_title_text='Test Group')\n",
    "fig.show()\n",
    "\n",
    "# Interactive task completion rates by segment\n",
    "task_success_pct = language_test_data.groupby(['group', 'preferred_language', 'age_group'])['task_completed'].mean().reset_index()\n",
    "task_success_pct['task_completed_pct'] = task_success_pct['task_completed'] * 100\n",
    "\n",
    "fig = px.bar(task_success_pct, x='preferred_language', y='task_completed_pct', color='group',\n",
    "            facet_col='age_group', \n",
    "            title='Task Completion Rate by Segment',\n",
    "            labels={'preferred_language': 'Preferred Language', \n",
    "                   'task_completed_pct': 'Task Completion Rate (%)', \n",
    "                   'group': 'Test Group',\n",
    "                   'age_group': 'Age Group'},\n",
    "            category_orders={'group': ['A', 'B']},\n",
    "            color_discrete_map={'A': '#636EFA', 'B': '#EF553B'})\n",
    "\n",
    "fig.update_layout(legend_title_text='Test Group')\n",
    "fig.show()\n",
    "\n",
    "# Sankey diagram for user flows (Group B only)\n",
    "group_b_users = language_test_data[language_test_data['group'] == 'B']\n",
    "\n",
    "# Create flow data\n",
    "language_counts = group_b_users['preferred_language'].value_counts()\n",
    "completed_by_language = group_b_users.groupby('preferred_language')['task_completed'].value_counts().unstack().fillna(0)\n",
    "satisfaction_by_completion = group_b_users.groupby('task_completed')['satisfaction_rating'].value_counts().unstack().fillna(0)\n",
    "\n",
    "# Define nodes and links\n",
    "label = ['Group B Users', 'English', 'Swahili', 'Completed Task', 'Didn\\'t Complete', \n",
    "        'Rating 1', 'Rating 2', 'Rating 3', 'Rating 4', 'Rating 5']\n",
    "\n",
    "# Node indices for source/target\n",
    "source = [0, 0, 1, 1, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4]\n",
    "target = [1, 2, 3, 4, 3, 4, 5, 6, 7, 8, 9, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Flow values (estimated from our data)\n",
    "english_users = language_counts.get('English', 0)\n",
    "swahili_users = language_counts.get('Swahili', 0)\n",
    "english_completed = completed_by_language.loc['English', 1] if 'English' in completed_by_language.index else 0\n",
    "english_not_completed = completed_by_language.loc['English', 0] if 'English' in completed_by_language.index else 0\n",
    "swahili_completed = completed_by_language.loc['Swahili', 1] if 'Swahili' in completed_by_language.index else 0\n",
    "swahili_not_completed = completed_by_language.loc['Swahili', 0] if 'Swahili' in completed_by_language.index else 0\n",
    "\n",
    "# Estimated distribution of ratings for demonstration\n",
    "completed_ratings = [50, 70, 150, 250, 100] # Just example values\n",
    "not_completed_ratings = [80, 100, 120, 50, 20] # Just example values\n",
    "\n",
    "value = [english_users, swahili_users, \n",
    "        english_completed, english_not_completed,\n",
    "        swahili_completed, swahili_not_completed,\n",
    "        *completed_ratings, *not_completed_ratings]\n",
    "\n",
    "# Create Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=label,\n",
    "        color=[\"#636EFA\", \"#EF553B\", \"#00CC96\", \"#AB63FA\", \"#FFA15A\", \n",
    "              \"#19D3F3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"]\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value\n",
    "    ))])\n",
    "\n",
    "fig.update_layout(title_text=\"User Flow for Group B (Bilingual Interface)\",\n",
    "                 font_size=12)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd731dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_power(sample_size_per_group, baseline_conversion, minimum_detectable_effect, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate the statistical power for a given sample size and effect\n",
    "    \n",
    "    Parameters:\n",
    "    - sample_size_per_group: Number of users per variant\n",
    "    - baseline_conversion: Expected conversion rate for control group\n",
    "    - minimum_detectable_effect: Smallest meaningful difference to detect\n",
    "    - alpha: Significance level (Type I error probability)\n",
    "    \n",
    "    Returns:\n",
    "    - power: Power of the test (1 - Type II error probability)\n",
    "    \"\"\"\n",
    "    # Standard normal critical value for alpha\n",
    "    z_alpha = stats.norm.ppf(1 - alpha/2)\n",
    "    \n",
    "    # Standard deviations under null and alternative\n",
    "    sd1 = np.sqrt(2 * baseline_conversion * (1 - baseline_conversion))\n",
    "    sd2 = np.sqrt(baseline_conversion * (1 - baseline_conversion) + \n",
    "                 (baseline_conversion + minimum_detectable_effect) * \n",
    "                 (1 - (baseline_conversion + minimum_detectable_effect)))\n",
    "    \n",
    "    # Calculate the critical value for beta (Type II error)\n",
    "    z_beta = (minimum_detectable_effect * np.sqrt(sample_size_per_group) - z_alpha * sd1) / sd2\n",
    "    \n",
    "    # Calculate power\n",
    "    power = stats.norm.cdf(z_beta)\n",
    "    \n",
    "    return power\n",
    "\n",
    "# Calculate power for our tests\n",
    "print(\"\\nPower Analysis for Actual Sample Sizes:\")\n",
    "for metric, results in language_analysis.items():\n",
    "    baseline = results['mean_a']\n",
    "    observed_diff = results['abs_diff']\n",
    "    \n",
    "    # Skip if baseline is zero to avoid division by zero\n",
    "    if baseline == 0:\n",
    "        continue\n",
    "        \n",
    "    mde = observed_diff / 2  # Use half of observed difference as MDE\n",
    "    sample_size = min(results['sample_size_a'], results['sample_size_b'])\n",
    "    \n",
    "    power = calculate_power(sample_size, baseline, mde)\n",
    "    \n",
    "    print(f\"Metric: {metric}\")\n",
    "    print(f\"Sample Size per Group: {sample_size}\")\n",
    "    print(f\"Baseline (Group A): {baseline:.4f}\")\n",
    "    print(f\"Observed Difference: {observed_diff:.4f}\")\n",
    "    print(f\"Minimum Detectable Effect: {mde:.4f}\")\n",
    "    print(f\"Statistical Power: {power:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c8401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_analysis(df, target, treatment_col='group', controls=None):\n",
    "    \"\"\"\n",
    "    Perform regression analysis to control for confounding variables\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with test data\n",
    "    - target: Name of the target variable\n",
    "    - treatment_col: Name of the treatment variable\n",
    "    - controls: List of control variables to include\n",
    "    \n",
    "    Returns:\n",
    "    - Regression results\n",
    "    \"\"\"\n",
    "    # Create treatment dummy (1 for B, 0 for A)\n",
    "    df['treatment'] = (df[treatment_col] == 'B').astype(int)\n",
    "    \n",
    "    # Prepare formula\n",
    "    controls = controls or []\n",
    "    formula = f\"{target} ~ treatment\"\n",
    "    \n",
    "    if controls:\n",
    "        formula += \" + \" + \" + \".join(controls)\n",
    "    \n",
    "    # Fit regression model\n",
    "    model = sm.OLS.from_formula(formula, data=df).fit()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Regression for engagement time, controlling for preferred language and age group\n",
    "engagement_reg = regression_analysis(\n",
    "    language_test_data, \n",
    "    'engagement_time', \n",
    "    controls=['C(preferred_language)', 'C(age_group)']\n",
    ")\n",
    "\n",
    "print(\"\\nRegression Analysis for Engagement Time:\")\n",
    "print(engagement_reg.summary().tables[1])\n",
    "\n",
    "# Regression for task completion, controlling for preferred language and age group\n",
    "task_reg = regression_analysis(\n",
    "    language_test_data, \n",
    "    'task_completed', \n",
    "    controls=['C(preferred_language)', 'C(age_group)']\n",
    ")\n",
    "\n",
    "print(\"\\nRegression Analysis for Task Completion:\")\n",
    "print(task_reg.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195ce55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nConclusions for Language Preference A/B Test:\")\n",
    "print(\"===========================================\")\n",
    "\n",
    "# Check if primary metric shows significant improvement\n",
    "primary_metric = 'engagement_time'\n",
    "if language_analysis[primary_metric]['significant']:\n",
    "    print(f\"âœ… Primary metric ({primary_metric}) shows statistically significant improvement:\")\n",
    "    print(f\"   Group B (Bilingual interface) outperforms Group A (English only) by {language_analysis[primary_metric]['rel_lift']:.2f}%\")\n",
    "    print(f\"   p-value: {language_analysis[primary_metric]['p_value']:.4f}\")\n",
    "else:\n",
    "    print(f\"âŒ Primary metric ({primary_metric}) does not show statistically significant improvement:\")\n",
    "    print(f\"   Group B (Bilingual interface) vs Group A (English only): {language_analysis[primary_metric]['rel_lift']:.2f}%\")\n",
    "    print(f\"   p-value: {language_analysis[primary_metric]['p_value']:.4f}\")\n",
    "\n",
    "# Check secondary metrics\n",
    "significant_secondary = []\n",
    "non_significant_secondary = []\n",
    "\n",
    "for metric in ['messages_sent', 'task_completed', 'satisfaction_rating']:\n",
    "    if language_analysis[metric]['significant']:\n",
    "        significant_secondary.append(f\"{metric} (+{language_analysis[metric]['rel_lift']:.2f}%)\")\n",
    "    else:\n",
    "        non_significant_secondary.append(f\"{metric} ({language_analysis[metric]['rel_lift']:.2f}%)\")\n",
    "\n",
    "if significant_secondary:\n",
    "    print(f\"\\nâœ… Secondary metrics showing significant improvement:\")\n",
    "    for metric in significant_secondary:\n",
    "        print(f\"   - {metric}\")\n",
    "\n",
    "if non_significant_secondary:\n",
    "    print(f\"\\nâŒ Secondary metrics without significant improvement:\")\n",
    "    for metric in non_significant_secondary:\n",
    "        print(f\"   - {metric}\")\n",
    "\n",
    "# Segment analysis\n",
    "key_segment = language_segments.sort_values('abs_diff', ascending=False).iloc[0]\n",
    "print(f\"\\nðŸ‘¥ Segment Analysis:\")\n",
    "print(f\"   - Largest impact observed for {key_segment['segment']} users: {key_segment['rel_lift']:.2f}% improvement\")\n",
    "\n",
    "# Final recommendation\n",
    "if language_analysis[primary_metric]['significant'] and language_analysis[primary_metric]['rel_lift'] > 5:\n",
    "    print(\"\\nðŸš€ RECOMMENDATION: Implement Bilingual Interface (Group B)\")\n",
    "    print(f\"   The bilingual interface significantly improves engagement time by {language_analysis[primary_metric]['rel_lift']:.2f}%,\")\n",
    "    print(f\"   particularly for Swahili-speaking users. This demonstrates the importance of language\")\n",
    "    print(f\"   localization for PesaGuru's target audience in Kenya.\")\n",
    "elif language_analysis[primary_metric]['significant']:\n",
    "    print(\"\\nðŸ¤” RECOMMENDATION: Consider Implementing Bilingual Interface (Group B)\")\n",
    "    print(f\"   While the improvement is statistically significant, the effect size ({language_analysis[primary_metric]['rel_lift']:.2f}%) is modest.\")\n",
    "    print(f\"   Consider the development costs versus the expected user engagement benefits.\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ RECOMMENDATION: Further Testing Required\")\n",
    "    print(f\"   The results are inconclusive. Consider a larger sample size or different implementation approach.\")\n",
    "\n",
    "print(\"\\nðŸ“Š Action Items:\")\n",
    "print(\"   1. Implement bilingual interface with option to toggle between English and Swahili\")\n",
    "print(\"   2. Monitor impact on actual user engagement and conversion metrics\")\n",
    "print(\"   3. Consider more localization features beyond language (e.g., local financial terms)\")\n",
    "print(\"   4. Develop educational content in both languages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea409013",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nProposed Future A/B Tests for PesaGuru:\")\n",
    "print(\"====================================\")\n",
    "print(\"1. Response Style Test: Compare formal vs. conversational tone\")\n",
    "print(\"2. Recommendation Algorithm: Rule-based vs. ML-based investment suggestions\")\n",
    "print(\"3. UI Design: Text-focused vs. Visualization-rich interface\")\n",
    "print(\"4. Onboarding Flow: Quick start vs. Guided tutorial\")\n",
    "print(\"5. Notification Strategy: Frequency and content of financial alerts\")\n",
    "\n",
    "# Save results for future reference\n",
    "results_summary = {\n",
    "    'test_name': ab_test_configs[0]['test_name'],\n",
    "    'test_date': datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    'primary_metric': primary_metric,\n",
    "    'primary_metric_results': language_analysis[primary_metric],\n",
    "    'secondary_metrics': {m: language_analysis[m] for m in ['messages_sent', 'task_completed', 'satisfaction_rating']},\n",
    "    'segment_analysis': language_segments.to_dict(),\n",
    "    'recommendation': 'Implement Bilingual Interface' if language_analysis[primary_metric]['significant'] else 'Further Testing Required'\n",
    "}\n",
    "\n",
    "print(\"\\nSaving results summary to file...\")\n",
    "with open('ab_test_results_language_preference.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"A/B Testing complete! Results saved for review.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pesaguru",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
