{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3502583d-ce95-4037-9293-6b7cc2244fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting AI Model Evaluation - 2025-03-14\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "import time\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add project root to path for importing PesaGuru modules\n",
    "sys.path.append('C:/xampp/htdocs/PesaGuru')\n",
    "\n",
    "# Configure paths\n",
    "MODEL_DIR = 'C:/xampp/htdocs/PesaGuru/ai/models'\n",
    "DATA_DIR = 'C:/xampp/htdocs/PesaGuru/notebooks/data'\n",
    "RESULTS_DIR = 'C:/xampp/htdocs/PesaGuru/notebooks/outputs/model_evaluation'\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Set style for visualizations\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Define evaluation date\n",
    "EVAL_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"Starting AI Model Evaluation - {EVAL_DATE}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b173cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_intent_classifier():\n",
    "    \"\"\"Load the intent classification model\"\"\"\n",
    "    try:\n",
    "        # Import the intent classifier module\n",
    "        from ai.models.intent_classifier import IntentClassifier\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = IntentClassifier()\n",
    "        model.load(os.path.join(MODEL_DIR, 'intent_classifier.pkl'))\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading intent classifier: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_entity_extractor():\n",
    "    \"\"\"Load the entity extraction model\"\"\"\n",
    "    try:\n",
    "        # Import the entity extractor module\n",
    "        from ai.models.entity_extractor import EntityExtractor\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = EntityExtractor()\n",
    "        model.load(os.path.join(MODEL_DIR, 'entity_extractor.pkl'))\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading entity extractor: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_sentiment_model():\n",
    "    \"\"\"Load the sentiment analysis model\"\"\"\n",
    "    try:\n",
    "        # Import the sentiment model\n",
    "        from ai.models.sentiment_model import SentimentAnalyzer\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = SentimentAnalyzer()\n",
    "        model.load(os.path.join(MODEL_DIR, 'sentiment_model.pkl'))\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment model: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_recommendation_model():\n",
    "    \"\"\"Load the investment recommendation model\"\"\"\n",
    "    try:\n",
    "        # Import the recommendation model\n",
    "        from ai.models.recommendation_model import RecommendationEngine\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = RecommendationEngine()\n",
    "        model.load(os.path.join(MODEL_DIR, 'recommendation_model.pkl'))\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading recommendation model: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_test_data():\n",
    "    \"\"\"Load test datasets for model evaluation\"\"\"\n",
    "    test_data = {}\n",
    "    \n",
    "    # Load intent classification test data\n",
    "    try:\n",
    "        test_data['intent'] = pd.read_csv(os.path.join(DATA_DIR, 'intent_test_data.csv'))\n",
    "        print(f\"Loaded intent test data: {len(test_data['intent'])} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading intent test data: {e}\")\n",
    "    \n",
    "    # Load entity extraction test data\n",
    "    try:\n",
    "        test_data['entity'] = pd.read_csv(os.path.join(DATA_DIR, 'entity_test_data.csv'))\n",
    "        print(f\"Loaded entity test data: {len(test_data['entity'])} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading entity test data: {e}\")\n",
    "    \n",
    "    # Load sentiment analysis test data\n",
    "    try:\n",
    "        test_data['sentiment'] = pd.read_csv(os.path.join(DATA_DIR, 'sentiment_test_data.csv'))\n",
    "        print(f\"Loaded sentiment test data: {len(test_data['sentiment'])} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment test data: {e}\")\n",
    "    \n",
    "    # Load recommendation test data\n",
    "    try:\n",
    "        test_data['recommendation'] = pd.read_csv(os.path.join(DATA_DIR, 'recommendation_test_data.csv'))\n",
    "        print(f\"Loaded recommendation test data: {len(test_data['recommendation'])} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading recommendation test data: {e}\")\n",
    "    \n",
    "    # Load user profiles for testing\n",
    "    try:\n",
    "        test_data['user_profiles'] = pd.read_csv(os.path.join(DATA_DIR, 'user_profiles_test.csv'))\n",
    "        print(f\"Loaded user profiles test data: {len(test_data['user_profiles'])} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading user profiles test data: {e}\")\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "# Attempt to load all models\n",
    "print(\"Loading models...\")\n",
    "intent_model = load_intent_classifier()\n",
    "entity_model = load_entity_extractor()\n",
    "sentiment_model = load_sentiment_model()\n",
    "recommendation_model = load_recommendation_model()\n",
    "\n",
    "# Load test data\n",
    "print(\"\\nLoading test data...\")\n",
    "test_data = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df46316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_intent_classification():\n",
    "    \"\"\"Evaluate intent classification performance\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"INTENT CLASSIFICATION EVALUATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if intent_model is None or 'intent' not in test_data:\n",
    "        print(\"Cannot evaluate: Intent model or test data not available\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare data\n",
    "    X_test = test_data['intent']['text'].tolist()\n",
    "    y_true = test_data['intent']['intent'].tolist()\n",
    "    \n",
    "    # Measure response time\n",
    "    start_time = time.time()\n",
    "    y_pred = []\n",
    "    \n",
    "    for query in tqdm(X_test, desc=\"Processing intents\"):\n",
    "        # Predict intent\n",
    "        intent = intent_model.predict(query)\n",
    "        y_pred.append(intent)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    avg_response_time = (end_time - start_time) / len(X_test) * 1000  # in milliseconds\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nIntent Classification Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Average Response Time: {avg_response_time:.2f} ms\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Intent Classification Confusion Matrix')\n",
    "    plt.xlabel('Predicted Intent')\n",
    "    plt.ylabel('Actual Intent')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, f'intent_cm_{EVAL_DATE}.png'))\n",
    "    \n",
    "    # Save results to dictionary\n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'response_time_ms': avg_response_time,\n",
    "        'date': EVAL_DATE\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_entity_extraction():\n",
    "    \"\"\"Evaluate entity extraction performance\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ENTITY EXTRACTION EVALUATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if entity_model is None or 'entity' not in test_data:\n",
    "        print(\"Cannot evaluate: Entity model or test data not available\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare data\n",
    "    texts = test_data['entity']['text'].tolist()\n",
    "    true_entities = test_data['entity']['entities'].tolist()\n",
    "    \n",
    "    # Convert string representation of entities to actual lists\n",
    "    true_entities = [json.loads(ent.replace(\"'\", \"\\\"\")) if isinstance(ent, str) else ent for ent in true_entities]\n",
    "    \n",
    "    # Measure response time\n",
    "    start_time = time.time()\n",
    "    predicted_entities = []\n",
    "    \n",
    "    for text in tqdm(texts, desc=\"Processing entities\"):\n",
    "        # Extract entities\n",
    "        entities = entity_model.extract_entities(text)\n",
    "        predicted_entities.append(entities)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    avg_response_time = (end_time - start_time) / len(texts) * 1000  # in milliseconds\n",
    "    \n",
    "    # Calculate metrics\n",
    "    # This is a simplified evaluation - in practice, entity extraction should be evaluated with more nuanced metrics\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for true, pred in zip(true_entities, predicted_entities):\n",
    "        # Count correct extractions\n",
    "        true_set = set([(e['entity'], e['value']) for e in true])\n",
    "        pred_set = set([(e['entity'], e['value']) for e in pred])\n",
    "        \n",
    "        correct += len(true_set.intersection(pred_set))\n",
    "        total += len(true_set)\n",
    "    \n",
    "    precision = correct / sum(len(p) for p in predicted_entities) if sum(len(p) for p in predicted_entities) > 0 else 0\n",
    "    recall = correct / total if total > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nEntity Extraction Results:\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Average Response Time: {avg_response_time:.2f} ms\")\n",
    "    \n",
    "    # Visualize entity extraction performance by entity type\n",
    "    entity_types = set()\n",
    "    for entities in true_entities:\n",
    "        for e in entities:\n",
    "            entity_types.add(e['entity'])\n",
    "    \n",
    "    entity_performance = {entity_type: {'true': 0, 'correct': 0} for entity_type in entity_types}\n",
    "    \n",
    "    for true, pred in zip(true_entities, predicted_entities):\n",
    "        for e in true:\n",
    "            entity_type = e['entity']\n",
    "            entity_performance[entity_type]['true'] += 1\n",
    "            \n",
    "            # Check if this entity was correctly predicted\n",
    "            if any(p['entity'] == entity_type and p['value'] == e['value'] for p in pred):\n",
    "                entity_performance[entity_type]['correct'] += 1\n",
    "    \n",
    "    # Calculate F1 scores by entity type\n",
    "    entity_f1 = {}\n",
    "    for entity_type, counts in entity_performance.items():\n",
    "        if counts['true'] == 0:\n",
    "            entity_f1[entity_type] = 0\n",
    "            continue\n",
    "            \n",
    "        recall = counts['correct'] / counts['true']\n",
    "        \n",
    "        # Calculate precision\n",
    "        predicted_count = sum(1 for entities in predicted_entities for e in entities if e['entity'] == entity_type)\n",
    "        precision = counts['correct'] / predicted_count if predicted_count > 0 else 0\n",
    "        \n",
    "        # Calculate F1\n",
    "        entity_f1[entity_type] = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Plot F1 scores by entity type\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(entity_f1.keys(), entity_f1.values())\n",
    "    plt.title('Entity Extraction F1 Score by Entity Type')\n",
    "    plt.xlabel('Entity Type')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, f'entity_f1_by_type_{EVAL_DATE}.png'))\n",
    "    \n",
    "    # Save results to dictionary\n",
    "    results = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'response_time_ms': avg_response_time,\n",
    "        'entity_f1': entity_f1,\n",
    "        'date': EVAL_DATE\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_sentiment_analysis():\n",
    "    \"\"\"Evaluate sentiment analysis performance\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"SENTIMENT ANALYSIS EVALUATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if sentiment_model is None or 'sentiment' not in test_data:\n",
    "        print(\"Cannot evaluate: Sentiment model or test data not available\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare data\n",
    "    texts = test_data['sentiment']['text'].tolist()\n",
    "    true_sentiments = test_data['sentiment']['sentiment'].tolist()\n",
    "    \n",
    "    # Measure response time\n",
    "    start_time = time.time()\n",
    "    predicted_sentiments = []\n",
    "    \n",
    "    for text in tqdm(texts, desc=\"Processing sentiment\"):\n",
    "        # Predict sentiment\n",
    "        sentiment = sentiment_model.predict(text)\n",
    "        predicted_sentiments.append(sentiment)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    avg_response_time = (end_time - start_time) / len(texts) * 1000  # in milliseconds\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_sentiments, predicted_sentiments)\n",
    "    precision = precision_score(true_sentiments, predicted_sentiments, average='weighted')\n",
    "    recall = recall_score(true_sentiments, predicted_sentiments, average='weighted')\n",
    "    f1 = f1_score(true_sentiments, predicted_sentiments, average='weighted')\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nSentiment Analysis Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Average Response Time: {avg_response_time:.2f} ms\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(true_sentiments, predicted_sentiments)\n",
    "    sentiment_labels = ['negative', 'neutral', 'positive']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=sentiment_labels, yticklabels=sentiment_labels)\n",
    "    plt.title('Sentiment Analysis Confusion Matrix')\n",
    "    plt.xlabel('Predicted Sentiment')\n",
    "    plt.ylabel('Actual Sentiment')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, f'sentiment_cm_{EVAL_DATE}.png'))\n",
    "    \n",
    "    # Save results to dictionary\n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'response_time_ms': avg_response_time,\n",
    "        'date': EVAL_DATE\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run NLP evaluations\n",
    "intent_results = evaluate_intent_classification()\n",
    "entity_results = evaluate_entity_extraction()\n",
    "sentiment_results = evaluate_sentiment_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da130b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendation_accuracy():\n",
    "    \"\"\"Evaluate investment recommendation accuracy\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"RECOMMENDATION ENGINE EVALUATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if recommendation_model is None or 'recommendation' not in test_data or 'user_profiles' not in test_data:\n",
    "        print(\"Cannot evaluate: Recommendation model or test data not available\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare data\n",
    "    user_profiles = test_data['user_profiles']\n",
    "    recommendations_test = test_data['recommendation']\n",
    "    \n",
    "    # Track metrics\n",
    "    correct_recommendations = 0\n",
    "    total_recommendations = len(recommendations_test)\n",
    "    response_times = []\n",
    "    risk_consistency = []\n",
    "    \n",
    "    # Process each test case\n",
    "    for idx, test_case in tqdm(recommendations_test.iterrows(), desc=\"Evaluating recommendations\", total=len(recommendations_test)):\n",
    "        user_id = test_case['user_id']\n",
    "        expected_recommendation = test_case['expected_recommendation']\n",
    "        \n",
    "        # Get user profile\n",
    "        user_profile = user_profiles[user_profiles['user_id'] == user_id].iloc[0].to_dict()\n",
    "        \n",
    "        # Measure response time\n",
    "        start_time = time.time()\n",
    "        recommendation = recommendation_model.get_recommendation(user_profile)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        response_time = (end_time - start_time) * 1000  # in milliseconds\n",
    "        response_times.append(response_time)\n",
    "        \n",
    "        # Check if recommendation matches expected\n",
    "        if recommendation == expected_recommendation:\n",
    "            correct_recommendations += 1\n",
    "        \n",
    "        # Check risk consistency\n",
    "        user_risk_tolerance = user_profile.get('risk_tolerance', 'moderate')\n",
    "        recommendation_risk = recommendation_model.get_recommendation_risk(recommendation)\n",
    "        \n",
    "        # Score risk consistency (0-1)\n",
    "        risk_consistency.append(recommendation_model.calculate_risk_consistency(user_risk_tolerance, recommendation_risk))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = correct_recommendations / total_recommendations if total_recommendations > 0 else 0\n",
    "    avg_response_time = sum(response_times) / len(response_times) if response_times else 0\n",
    "    avg_risk_consistency = sum(risk_consistency) / len(risk_consistency) if risk_consistency else 0\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nRecommendation Engine Results:\")\n",
    "    print(f\"Recommendation Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Average Response Time: {avg_response_time:.2f} ms\")\n",
    "    print(f\"Risk Profile Consistency: {avg_risk_consistency:.4f}\")\n",
    "    \n",
    "    # Plot recommendation accuracy by risk profile\n",
    "    recommendation_by_risk = recommendations_test.copy()\n",
    "    recommendation_by_risk['user_risk'] = recommendation_by_risk['user_id'].apply(\n",
    "        lambda uid: user_profiles[user_profiles['user_id'] == uid]['risk_tolerance'].values[0]\n",
    "    )\n",
    "    \n",
    "    recommendation_by_risk['correct'] = recommendation_by_risk.apply(\n",
    "        lambda row: recommendation_model.get_recommendation(\n",
    "            user_profiles[user_profiles['user_id'] == row['user_id']].iloc[0].to_dict()\n",
    "        ) == row['expected_recommendation'],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Calculate accuracy by risk profile\n",
    "    accuracy_by_risk = recommendation_by_risk.groupby('user_risk')['correct'].mean()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    accuracy_by_risk.plot(kind='bar')\n",
    "    plt.title('Recommendation Accuracy by Risk Profile')\n",
    "    plt.xlabel('Risk Tolerance')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, f'recommendation_by_risk_{EVAL_DATE}.png'))\n",
    "    \n",
    "    # Save results to dictionary\n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'response_time_ms': avg_response_time,\n",
    "        'risk_consistency': avg_risk_consistency,\n",
    "        'accuracy_by_risk': accuracy_by_risk.to_dict(),\n",
    "        'date': EVAL_DATE\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "recommendation_results = evaluate_recommendation_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c0cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_chatbot_response_time():\n",
    "    \"\"\"Evaluate chatbot response time for common queries\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"CHATBOT RESPONSE TIME EVALUATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Define sample queries of different complexity\n",
    "    sample_queries = [\n",
    "        {\"query\": \"What is the current exchange rate for USD to KES?\", \"complexity\": \"simple\"},\n",
    "        {\"query\": \"How do I open a stock trading account?\", \"complexity\": \"simple\"},\n",
    "        {\"query\": \"What are the best investment options for me?\", \"complexity\": \"medium\"},\n",
    "        {\"query\": \"Compare M-Shwari and KCB-M-Pesa loan rates\", \"complexity\": \"medium\"},\n",
    "        {\"query\": \"Given my savings of 50,000 KES, what investment portfolio would you recommend for moderate risk?\", \"complexity\": \"complex\"},\n",
    "        {\"query\": \"Show me the performance of Safaricom stock over the past 3 months and analyze its trend\", \"complexity\": \"complex\"}\n",
    "    ]\n",
    "    \n",
    "    # Setup connection to chatbot API\n",
    "    try:\n",
    "        # Import the chatbot controller\n",
    "        sys.path.append('C:/xampp/htdocs/PesaGuru/server')\n",
    "        from controllers.chatbotController import ChatbotController\n",
    "        \n",
    "        chatbot = ChatbotController()\n",
    "        \n",
    "        # Process each query\n",
    "        response_times = []\n",
    "        \n",
    "        for query_data in tqdm(sample_queries, desc=\"Testing chatbot response time\"):\n",
    "            query = query_data[\"query\"]\n",
    "            complexity = query_data[\"complexity\"]\n",
    "            \n",
    "            # Create dummy user for testing\n",
    "            user = {\n",
    "                \"user_id\": \"test_user_1\",\n",
    "                \"name\": \"Test User\",\n",
    "                \"language\": \"en\",\n",
    "                \"risk_tolerance\": \"moderate\"\n",
    "            }\n",
    "            \n",
    "            # Measure response time\n",
    "            start_time = time.time()\n",
    "            response = chatbot.process_query(query, user)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            response_time = (end_time - start_time) * 1000  # in milliseconds\n",
    "            \n",
    "            response_times.append({\n",
    "                \"query\": query,\n",
    "                \"complexity\": complexity,\n",
    "                \"response_time_ms\": response_time\n",
    "            })\n",
    "        \n",
    "        # Calculate average response time by complexity\n",
    "        response_df = pd.DataFrame(response_times)\n",
    "        avg_by_complexity = response_df.groupby('complexity')['response_time_ms'].mean()\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nChatbot Response Time Results:\")\n",
    "        print(f\"Overall Average Response Time: {response_df['response_time_ms'].mean():.2f} ms\")\n",
    "        print(\"\\nAverage Response Time by Complexity:\")\n",
    "        for complexity, avg_time in avg_by_complexity.items():\n",
    "            print(f\"{complexity.capitalize()}: {avg_time:.2f} ms\")\n",
    "        \n",
    "        # Plot response times by complexity\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='complexity', y='response_time_ms', data=response_df, order=['simple', 'medium', 'complex'])\n",
    "        plt.title('Average Response Time by Query Complexity')\n",
    "        plt.xlabel('Query Complexity')\n",
    "        plt.ylabel('Response Time (ms)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(RESULTS_DIR, f'response_time_by_complexity_{EVAL_DATE}.png'))\n",
    "        \n",
    "        # Return results\n",
    "        results = {\n",
    "            'overall_avg_response_time_ms': response_df['response_time_ms'].mean(),\n",
    "            'response_time_by_complexity': avg_by_complexity.to_dict(),\n",
    "            'date': EVAL_DATE\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating chatbot response time: {e}\")\n",
    "        return None\n",
    "\n",
    "chatbot_response_results = evaluate_chatbot_response_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb1dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_benchmarks():\n",
    "    \"\"\"Compare evaluation results with target benchmarks\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"COMPARISON WITH BENCHMARK TARGETS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Define target benchmarks\n",
    "    benchmarks = {\n",
    "        'intent_recognition': {\n",
    "            'accuracy': 0.90,\n",
    "            'response_time_ms': 1000  # 1 second\n",
    "        },\n",
    "        'entity_extraction': {\n",
    "            'f1': 0.85,\n",
    "            'response_time_ms': 1000\n",
    "        },\n",
    "        'sentiment_analysis': {\n",
    "            'accuracy': 0.80,\n",
    "            'response_time_ms': 1000\n",
    "        },\n",
    "        'recommendation': {\n",
    "            'accuracy': 0.85,\n",
    "            'risk_consistency': 0.90,\n",
    "            'response_time_ms': 2000  # 2 seconds\n",
    "        },\n",
    "        'chatbot_response': {\n",
    "            'overall_avg_response_time_ms': 2000,\n",
    "            'simple_response_time_ms': 1000,\n",
    "            'medium_response_time_ms': 2000,\n",
    "            'complex_response_time_ms': 3000\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Gather current results\n",
    "    current_results = {\n",
    "        'intent_recognition': intent_results if intent_results else {},\n",
    "        'entity_extraction': entity_results if entity_results else {},\n",
    "        'sentiment_analysis': sentiment_results if sentiment_results else {},\n",
    "        'recommendation': recommendation_results if recommendation_results else {},\n",
    "        'chatbot_response': chatbot_response_results if chatbot_response_results else {}\n",
    "    }\n",
    "    \n",
    "    # Create comparison\n",
    "    comparison = {}\n",
    "    \n",
    "    # Intent recognition comparison\n",
    "    if intent_results:\n",
    "        comparison['intent_recognition'] = {\n",
    "            'accuracy': {\n",
    "                'current': intent_results['accuracy'],\n",
    "                'target': benchmarks['intent_recognition']['accuracy'],\n",
    "                'met': intent_results['accuracy'] >= benchmarks['intent_recognition']['accuracy']\n",
    "            },\n",
    "            'response_time_ms': {\n",
    "                'current': intent_results['response_time_ms'],\n",
    "                'target': benchmarks['intent_recognition']['response_time_ms'],\n",
    "                'met': intent_results['response_time_ms'] <= benchmarks['intent_recognition']['response_time_ms']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Entity extraction comparison\n",
    "    if entity_results:\n",
    "        comparison['entity_extraction'] = {\n",
    "            'f1': {\n",
    "                'current': entity_results['f1'],\n",
    "                'target': benchmarks['entity_extraction']['f1'],\n",
    "                'met': entity_results['f1'] >= benchmarks['entity_extraction']['f1']\n",
    "            },\n",
    "            'response_time_ms': {\n",
    "                'current': entity_results['response_time_ms'],\n",
    "                'target': benchmarks['entity_extraction']['response_time_ms'],\n",
    "                'met': entity_results['response_time_ms'] <= benchmarks['entity_extraction']['response_time_ms']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Sentiment analysis comparison\n",
    "    if sentiment_results:\n",
    "        comparison['sentiment_analysis'] = {\n",
    "            'accuracy': {\n",
    "                'current': sentiment_results['accuracy'],\n",
    "                'target': benchmarks['sentiment_analysis']['accuracy'],\n",
    "                'met': sentiment_results['accuracy'] >= benchmarks['sentiment_analysis']['accuracy']\n",
    "            },\n",
    "            'response_time_ms': {\n",
    "                'current': sentiment_results['response_time_ms'],\n",
    "                'target': benchmarks['sentiment_analysis']['response_time_ms'],\n",
    "                'met': sentiment_results['response_time_ms'] <= benchmarks['sentiment_analysis']['response_time_ms']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Recommendation engine comparison\n",
    "    if recommendation_results:\n",
    "        comparison['recommendation'] = {\n",
    "            'accuracy': {\n",
    "                'current': recommendation_results['accuracy'],\n",
    "                'target': benchmarks['recommendation']['accuracy'],\n",
    "                'met': recommendation_results['accuracy'] >= benchmarks['recommendation']['accuracy']\n",
    "            },\n",
    "            'risk_consistency': {\n",
    "                'current': recommendation_results['risk_consistency'],\n",
    "                'target': benchmarks['recommendation']['risk_consistency'],\n",
    "                'met': recommendation_results['risk_consistency'] >= benchmarks['recommendation']['risk_consistency']\n",
    "            },\n",
    "            'response_time_ms': {\n",
    "                'current': recommendation_results['response_time_ms'],\n",
    "                'target': benchmarks['recommendation']['response_time_ms'],\n",
    "                'met': recommendation_results['response_time_ms'] <= benchmarks['recommendation']['response_time_ms']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Chatbot response time comparison\n",
    "    if chatbot_response_results:\n",
    "        comparison['chatbot_response'] = {\n",
    "            'overall_avg_response_time_ms': {\n",
    "                'current': chatbot_response_results['overall_avg_response_time_ms'],\n",
    "                'target': benchmarks['chatbot_response']['overall_avg_response_time_ms'],\n",
    "                'met': chatbot_response_results['overall_avg_response_time_ms'] <= benchmarks['chatbot_response']['overall_avg_response_time_ms']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add complexity-specific comparisons if available\n",
    "        if 'response_time_by_complexity' in chatbot_response_results:\n",
    "            for complexity in ['simple', 'medium', 'complex']:\n",
    "                if complexity in chatbot_response_results['response_time_by_complexity']:\n",
    "                    current_value = chatbot_response_results['response_time_by_complexity'][complexity]\n",
    "                    target_value = benchmarks['chatbot_response'][f'{complexity}_response_time_ms']\n",
    "                    \n",
    "                    comparison['chatbot_response'][f'{complexity}_response_time_ms'] = {\n",
    "                        'current': current_value,\n",
    "                        'target': target_value,\n",
    "                        'met': current_value <= target_value\n",
    "                    }\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"\\nPerformance Comparison with Benchmarks:\")\n",
    "    \n",
    "    for model, metrics in comparison.items():\n",
    "        print(f\"\\n{model.upper()}\")\n",
    "        for metric, values in metrics.items():\n",
    "            status = \"✅ MET\" if values['met'] else \"❌ NOT MET\"\n",
    "            print(f\"  {metric}: {values['current']:.2f} vs target {values['target']:.2f} - {status}\")\n",
    "    \n",
    "    # Create visual benchmark comparison\n",
    "    plot_data = []\n",
    "    \n",
    "    for model, metrics in comparison.items():\n",
    "        for metric, values in metrics.items():\n",
    "            if 'response_time' in metric:\n",
    "                # For response time, lower is better\n",
    "                performance_ratio = values['target'] / values['current'] if values['current'] > 0 else 1\n",
    "                performance_ratio = min(performance_ratio, 1.5)  # Cap at 150% for visualization\n",
    "            else:\n",
    "                # For accuracy metrics, higher is better\n",
    "                performance_ratio = values['current'] / values['target'] if values['target'] > 0 else 0\n",
    "            \n",
    "            plot_data.append({\n",
    "                'model': model,\n",
    "                'metric': metric,\n",
    "                'ratio': performance_ratio,\n",
    "                'met': values['met']\n",
    "            })\n",
    "    \n",
    "    # Plot benchmark comparison\n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = sns.barplot(x='model', y='ratio', hue='metric', data=plot_df)\n",
    "    \n",
    "    # Highlight bars by whether they met the benchmark\n",
    "    for i, bar in enumerate(bars.patches):\n",
    "        if plot_df.iloc[i]['met']:\n",
    "            bar.set_edgecolor('green')\n",
    "            bar.set_linewidth(2)\n",
    "        else:\n",
    "            bar.set_edgecolor('red')\n",
    "            bar.set_linewidth(2)\n",
    "    \n",
    "    plt.axhline(y=1.0, color='r', linestyle='--', alpha=0.7)\n",
    "    plt.title('Model Performance Relative to Benchmarks')\n",
    "    plt.xlabel('Model Component')\n",
    "    plt.ylabel('Performance Ratio (Current/Target)')\n",
    "    plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, f'benchmark_comparison_{EVAL_DATE}.png'))\n",
    "    \n",
    "    # Return comparison\n",
    "    return comparison\n",
    "\n",
    "benchmark_comparison = compare_with_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b745ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_report():\n",
    "    \"\"\"Generate a summary report of all evaluations\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"GENERATING SUMMARY REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Combine all results\n",
    "    all_results = {\n",
    "        'intent_recognition': intent_results,\n",
    "        'entity_extraction': entity_results,\n",
    "        'sentiment_analysis': sentiment_results,\n",
    "        'recommendation': recommendation_results,\n",
    "        'chatbot_response': chatbot_response_results,\n",
    "        'benchmark_comparison': benchmark_comparison,\n",
    "        'evaluation_date': EVAL_DATE\n",
    "    }\n",
    "    \n",
    "    # Create summary metrics\n",
    "    summary = {\n",
    "        'overall_performance': {},\n",
    "        'areas_of_strength': [],\n",
    "        'areas_for_improvement': [],\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Calculate overall performance score\n",
    "    scores = []\n",
    "    \n",
    "    if intent_results:\n",
    "        scores.append(intent_results['accuracy'])\n",
    "    \n",
    "    if entity_results:\n",
    "        scores.append(entity_results['f1'])\n",
    "    \n",
    "    if sentiment_results:\n",
    "        scores.append(sentiment_results['accuracy'])\n",
    "    \n",
    "    if recommendation_results:\n",
    "        scores.append(recommendation_results['accuracy'])\n",
    "        scores.append(recommendation_results['risk_consistency'])\n",
    "    \n",
    "    if scores:\n",
    "        summary['overall_performance']['average_score'] = sum(scores) / len(scores)\n",
    "    \n",
    "    # Determine areas of strength\n",
    "    if benchmark_comparison:\n",
    "        for model, metrics in benchmark_comparison.items():\n",
    "            for metric, values in metrics.items():\n",
    "                if values['met']:\n",
    "                    summary['areas_of_strength'].append(f\"{model} - {metric}\")\n",
    "                else:\n",
    "                    summary['areas_for_improvement'].append(f\"{model} - {metric}\")\n",
    "    \n",
    "    # Generate recommendations\n",
    "    if 'intent_recognition' in summary['areas_for_improvement']:\n",
    "        summary['recommendations'].append(\n",
    "            \"Improve intent recognition by expanding the training dataset with more diverse user queries.\"\n",
    "        )\n",
    "    \n",
    "    if 'entity_extraction' in summary['areas_for_improvement']:\n",
    "        summary['recommendations'].append(\n",
    "            \"Enhance entity extraction by fine-tuning the model on Kenyan financial terminology and adding more entity types.\"\n",
    "        )\n",
    "    \n",
    "    if 'sentiment_analysis' in summary['areas_for_improvement']:\n",
    "        summary['recommendations'].append(\n",
    "            \"Improve sentiment analysis accuracy by training on more financial-specific sentiment data and local expressions.\"\n",
    "        )\n",
    "    \n",
    "    if 'recommendation' in summary['areas_for_improvement']:\n",
    "        summary['recommendations'].append(\n",
    "            \"Enhance investment recommendation accuracy by improving risk profiling and expanding the product database.\"\n",
    "        )\n",
    "    \n",
    "    if 'chatbot_response' in summary['areas_for_improvement']:\n",
    "        summary['recommendations'].append(\n",
    "            \"Optimize response time by implementing caching mechanisms and parallel processing for complex queries.\"\n",
    "        )\n",
    "    \n",
    "    # Add general recommendations\n",
    "    summary['recommendations'].extend([\n",
    "        \"Implement continuous learning from user feedback to improve model accuracy over time.\",\n",
    "        \"Add support for Swahili language to increase user accessibility.\",\n",
    "        \"Expand test datasets to cover more edge cases and real-world scenarios.\"\n",
    "    ])\n",
    "    \n",
    "    # Save all results to JSON\n",
    "    with open(os.path.join(RESULTS_DIR, f'evaluation_results_{EVAL_DATE}.json'), 'w') as f:\n",
    "        json.dump(all_results, f, indent=4)\n",
    "    \n",
    "    # Save summary to JSON\n",
    "    with open(os.path.join(RESULTS_DIR, f'evaluation_summary_{EVAL_DATE}.json'), 'w') as f:\n",
    "        json.dump(summary, f, indent=4)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nEvaluation Summary:\")\n",
    "    \n",
    "    if 'average_score' in summary['overall_performance']:\n",
    "        print(f\"Overall Performance Score: {summary['overall_performance']['average_score']:.2f}\")\n",
    "    \n",
    "    print(\"\\nAreas of Strength:\")\n",
    "    for strength in summary['areas_of_strength']:\n",
    "        print(f\"- {strength}\")\n",
    "    \n",
    "    print(\"\\nAreas for Improvement:\")\n",
    "    for improvement in summary['areas_for_improvement']:\n",
    "        print(f\"- {improvement}\")\n",
    "    \n",
    "    print(\"\\nRecommendations:\")\n",
    "    for recommendation in summary['recommendations']:\n",
    "        print(f\"- {recommendation}\")\n",
    "    \n",
    "    print(f\"\\nDetailed results saved to: {os.path.join(RESULTS_DIR, f'evaluation_results_{EVAL_DATE}.json')}\")\n",
    "    print(f\"Summary saved to: {os.path.join(RESULTS_DIR, f'evaluation_summary_{EVAL_DATE}.json')}\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate summary report\n",
    "summary_report = generate_summary_report()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"AI MODEL EVALUATION COMPLETE\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e147441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_performance_over_time():\n",
    "    \"\"\"Load previous evaluation results and plot performance trends\"\"\"\n",
    "    # Find all evaluation result files\n",
    "    result_files = [f for f in os.listdir(RESULTS_DIR) if f.startswith('evaluation_results_') and f.endswith('.json')]\n",
    "    \n",
    "    if len(result_files) <= 1:\n",
    "        print(\"Not enough historical data to plot performance trends.\")\n",
    "        return\n",
    "    \n",
    "    # Load all evaluation results\n",
    "    performance_history = []\n",
    "    \n",
    "    for result_file in result_files:\n",
    "        try:\n",
    "            with open(os.path.join(RESULTS_DIR, result_file), 'r') as f:\n",
    "                results = json.load(f)\n",
    "                \n",
    "                data_point = {\n",
    "                    'date': results['evaluation_date']\n",
    "                }\n",
    "                \n",
    "                # Extract key metrics\n",
    "                if 'intent_recognition' in results and results['intent_recognition']:\n",
    "                    data_point['intent_accuracy'] = results['intent_recognition']['accuracy']\n",
    "                \n",
    "                if 'entity_extraction' in results and results['entity_extraction']:\n",
    "                    data_point['entity_f1'] = results['entity_extraction']['f1']\n",
    "                \n",
    "                if 'sentiment_analysis' in results and results['sentiment_analysis']:\n",
    "                    data_point['sentiment_accuracy'] = results['sentiment_analysis']['accuracy']\n",
    "                \n",
    "                if 'recommendation' in results and results['recommendation']:\n",
    "                    data_point['recommendation_accuracy'] = results['recommendation']['accuracy']\n",
    "                \n",
    "                performance_history.append(data_point)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading results from {result_file}: {e}\")\n",
    "    \n",
    "    # Sort by date\n",
    "    performance_history.sort(key=lambda x: x['date'])\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    history_df = pd.DataFrame(performance_history)\n",
    "    \n",
    "    # Plot performance trends\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    metrics = [col for col in history_df.columns if col != 'date']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        if metric in history_df.columns:\n",
    "            plt.plot(history_df['date'], history_df[metric], marker='o', label=metric)\n",
    "    \n",
    "    plt.title('Model Performance Trends Over Time')\n",
    "    plt.xlabel('Evaluation Date')\n",
    "    plt.ylabel('Performance Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'performance_trends.png'))\n",
    "    \n",
    "    print(f\"Performance trends plotted and saved to: {os.path.join(RESULTS_DIR, 'performance_trends.png')}\")\n",
    "\n",
    "# Track performance over time\n",
    "try:\n",
    "    track_performance_over_time()\n",
    "except Exception as e:\n",
    "    print(f\"Error tracking performance over time: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfb4fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_financial_accuracy():\n",
    "    \"\"\"Evaluate accuracy of financial calculations and market data\"\"\"\n",
    "    # Test stock price accuracy\n",
    "    nse_stocks = ['SCOM', 'KCB', 'EQTY', 'COOP', 'BAT']\n",
    "    price_accuracy = []\n",
    "    \n",
    "    for stock in nse_stocks:\n",
    "        # Get actual price from NSE API\n",
    "        actual_price = get_nse_price(stock)\n",
    "        # Get price from chatbot\n",
    "        chatbot_price = get_chatbot_stock_price(stock)\n",
    "        # Calculate percentage difference\n",
    "        diff_pct = abs(actual_price - chatbot_price) / actual_price * 100\n",
    "        price_accuracy.append({\n",
    "            'stock': stock,\n",
    "            'actual_price': actual_price,\n",
    "            'chatbot_price': chatbot_price,\n",
    "            'diff_pct': diff_pct\n",
    "        })\n",
    "    \n",
    "    # Test loan calculation accuracy\n",
    "    loan_scenarios = [\n",
    "        {'amount': 50000, 'term': 12, 'rate': 14},\n",
    "        {'amount': 100000, 'term': 24, 'rate': 13},\n",
    "        {'amount': 500000, 'term': 36, 'rate': 12.5}\n",
    "    ]\n",
    "    \n",
    "    loan_accuracy = []\n",
    "    for scenario in loan_scenarios:\n",
    "        # Calculate expected EMI\n",
    "        expected_emi = calculate_loan_emi(scenario['amount'], scenario['term'], scenario['rate'])\n",
    "        # Get chatbot calculation\n",
    "        chatbot_emi = get_chatbot_loan_emi(scenario)\n",
    "        # Calculate percentage difference\n",
    "        diff_pct = abs(expected_emi - chatbot_emi) / expected_emi * 100\n",
    "        loan_accuracy.append({\n",
    "            'scenario': scenario,\n",
    "            'expected_emi': expected_emi,\n",
    "            'chatbot_emi': chatbot_emi,\n",
    "            'diff_pct': diff_pct\n",
    "        })\n",
    "    \n",
    "    return {'stock_price_accuracy': price_accuracy, 'loan_calculation_accuracy': loan_accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe7b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_user_experience_metrics():\n",
    "    \"\"\"Evaluate user satisfaction and engagement metrics\"\"\"\n",
    "    # Load user interaction logs\n",
    "    interaction_logs = pd.read_csv(os.path.join(DATA_DIR, 'user_interaction_logs.csv'))\n",
    "    \n",
    "    # Calculate session completion rate\n",
    "    total_sessions = len(interaction_logs['session_id'].unique())\n",
    "    completed_sessions = len(interaction_logs[interaction_logs['session_completed'] == True]['session_id'].unique())\n",
    "    completion_rate = completed_sessions / total_sessions if total_sessions > 0 else 0\n",
    "    \n",
    "    # Calculate user satisfaction from feedback\n",
    "    satisfaction_scores = interaction_logs['satisfaction_score'].dropna()\n",
    "    avg_satisfaction = satisfaction_scores.mean() if len(satisfaction_scores) > 0 else 0\n",
    "    \n",
    "    # Calculate task success rate\n",
    "    tasks_attempted = len(interaction_logs['task_id'].dropna())\n",
    "    tasks_completed = len(interaction_logs[interaction_logs['task_completed'] == True])\n",
    "    task_success_rate = tasks_completed / tasks_attempted if tasks_attempted > 0 else 0\n",
    "    \n",
    "    # Calculate average queries per session\n",
    "    avg_queries_per_session = interaction_logs.groupby('session_id').size().mean()\n",
    "    \n",
    "    return {\n",
    "        'completion_rate': completion_rate,\n",
    "        'avg_satisfaction': avg_satisfaction,\n",
    "        'task_success_rate': task_success_rate,\n",
    "        'avg_queries_per_session': avg_queries_per_session\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16a8bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multilingual_support():\n",
    "    \"\"\"Evaluate chatbot performance in both English and Swahili\"\"\"\n",
    "    languages = ['en', 'sw']\n",
    "    results = {}\n",
    "    \n",
    "    for lang in languages:\n",
    "        # Load language-specific test data\n",
    "        test_data = pd.read_csv(os.path.join(DATA_DIR, f'intent_test_data_{lang}.csv'))\n",
    "        \n",
    "        # Test intent recognition\n",
    "        X_test = test_data['text'].tolist()\n",
    "        y_true = test_data['intent'].tolist()\n",
    "        \n",
    "        # Set language in model\n",
    "        intent_model.set_language(lang)\n",
    "        \n",
    "        # Predict intents\n",
    "        y_pred = []\n",
    "        for query in X_test:\n",
    "            intent = intent_model.predict(query)\n",
    "            y_pred.append(intent)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "        results[lang] = {\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1\n",
    "        }\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    languages_display = {'en': 'English', 'sw': 'Swahili'}\n",
    "    metrics = ['accuracy', 'f1']\n",
    "    \n",
    "    x = np.arange(len(languages_display))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [results[lang][metric] for lang in languages]\n",
    "        ax.bar(x + i*width, values, width, label=metric)\n",
    "    \n",
    "    ax.set_xlabel('Language')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Performance Comparison by Language')\n",
    "    ax.set_xticks(x + width/2)\n",
    "    ax.set_xticklabels(list(languages_display.values()))\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, f'multilingual_performance_{EVAL_DATE}.png'))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a638c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_model_improvements(evaluation_results):\n",
    "    \"\"\"Implement automatic model improvements based on evaluation\"\"\"\n",
    "    # Identify areas needing improvement\n",
    "    improvement_needed = []\n",
    "    \n",
    "    if evaluation_results['intent_recognition']['accuracy'] < 0.90:\n",
    "        improvement_needed.append('intent_recognition')\n",
    "    \n",
    "    if evaluation_results['entity_extraction']['f1'] < 0.85:\n",
    "        improvement_needed.append('entity_extraction')\n",
    "    \n",
    "    # Implement improvements for each area\n",
    "    for area in improvement_needed:\n",
    "        if area == 'intent_recognition':\n",
    "            # Extract misclassified examples\n",
    "            misclassified = get_misclassified_intents()\n",
    "            # Add to training data with correct labels\n",
    "            update_intent_training_data(misclassified)\n",
    "            # Retrain model\n",
    "            retrain_intent_model()\n",
    "        \n",
    "        elif area == 'entity_extraction':\n",
    "            # Extract missed entities\n",
    "            missed_entities = get_missed_entities()\n",
    "            # Add to training data\n",
    "            update_entity_training_data(missed_entities)\n",
    "            # Retrain model\n",
    "            retrain_entity_model()\n",
    "    \n",
    "    return improvement_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96063dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_dashboard():\n",
    "    \"\"\"Create an HTML dashboard with evaluation results\"\"\"\n",
    "    dashboard_html = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>PesaGuru AI Model Evaluation - {EVAL_DATE}</title>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "            .metric-card {{ border: 1px solid #ddd; border-radius: 8px; padding: 15px; margin: 10px; display: inline-block; width: 200px; }}\n",
    "            .metric-value {{ font-size: 24px; font-weight: bold; }}\n",
    "            .metric-name {{ font-size: 14px; color: #666; }}\n",
    "            .metric-status {{ font-size: 12px; margin-top: 5px; }}\n",
    "            .met {{ color: green; }}\n",
    "            .not-met {{ color: red; }}\n",
    "            .charts {{ display: flex; flex-wrap: wrap; margin-top: 20px; }}\n",
    "            .chart {{ margin: 10px; border: 1px solid #ddd; border-radius: 8px; padding: 10px; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>PesaGuru AI Model Evaluation Dashboard</h1>\n",
    "        <p>Evaluation Date: {EVAL_DATE}</p>\n",
    "        \n",
    "        <h2>Performance Metrics</h2>\n",
    "        <div class=\"metrics-container\">\n",
    "            <!-- Metrics will be inserted here -->\n",
    "        </div>\n",
    "        \n",
    "        <h2>Visualizations</h2>\n",
    "        <div class=\"charts\">\n",
    "            <!-- Charts will be inserted here -->\n",
    "        </div>\n",
    "        \n",
    "        <h2>Recommendations</h2>\n",
    "        <ul>\n",
    "            <!-- Recommendations will be inserted here -->\n",
    "        </ul>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(os.path.join(RESULTS_DIR, f'evaluation_dashboard_{EVAL_DATE}.html'), 'w') as f:\n",
    "        f.write(dashboard_html)\n",
    "    \n",
    "    print(f\"Dashboard created at: {os.path.join(RESULTS_DIR, f'evaluation_dashboard_{EVAL_DATE}.html')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pesaguru",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
