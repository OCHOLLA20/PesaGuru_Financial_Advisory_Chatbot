{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "535cb5d9-f07f-4b1d-8865-fcca36e443f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Embedding, Flatten, Concatenate\n",
    "\n",
    "# File handling and system libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables (API keys, etc.)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1640e27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: ../../data/processed/user_profiles.csv does not exist. Will need to prepare data.\n",
      "Warning: ../../data/processed/kenya_investment_products.csv does not exist. Will need to prepare data.\n",
      "Warning: ../../data/processed/nse_historical_data.csv does not exist. Will need to prepare data.\n",
      "Warning: ../../data/external/financial_advisory_chatbot_survey.csv does not exist. Will need to prepare data.\n",
      "Survey data file not found at ../../data/external/financial_advisory_chatbot_survey.csv\n"
     ]
    }
   ],
   "source": [
    "# Define paths to data sources\n",
    "base_path = '../../data/'\n",
    "user_profiles_path = os.path.join(base_path, 'processed/user_profiles.csv')\n",
    "investment_products_path = os.path.join(base_path, 'processed/kenya_investment_products.csv')\n",
    "market_data_path = os.path.join(base_path, 'processed/nse_historical_data.csv')\n",
    "survey_data_path = os.path.join(base_path, 'external/financial_advisory_chatbot_survey.csv')\n",
    "\n",
    "# Check for data files existence\n",
    "file_paths = [user_profiles_path, investment_products_path, market_data_path, survey_data_path]\n",
    "for path in file_paths:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Warning: {path} does not exist. Will need to prepare data.\")\n",
    "\n",
    "# Load survey data for user preference analysis\n",
    "try:\n",
    "    survey_data = pd.read_csv(survey_data_path)\n",
    "    print(f\"Loaded survey data with {survey_data.shape[0]} responses and {survey_data.shape[1]} features\")\n",
    "    survey_data.head()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Survey data file not found at {survey_data_path}\")\n",
    "    # If we're running this in development, we can use sample data\n",
    "    # Otherwise, we'd need to prepare the data first\n",
    "    survey_data = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3b7d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch NSE data from API if not already downloaded\n",
    "def fetch_nse_data(symbols, start_date, end_date):\n",
    "    \"\"\"Fetch historical stock data for NSE stocks\"\"\"\n",
    "    try:\n",
    "        # Try to load from Alpha Vantage API\n",
    "        alpha_vantage_key = os.getenv('ALPHA_VANTAGE_API_KEY')\n",
    "        if not alpha_vantage_key:\n",
    "            raise ValueError(\"Alpha Vantage API key not found in environment variables\")\n",
    "            \n",
    "        all_data = []\n",
    "        for symbol in symbols:\n",
    "            url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}.NBO&outputsize=full&apikey={alpha_vantage_key}\"\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "            \n",
    "            if 'Time Series (Daily)' not in data:\n",
    "                print(f\"Error fetching data for {symbol}: {data}\")\n",
    "                continue\n",
    "                \n",
    "            time_series = data['Time Series (Daily)']\n",
    "            df = pd.DataFrame(time_series).T\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "            df = df.rename(columns={'1. open': 'Open', '2. high': 'High', '3. low': 'Low', '4. close': 'Close', '5. volume': 'Volume'})\n",
    "            df = df.astype({'Open': float, 'High': float, 'Low': float, 'Close': float, 'Volume': float})\n",
    "            df['Symbol'] = symbol\n",
    "            \n",
    "            # Filter by date range\n",
    "            mask = (df.index >= start_date) & (df.index <= end_date)\n",
    "            df = df.loc[mask]\n",
    "            \n",
    "            all_data.append(df)\n",
    "            \n",
    "        # Combine all data and save\n",
    "        if all_data:\n",
    "            combined_data = pd.concat(all_data)\n",
    "            combined_data.to_csv(market_data_path)\n",
    "            return combined_data\n",
    "        else:\n",
    "            raise ValueError(\"No data fetched from API\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching NSE data: {e}\")\n",
    "        # Return a sample dataframe for development purposes\n",
    "        return pd.DataFrame({'Symbol': ['SCOM', 'EQTY', 'KCB'], \n",
    "                            'Date': [datetime.now() - timedelta(days=i) for i in range(30)],\n",
    "                            'Close': np.random.uniform(10, 50, 30)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e063ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or create investment products dataset\n",
    "try:\n",
    "    investment_products = pd.read_csv(investment_products_path)\n",
    "    print(f\"Loaded {len(investment_products)} investment products\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Investment products dataset not found. Creating sample dataset...\")\n",
    "    \n",
    "    # Create a sample investment products dataset\n",
    "    investment_products = pd.DataFrame({\n",
    "        'product_id': range(1, 21),\n",
    "        'name': [\n",
    "            'Safaricom Stock', 'Equity Bank Stock', 'KCB Stock', 'EABL Stock',\n",
    "            'T-Bill 91 Days', 'T-Bill 182 Days', 'T-Bill 364 Days',\n",
    "            'T-Bond 2 Year', 'T-Bond 5 Year', 'T-Bond 10 Year', 'T-Bond 15 Year',\n",
    "            'Money Market Fund - CIC', 'Money Market Fund - Britam',\n",
    "            'Balanced Fund - Old Mutual', 'Equity Fund - Sanlam',\n",
    "            'REIT - Stanlib Fahari', 'Cytonn High Yield Fund',\n",
    "            'Mwalimu SACCO Savings', 'Stima SACCO Deposits',\n",
    "            'M-Akiba Government Bond'\n",
    "        ],\n",
    "        'category': [\n",
    "            'Equity', 'Equity', 'Equity', 'Equity',\n",
    "            'Government Security', 'Government Security', 'Government Security',\n",
    "            'Government Security', 'Government Security', 'Government Security', 'Government Security',\n",
    "            'Money Market', 'Money Market',\n",
    "            'Balanced Fund', 'Equity Fund',\n",
    "            'Real Estate', 'High Yield',\n",
    "            'SACCO', 'SACCO',\n",
    "            'Mobile Bond'\n",
    "        ],\n",
    "        'min_investment': [\n",
    "            1000, 1000, 1000, 1000,\n",
    "            100000, 100000, 100000,\n",
    "            50000, 50000, 50000, 50000,\n",
    "            5000, 5000,\n",
    "            10000, 10000,\n",
    "            20000, 100000,\n",
    "            1000, 1000,\n",
    "            3000\n",
    "        ],\n",
    "        'risk_level': [\n",
    "            4, 4, 4, 4,\n",
    "            1, 1, 2,\n",
    "            2, 3, 3, 3,\n",
    "            2, 2,\n",
    "            3, 4,\n",
    "            3, 5,\n",
    "            2, 2,\n",
    "            1\n",
    "        ],\n",
    "        'avg_annual_return': [\n",
    "            0.15, 0.12, 0.10, 0.09,\n",
    "            0.06, 0.07, 0.085,\n",
    "            0.095, 0.11, 0.125, 0.13,\n",
    "            0.09, 0.085,\n",
    "            0.11, 0.13,\n",
    "            0.10, 0.18,\n",
    "            0.08, 0.09,\n",
    "            0.10\n",
    "        ],\n",
    "        'liquidity': [\n",
    "            'High', 'High', 'High', 'High',\n",
    "            'Low', 'Medium', 'Medium',\n",
    "            'Low', 'Low', 'Low', 'Low',\n",
    "            'High', 'High',\n",
    "            'Medium', 'Medium',\n",
    "            'Low', 'Low',\n",
    "            'Medium', 'Medium',\n",
    "            'Medium'\n",
    "        ],\n",
    "        'investment_horizon': [\n",
    "            'Long-term', 'Long-term', 'Long-term', 'Long-term',\n",
    "            'Short-term', 'Short-term', 'Short-term',\n",
    "            'Medium-term', 'Medium-term', 'Long-term', 'Long-term',\n",
    "            'Short-term', 'Short-term',\n",
    "            'Medium-term', 'Long-term',\n",
    "            'Long-term', 'Medium-term',\n",
    "            'Medium-term', 'Medium-term',\n",
    "            'Medium-term'\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Save the sample dataset\n",
    "    os.makedirs(os.path.dirname(investment_products_path), exist_ok=True)\n",
    "    investment_products.to_csv(investment_products_path, index=False)\n",
    "    print(f\"Created and saved {len(investment_products)} sample investment products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61c410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or generate user profile data\n",
    "try:\n",
    "    user_profiles = pd.read_csv(user_profiles_path)\n",
    "    print(f\"Loaded {len(user_profiles)} user profiles\")\n",
    "except FileNotFoundError:\n",
    "    print(\"User profiles dataset not found. Creating dataset from survey data...\")\n",
    "    \n",
    "    if survey_data is not None:\n",
    "        # Generate user profiles from survey data\n",
    "        user_profiles = pd.DataFrame()\n",
    "        user_profiles['user_id'] = range(1, len(survey_data) + 1)\n",
    "        user_profiles['age_group'] = survey_data['Age Group']\n",
    "        user_profiles['location'] = survey_data['Location']\n",
    "        user_profiles['employment_status'] = survey_data['Employment Status']\n",
    "        user_profiles['income_range'] = survey_data['Monthly Income Range (KES)']\n",
    "        user_profiles['financial_literacy'] = survey_data['Financial Literacy Level']\n",
    "        user_profiles['primary_goal'] = survey_data['Primary Financial Goals']\n",
    "        user_profiles['challenges'] = survey_data['Financial Challenges']\n",
    "        \n",
    "        # Map risk tolerance based on demographic factors and financial literacy\n",
    "        # This is a simplification - in practice, risk tolerance would be assessed more thoroughly\n",
    "        def map_risk_tolerance(row):\n",
    "            # Start with a base risk tolerance of 3 (medium)\n",
    "            base_risk = 3\n",
    "            \n",
    "            # Adjust based on age group\n",
    "            if row['age_group'] == '18-24' or row['age_group'] == '25-34':\n",
    "                base_risk += 1\n",
    "            elif row['age_group'] == '55-64' or row['age_group'] == '65+':\n",
    "                base_risk -= 1\n",
    "                \n",
    "            # Adjust based on financial literacy\n",
    "            if row['financial_literacy'] >= 8:\n",
    "                base_risk += 1\n",
    "            elif row['financial_literacy'] <= 4:\n",
    "                base_risk -= 1\n",
    "                \n",
    "            # Clamp to 1-5 scale\n",
    "            return max(1, min(5, base_risk))\n",
    "        \n",
    "        user_profiles['risk_tolerance'] = user_profiles.apply(map_risk_tolerance, axis=1)\n",
    "        \n",
    "        # Generate investment horizons based on financial goals\n",
    "        def map_investment_horizon(goal):\n",
    "            short_term_goals = ['Emergency fund', 'Paying off debt', 'Education funding (near term)']\n",
    "            medium_term_goals = ['Buying a home', 'Starting a business', 'Education funding (long term)']\n",
    "            long_term_goals = ['Retirement savings', 'Building wealth', 'Financial independence']\n",
    "            \n",
    "            # Check each category\n",
    "            for st_goal in short_term_goals:\n",
    "                if st_goal.lower() in goal.lower():\n",
    "                    return 'Short-term'\n",
    "            for mt_goal in medium_term_goals:\n",
    "                if mt_goal.lower() in goal.lower():\n",
    "                    return 'Medium-term'\n",
    "            for lt_goal in long_term_goals:\n",
    "                if lt_goal.lower() in goal.lower():\n",
    "                    return 'Long-term'\n",
    "            \n",
    "            # Default to medium-term if no match\n",
    "            return 'Medium-term'\n",
    "        \n",
    "        user_profiles['investment_horizon'] = user_profiles['primary_goal'].apply(map_investment_horizon)\n",
    "        \n",
    "        # Save the processed user profiles\n",
    "        os.makedirs(os.path.dirname(user_profiles_path), exist_ok=True)\n",
    "        user_profiles.to_csv(user_profiles_path, index=False)\n",
    "        print(f\"Created and saved {len(user_profiles)} user profiles from survey data\")\n",
    "    else:\n",
    "        # Create a sample user profiles dataset if survey data is not available\n",
    "        print(\"Survey data not available. Creating sample user profiles...\")\n",
    "        np.random.seed(42)\n",
    "        num_profiles = 500\n",
    "        \n",
    "        age_groups = ['18-24', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    "        locations = ['Nairobi', 'Mombasa', 'Kisumu', 'Nakuru', 'Eldoret', 'Rural']\n",
    "        employment = ['Employed', 'Self-employed', 'Unemployed', 'Student', 'Retired']\n",
    "        income_ranges = ['Below 15,000', '15,000-30,000', '30,001-50,000', '50,001-100,000', 'Above 100,000']\n",
    "        goals = [\n",
    "            'Emergency fund', 'Buying a home', 'Retirement savings', 'Education funding',\n",
    "            'Starting a business', 'Building wealth', 'Paying off debt', 'Financial independence'\n",
    "        ]\n",
    "        challenges = [\n",
    "            'Low income', 'High debt', 'Irregular income', 'Poor financial knowledge',\n",
    "            'High expenses', 'Lack of discipline', 'Market volatility', 'Access to financial services'\n",
    "        ]\n",
    "        horizons = ['Short-term', 'Medium-term', 'Long-term']\n",
    "        \n",
    "        user_profiles = pd.DataFrame({\n",
    "            'user_id': range(1, num_profiles + 1),\n",
    "            'age_group': np.random.choice(age_groups, num_profiles),\n",
    "            'location': np.random.choice(locations, num_profiles),\n",
    "            'employment_status': np.random.choice(employment, num_profiles),\n",
    "            'income_range': np.random.choice(income_ranges, num_profiles),\n",
    "            'financial_literacy': np.random.randint(1, 11, num_profiles),\n",
    "            'primary_goal': np.random.choice(goals, num_profiles),\n",
    "            'challenges': np.random.choice(challenges, num_profiles),\n",
    "            'risk_tolerance': np.random.randint(1, 6, num_profiles),\n",
    "            'investment_horizon': np.random.choice(horizons, num_profiles)\n",
    "        })\n",
    "        \n",
    "        # Save the sample user profiles\n",
    "        os.makedirs(os.path.dirname(user_profiles_path), exist_ok=True)\n",
    "        user_profiles.to_csv(user_profiles_path, index=False)\n",
    "        print(f\"Created and saved {len(user_profiles)} sample user profiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba2762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load market data\n",
    "try:\n",
    "    market_data = pd.read_csv(market_data_path)\n",
    "    print(f\"Loaded market data with {len(market_data)} records\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Market data not found. Fetching from NSE API...\")\n",
    "    # List of top NSE stocks\n",
    "    nse_symbols = ['SCOM', 'EQTY', 'KCB', 'EABL', 'BAT', 'COOP', 'SBIC', 'SCAN', 'ABSA', 'NCBA']\n",
    "    # Fetch data for the past 2 years\n",
    "    end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    start_date = (datetime.now() - timedelta(days=730)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    market_data = fetch_nse_data(nse_symbols, start_date, end_date)\n",
    "    print(f\"Fetched and saved market data with {len(market_data)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5249c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring user profiles\n",
    "print(\"User profile summary statistics:\")\n",
    "user_profiles.describe(include='all')\n",
    "\n",
    "# Distribution of risk tolerance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='risk_tolerance', data=user_profiles)\n",
    "plt.title('Distribution of Risk Tolerance Levels')\n",
    "plt.xlabel('Risk Tolerance (1=Low, 5=High)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Distribution of investment horizons\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='investment_horizon', data=user_profiles)\n",
    "plt.title('Distribution of Investment Horizons')\n",
    "plt.xlabel('Investment Horizon')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Age group vs. risk tolerance\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='age_group', y='risk_tolerance', data=user_profiles)\n",
    "plt.title('Risk Tolerance by Age Group')\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Risk Tolerance (1=Low, 5=High)')\n",
    "plt.show()\n",
    "\n",
    "# Income range vs. risk tolerance\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='income_range', y='risk_tolerance', data=user_profiles)\n",
    "plt.title('Risk Tolerance by Income Range')\n",
    "plt.xlabel('Monthly Income Range (KES)')\n",
    "plt.ylabel('Risk Tolerance (1=Low, 5=High)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d52161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring investment products\n",
    "print(\"Investment products summary:\")\n",
    "investment_products.describe(include='all')\n",
    "\n",
    "# Plot investment product returns vs. risk\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='risk_level', y='avg_annual_return', \n",
    "                hue='category', size='min_investment', sizes=(50, 500),\n",
    "                data=investment_products, alpha=0.7)\n",
    "plt.title('Investment Products: Risk vs. Return')\n",
    "plt.xlabel('Risk Level (1=Low, 5=High)')\n",
    "plt.ylabel('Average Annual Return')\n",
    "\n",
    "# Add product names as text labels\n",
    "for i, row in investment_products.iterrows():\n",
    "    plt.text(row['risk_level'], row['avg_annual_return'], row['name'], \n",
    "             fontsize=9, ha='right', va='bottom')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Investment categories by risk level\n",
    "plt.figure(figsize=(12, 6))\n",
    "category_risk = investment_products.groupby('category')['risk_level'].mean().sort_values()\n",
    "category_risk.plot(kind='barh')\n",
    "plt.title('Average Risk Level by Investment Category')\n",
    "plt.xlabel('Average Risk Level (1=Low, 5=High)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5be325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables to numeric for modeling\n",
    "def preprocess_user_profiles(df):\n",
    "    \"\"\"Preprocess user profiles for model input\"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Create age group numeric mapping\n",
    "    age_mapping = {\n",
    "        '18-24': 1,\n",
    "        '25-34': 2,\n",
    "        '35-44': 3,\n",
    "        '45-54': 4,\n",
    "        '55-64': 5,\n",
    "        '65+': 6\n",
    "    }\n",
    "    df_processed['age_numeric'] = df_processed['age_group'].map(age_mapping)\n",
    "    \n",
    "    # Create income range numeric mapping\n",
    "    income_mapping = {\n",
    "        'Below 15,000': 1,\n",
    "        '15,000-30,000': 2,\n",
    "        '30,001-50,000': 3,\n",
    "        '50,001-100,000': 4,\n",
    "        'Above 100,000': 5\n",
    "    }\n",
    "    df_processed['income_numeric'] = df_processed['income_range'].map(income_mapping)\n",
    "    \n",
    "    # Create investment horizon numeric mapping\n",
    "    horizon_mapping = {\n",
    "        'Short-term': 1,\n",
    "        'Medium-term': 2,\n",
    "        'Long-term': 3\n",
    "    }\n",
    "    df_processed['horizon_numeric'] = df_processed['investment_horizon'].map(horizon_mapping)\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    cat_vars = ['location', 'employment_status', 'primary_goal', 'challenges']\n",
    "    for var in cat_vars:\n",
    "        dummies = pd.get_dummies(df_processed[var], prefix=var)\n",
    "        df_processed = pd.concat([df_processed, dummies], axis=1)\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Preprocess investment products\n",
    "def preprocess_investment_products(df):\n",
    "    \"\"\"Preprocess investment products for model input\"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Create liquidity numeric mapping\n",
    "    liquidity_mapping = {\n",
    "        'Low': 1,\n",
    "        'Medium': 2,\n",
    "        'High': 3\n",
    "    }\n",
    "    df_processed['liquidity_numeric'] = df_processed['liquidity'].map(liquidity_mapping)\n",
    "    \n",
    "    # Create investment horizon numeric mapping\n",
    "    horizon_mapping = {\n",
    "        'Short-term': 1,\n",
    "        'Medium-term': 2,\n",
    "        'Long-term': 3\n",
    "    }\n",
    "    df_processed['horizon_numeric'] = df_processed['investment_horizon'].map(horizon_mapping)\n",
    "    \n",
    "    # One-hot encode categories\n",
    "    cat_vars = ['category']\n",
    "    for var in cat_vars:\n",
    "        dummies = pd.get_dummies(df_processed[var], prefix=var)\n",
    "        df_processed = pd.concat([df_processed, dummies], axis=1)\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Apply preprocessing\n",
    "user_profiles_processed = preprocess_user_profiles(user_profiles)\n",
    "investment_products_processed = preprocess_investment_products(investment_products)\n",
    "\n",
    "print(\"Processed user profiles shape:\", user_profiles_processed.shape)\n",
    "print(\"Processed investment products shape:\", investment_products_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda56d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simulated historical performance dataset\n",
    "def simulate_historical_performance():\n",
    "    \"\"\"Create a simulated investment performance history for training\"\"\"\n",
    "    np.random.seed(42)\n",
    "    num_samples = 2000\n",
    "    \n",
    "    # Sample user IDs and product IDs randomly\n",
    "    user_ids = np.random.choice(user_profiles['user_id'], num_samples)\n",
    "    product_ids = np.random.choice(investment_products['product_id'], num_samples)\n",
    "    \n",
    "    # Create investment history\n",
    "    investment_history = pd.DataFrame({\n",
    "        'user_id': user_ids,\n",
    "        'product_id': product_ids,\n",
    "        'investment_date': pd.date_range(start='2022-01-01', periods=num_samples),\n",
    "        'investment_amount': np.random.uniform(1000, 100000, num_samples),\n",
    "        'duration_months': np.random.randint(1, 36, num_samples)\n",
    "    })\n",
    "    \n",
    "    # Join with user profiles and investment products\n",
    "    investment_history = investment_history.merge(\n",
    "        user_profiles[['user_id', 'risk_tolerance', 'investment_horizon']], \n",
    "        on='user_id'\n",
    "    )\n",
    "    investment_history = investment_history.merge(\n",
    "        investment_products[['product_id', 'risk_level', 'avg_annual_return', 'investment_horizon']], \n",
    "        on='product_id', \n",
    "        suffixes=('_user', '_product')\n",
    "    )\n",
    "    \n",
    "    # Calculate a risk-return alignment score\n",
    "    # Higher score = better alignment between user profile and product\n",
    "    investment_history['risk_alignment'] = 5 - abs(investment_history['risk_tolerance'] - investment_history['risk_level'])\n",
    "    \n",
    "    # Calculate a horizon alignment score\n",
    "    horizon_map = {'Short-term': 1, 'Medium-term': 2, 'Long-term': 3}\n",
    "    investment_history['horizon_user_numeric'] = investment_history['investment_horizon_user'].map(horizon_map)\n",
    "    investment_history['horizon_product_numeric'] = investment_history['investment_horizon_product'].map(horizon_map)\n",
    "    investment_history['horizon_alignment'] = 3 - abs(investment_history['horizon_user_numeric'] - investment_history['horizon_product_numeric'])\n",
    "    \n",
    "    # Calculate an overall recommendation score\n",
    "    # This combines alignment factors with a randomization factor to simulate real-world noise\n",
    "    investment_history['recommendation_score'] = (\n",
    "        investment_history['risk_alignment'] * 0.5 +\n",
    "        investment_history['horizon_alignment'] * 0.3 +\n",
    "        np.random.uniform(0, 1, num_samples) * 0.2  # Random factor for diversity\n",
    "    )\n",
    "    \n",
    "    # Calculate a user satisfaction score (simulated)\n",
    "    # This would normally come from user feedback\n",
    "    base_satisfaction = investment_history['recommendation_score'] * 2  # Base satisfaction from good recommendations\n",
    "    returns_factor = investment_history['avg_annual_return'] * 5  # Returns influence satisfaction\n",
    "    noise = np.random.normal(0, 0.5, num_samples)  # Random noise\n",
    "    \n",
    "    investment_history['user_satisfaction'] = base_satisfaction + returns_factor + noise\n",
    "    investment_history['user_satisfaction'] = investment_history['user_satisfaction'].clip(1, 10)  # Clip to 1-10 scale\n",
    "    \n",
    "    # Create a binary target for classification: Was this a good recommendation?\n",
    "    investment_history['good_recommendation'] = (investment_history['user_satisfaction'] >= 7).astype(int)\n",
    "    \n",
    "    return investment_history\n",
    "\n",
    "# Generate simulated investment history\n",
    "investment_history = simulate_historical_performance()\n",
    "print(f\"Generated {len(investment_history)} investment history records\")\n",
    "investment_history.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c58bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Analyze the investment history data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=investment_history, x='user_satisfaction', bins=20)\n",
    "plt.title('Distribution of User Satisfaction Scores')\n",
    "plt.axvline(x=7, color='red', linestyle='--')\n",
    "plt.xlabel('Satisfaction Score (1-10)')\n",
    "plt.show()\n",
    "\n",
    "# Plot risk alignment vs. satisfaction\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='risk_alignment', y='user_satisfaction', data=investment_history)\n",
    "plt.title('Risk Alignment vs. User Satisfaction')\n",
    "plt.xlabel('Risk Alignment Score (higher = better)')\n",
    "plt.ylabel('User Satisfaction (1-10)')\n",
    "plt.show()\n",
    "\n",
    "# Plot horizon alignment vs. satisfaction\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='horizon_alignment', y='user_satisfaction', data=investment_history)\n",
    "plt.title('Investment Horizon Alignment vs. User Satisfaction')\n",
    "plt.xlabel('Horizon Alignment Score (higher = better)')\n",
    "plt.ylabel('User Satisfaction (1-10)')\n",
    "plt.show()\n",
    "\n",
    "# Plot correlation matrix\n",
    "correlations = investment_history[\n",
    "    ['risk_alignment', 'horizon_alignment', 'avg_annual_return', \n",
    "     'recommendation_score', 'user_satisfaction', 'good_recommendation']\n",
    "].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlations, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix of Key Investment Factors')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af20672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for the classification model\n",
    "def prepare_features(history_df, user_df, product_df):\n",
    "    \"\"\"Prepare features for the recommendation model\"\"\"\n",
    "    # Merge history with user features\n",
    "    features_df = history_df.merge(\n",
    "        user_df[['user_id', 'age_numeric', 'income_numeric', 'financial_literacy', 'horizon_numeric']],\n",
    "        on='user_id'\n",
    "    )\n",
    "    \n",
    "    # Merge with product features\n",
    "    features_df = features_df.merge(\n",
    "        product_df[['product_id', 'risk_level', 'avg_annual_return', 'min_investment', 'liquidity_numeric', 'horizon_numeric']],\n",
    "        on='product_id',\n",
    "        suffixes=('_user', '_product')\n",
    "    )\n",
    "    \n",
    "    # Create interaction features\n",
    "    features_df['risk_diff'] = abs(features_df['risk_tolerance'] - features_df['risk_level'])\n",
    "    features_df['horizon_diff'] = abs(features_df['horizon_numeric_user'] - features_df['horizon_numeric_product'])\n",
    "    features_df['risk_return_ratio'] = features_df['risk_level'] / features_df['avg_annual_return']\n",
    "    features_df['affordability_ratio'] = features_df['min_investment'] / features_df['income_numeric']\n",
    "    \n",
    "    # Get relevant features\n",
    "    X_features = features_df[[\n",
    "        'risk_tolerance', 'risk_level', 'risk_diff',\n",
    "        'horizon_numeric_user', 'horizon_numeric_product', 'horizon_diff',\n",
    "        'age_numeric', 'income_numeric', 'financial_literacy',\n",
    "        'avg_annual_return', 'liquidity_numeric_product', 'risk_return_ratio', 'affordability_ratio',\n",
    "        'investment_amount', 'duration_months'\n",
    "    ]]\n",
    "    \n",
    "    # Get target variable\n",
    "    y_target = features_df['good_recommendation']\n",
    "    \n",
    "    return X_features, y_target, features_df\n",
    "\n",
    "# Prepare the model features\n",
    "X, y, features_df = prepare_features(\n",
    "    investment_history, \n",
    "    user_profiles_processed, \n",
    "    investment_products_processed\n",
    ")\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Target vector shape:\", y.shape)\n",
    "print(\"Class distribution:\\n\", y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e505ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd033f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "print(\"Training Random Forest classifier...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "y_prob_rf = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nRandom Forest Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "plt.title('Feature Importance - Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279e7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Gradient Boosting classifier for comparison\n",
    "print(\"Training Gradient Boosting classifier...\")\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gb = gb_model.predict(X_test_scaled)\n",
    "y_prob_gb = gb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nGradient Boosting Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_gb)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Gradient Boosting')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3593f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a neural network model for investment recommendations\n",
    "def build_nn_recommender():\n",
    "    \"\"\"Build a neural network recommendation model\"\"\"\n",
    "    # User features input\n",
    "    user_input = Input(shape=(5,), name='user_features')  # age, income, literacy, risk_tolerance, horizon\n",
    "    \n",
    "    # Product features input\n",
    "    product_input = Input(shape=(5,), name='product_features')  # risk, return, min_investment, liquidity, horizon\n",
    "    \n",
    "    # User branch\n",
    "    user_dense = Dense(16, activation='relu')(user_input)\n",
    "    user_dense = Dropout(0.2)(user_dense)\n",
    "    user_dense = Dense(8, activation='relu')(user_dense)\n",
    "    \n",
    "    # Product branch\n",
    "    product_dense = Dense(16, activation='relu')(product_input)\n",
    "    product_dense = Dropout(0.2)(product_dense)\n",
    "    product_dense = Dense(8, activation='relu')(product_dense)\n",
    "    \n",
    "    # Combine both branches\n",
    "    concatenated = Concatenate()([user_dense, product_dense])\n",
    "    \n",
    "    # Joint network\n",
    "    joint = Dense(16, activation='relu')(concatenated)\n",
    "    joint = Dropout(0.3)(joint)\n",
    "    joint = Dense(8, activation='relu')(joint)\n",
    "    \n",
    "    # Output (recommendation score)\n",
    "    output = Dense(1, activation='sigmoid', name='recommendation')(joint)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=[user_input, product_input], outputs=output)\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Prepare data for neural network\n",
    "def prepare_nn_data(features_df):\n",
    "    \"\"\"Prepare data for neural network model\"\"\"\n",
    "    # User features\n",
    "    user_features = features_df[[\n",
    "        'age_numeric', 'income_numeric', 'financial_literacy', 'risk_tolerance', 'horizon_numeric_user'\n",
    "    ]].values\n",
    "    \n",
    "    # Product features\n",
    "    product_features = features_df[[\n",
    "        'risk_level', 'avg_annual_return', 'min_investment', 'liquidity_numeric_product', 'horizon_numeric_product'\n",
    "    ]].values\n",
    "    \n",
    "    # Target\n",
    "    target = features_df['good_recommendation'].values\n",
    "    \n",
    "    return user_features, product_features, target\n",
    "\n",
    "# Scale features for neural network\n",
    "user_scaler = StandardScaler()\n",
    "product_scaler = StandardScaler()\n",
    "\n",
    "# Assume features_df is provided\n",
    "user_features, product_features, target = prepare_nn_data(features_df)\n",
    "\n",
    "# Split data\n",
    "indices = np.arange(len(target))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42, stratify=target)\n",
    "\n",
    "# Scale features\n",
    "user_features_train = user_scaler.fit_transform(user_features[train_idx])\n",
    "user_features_test = user_scaler.transform(user_features[test_idx])\n",
    "\n",
    "product_features_train = product_scaler.fit_transform(product_features[train_idx])\n",
    "product_features_test = product_scaler.transform(product_features[test_idx])\n",
    "\n",
    "y_train_nn = target[train_idx]\n",
    "y_test_nn = target[test_idx]\n",
    "\n",
    "# Build and train the neural network model\n",
    "nn_model = build_nn_recommender()\n",
    "print(nn_model.summary())\n",
    "\n",
    "# Train the model\n",
    "history = nn_model.fit(\n",
    "    [user_features_train, product_features_train], y_train_nn,\n",
    "    validation_data=([user_features_test, product_features_test], y_test_nn),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "nn_evaluation = nn_model.evaluate([user_features_test, product_features_test], y_test_nn)\n",
    "print(f\"Neural Network - Loss: {nn_evaluation[0]:.4f}, Accuracy: {nn_evaluation[1]:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e7d747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for product clustering\n",
    "product_features_for_clustering = investment_products_processed[[\n",
    "    'risk_level', 'avg_annual_return', 'liquidity_numeric', 'horizon_numeric'\n",
    "]]\n",
    "\n",
    "# Scale features\n",
    "product_cluster_scaler = StandardScaler()\n",
    "product_features_scaled = product_cluster_scaler.fit_transform(product_features_for_clustering)\n",
    "\n",
    "# Determine optimal number of clusters using the elbow method\n",
    "inertia = []\n",
    "k_range = range(1, 10)\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(product_features_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Based on the elbow curve, choose optimal k and perform clustering\n",
    "optimal_k = 5  # This should be chosen based on the elbow plot\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "clusters = kmeans.fit_predict(product_features_scaled)\n",
    "\n",
    "# Add cluster labels to the investment products\n",
    "investment_products_processed['cluster'] = clusters\n",
    "\n",
    "# Visualize clusters using PCA\n",
    "pca = PCA(n_components=2)\n",
    "product_features_2d = pca.fit_transform(product_features_scaled)\n",
    "\n",
    "# Plot clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(product_features_2d[:, 0], product_features_2d[:, 1], \n",
    "                     c=clusters, cmap='viridis', s=100, alpha=0.8)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "\n",
    "# Add product names as annotations\n",
    "for i, (x, y) in enumerate(product_features_2d):\n",
    "    plt.annotate(investment_products_processed.iloc[i]['name'], \n",
    "                 (x, y), fontsize=8, ha='center', va='center',\n",
    "                 bbox=dict(boxstyle='round,pad=0.3', fc='white', alpha=0.7))\n",
    "\n",
    "plt.title('Investment Product Clusters (PCA Visualization)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze clusters\n",
    "cluster_analysis = investment_products_processed.groupby('cluster').agg({\n",
    "    'risk_level': 'mean',\n",
    "    'avg_annual_return': 'mean',\n",
    "    'min_investment': 'mean',\n",
    "    'liquidity_numeric': 'mean',\n",
    "    'horizon_numeric': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "print(\"Cluster Analysis:\")\n",
    "print(cluster_analysis)\n",
    "\n",
    "# Print products in each cluster\n",
    "for cluster_id in range(optimal_k):\n",
    "    print(f\"\\nCluster {cluster_id} Products:\")\n",
    "    cluster_products = investment_products_processed[investment_products_processed['cluster'] == cluster_id]\n",
    "    for _, product in cluster_products.iterrows():\n",
    "        print(f\"  - {product['name']} (Risk: {product['risk_level']}, Return: {product['avg_annual_return']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a638547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvestmentRecommender:\n",
    "    \"\"\"Investment recommendation engine for PesaGuru chatbot\"\"\"\n",
    "    \n",
    "    def __init__(self, rf_model, nn_model, product_clusters, products_df):\n",
    "        \"\"\"Initialize with trained models\"\"\"\n",
    "        self.rf_model = rf_model\n",
    "        self.nn_model = nn_model\n",
    "        self.product_clusters = product_clusters\n",
    "        self.products_df = products_df\n",
    "        self.user_scaler = user_scaler\n",
    "        self.product_scaler = product_scaler\n",
    "        self.feature_scaler = scaler\n",
    "        \n",
    "    def recommend_products(self, user_profile, top_n=5, diversify=True):\n",
    "        \"\"\"Generate personalized investment recommendations\"\"\"\n",
    "        # Extract key user features\n",
    "        user_features = np.array([[\n",
    "            user_profile['age_numeric'], \n",
    "            user_profile['income_numeric'], \n",
    "            user_profile['financial_literacy'],\n",
    "            user_profile['risk_tolerance'],\n",
    "            user_profile['horizon_numeric']\n",
    "        ]])\n",
    "        \n",
    "        # Scale user features\n",
    "        user_features_scaled = self.user_scaler.transform(user_features)\n",
    "        \n",
    "        # Initialize scores dictionary\n",
    "        product_scores = {}\n",
    "        \n",
    "        # Evaluate each product\n",
    "        for _, product in self.products_df.iterrows():\n",
    "            # Extract product features\n",
    "            product_features = np.array([[\n",
    "                product['risk_level'],\n",
    "                product['avg_annual_return'],\n",
    "                product['min_investment'],\n",
    "                product['liquidity_numeric'],\n",
    "                product['horizon_numeric']\n",
    "            ]])\n",
    "            \n",
    "            # Scale product features\n",
    "            product_features_scaled = self.product_scaler.transform(product_features)\n",
    "            \n",
    "            # Combined features for RF model\n",
    "            combined_features = np.array([[\n",
    "                user_profile['risk_tolerance'], \n",
    "                product['risk_level'],\n",
    "                abs(user_profile['risk_tolerance'] - product['risk_level']),  # risk_diff\n",
    "                user_profile['horizon_numeric'],\n",
    "                product['horizon_numeric'],\n",
    "                abs(user_profile['horizon_numeric'] - product['horizon_numeric']),  # horizon_diff\n",
    "                user_profile['age_numeric'],\n",
    "                user_profile['income_numeric'],\n",
    "                user_profile['financial_literacy'],\n",
    "                product['avg_annual_return'],\n",
    "                product['liquidity_numeric'],\n",
    "                product['risk_level'] / max(0.01, product['avg_annual_return']),  # risk_return_ratio\n",
    "                product['min_investment'] / max(1, user_profile['income_numeric']),  # affordability_ratio\n",
    "                50000,  # dummy investment_amount\n",
    "                12      # dummy duration_months\n",
    "            ]])\n",
    "            \n",
    "            # Scale combined features\n",
    "            combined_features_scaled = self.feature_scaler.transform(combined_features)\n",
    "            \n",
    "            # Get predictions from both models\n",
    "            rf_prob = self.rf_model.predict_proba(combined_features_scaled)[0, 1]\n",
    "            nn_prob = self.nn_model.predict([user_features_scaled, product_features_scaled])[0, 0]\n",
    "            \n",
    "            # Combine scores (weighted average)\n",
    "            combined_score = (rf_prob * 0.6) + (nn_prob * 0.4)\n",
    "            \n",
    "            # Store score\n",
    "            product_scores[product['product_id']] = {\n",
    "                'product_id': product['product_id'],\n",
    "                'name': product['name'],\n",
    "                'category': product['category'],\n",
    "                'risk_level': product['risk_level'],\n",
    "                'avg_annual_return': product['avg_annual_return'],\n",
    "                'min_investment': product['min_investment'],\n",
    "                'cluster': product['cluster'],\n",
    "                'score': combined_score\n",
    "            }\n",
    "        \n",
    "        # Get top recommendations\n",
    "        if diversify:\n",
    "            # Group by cluster\n",
    "            cluster_products = {}\n",
    "            for product_id, info in product_scores.items():\n",
    "                cluster = info['cluster']\n",
    "                if cluster not in cluster_products:\n",
    "                    cluster_products[cluster] = []\n",
    "                cluster_products[cluster].append(info)\n",
    "            \n",
    "            # Get top product from each cluster\n",
    "            recommendations = []\n",
    "            for cluster in sorted(cluster_products.keys()):\n",
    "                sorted_cluster_products = sorted(cluster_products[cluster], key=lambda x: x['score'], reverse=True)\n",
    "                if sorted_cluster_products:\n",
    "                    recommendations.append(sorted_cluster_products[0])\n",
    "            \n",
    "            # Sort final recommendations by score\n",
    "            recommendations = sorted(recommendations, key=lambda x: x['score'], reverse=True)[:top_n]\n",
    "        else:\n",
    "            # Simply get top N products by score\n",
    "            recommendations = sorted(\n",
    "                [info for _, info in product_scores.items()], \n",
    "                key=lambda x: x['score'], \n",
    "                reverse=True\n",
    "            )[:top_n]\n",
    "        \n",
    "        return recommendations\n",
    "        \n",
    "    def explain_recommendation(self, recommendation, user_profile):\n",
    "        \"\"\"Generate explanation for why a product was recommended\"\"\"\n",
    "        explanations = []\n",
    "        \n",
    "        # Risk tolerance alignment\n",
    "        risk_diff = abs(user_profile['risk_tolerance'] - recommendation['risk_level'])\n",
    "        if risk_diff <= 1:\n",
    "            explanations.append(f\"This investment aligns well with your risk tolerance level of {user_profile['risk_tolerance']}/5.\")\n",
    "        else:\n",
    "            risk_comparison = \"higher\" if recommendation['risk_level'] > user_profile['risk_tolerance'] else \"lower\"\n",
    "            explanations.append(f\"This investment has a {risk_comparison} risk level ({recommendation['risk_level']}/5) \"\n",
    "                               f\"compared to your risk tolerance ({user_profile['risk_tolerance']}/5).\")\n",
    "        \n",
    "        # Return potential\n",
    "        returns_percent = recommendation['avg_annual_return'] * 100\n",
    "        explanations.append(f\"It has an average annual return of {returns_percent:.1f}%.\")\n",
    "        \n",
    "        # Investment horizon\n",
    "        horizon_map = {1: 'Short-term', 2: 'Medium-term', 3: 'Long-term'}\n",
    "        user_horizon = horizon_map.get(user_profile['horizon_numeric'], 'Medium-term')\n",
    "        product_horizon = horizon_map.get(recommendation.get('horizon_numeric', 2), 'Medium-term')\n",
    "        \n",
    "        if user_horizon == product_horizon:\n",
    "            explanations.append(f\"This aligns with your {user_horizon.lower()} investment horizon.\")\n",
    "        else:\n",
    "            explanations.append(f\"While you indicated a {user_horizon.lower()} investment horizon, \"\n",
    "                              f\"this is a {product_horizon.lower()} investment.\")\n",
    "        \n",
    "        # Affordability\n",
    "        min_investment = recommendation['min_investment']\n",
    "        explanations.append(f\"This requires a minimum investment of KES {min_investment:,.0f}.\")\n",
    "        \n",
    "        return explanations\n",
    "\n",
    "# Create the recommendation engine\n",
    "recommender = InvestmentRecommender(\n",
    "    rf_model=rf_model,\n",
    "    nn_model=nn_model,\n",
    "    product_clusters=kmeans,\n",
    "    products_df=investment_products_processed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c432ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the recommendation engine with a few sample user profiles\n",
    "\n",
    "def test_recommender_with_sample_profiles():\n",
    "    \"\"\"Test the recommendation engine with sample user profiles\"\"\"\n",
    "    # Sample profiles\n",
    "    test_profiles = [\n",
    "        {\n",
    "            \"profile_name\": \"Young Professional with High Risk Tolerance\",\n",
    "            \"age_numeric\": 2,  # 25-34\n",
    "            \"income_numeric\": 3,  # 30,001-50,000\n",
    "            \"financial_literacy\": 7,\n",
    "            \"risk_tolerance\": 4,\n",
    "            \"horizon_numeric\": 3  # Long-term\n",
    "        },\n",
    "        {\n",
    "            \"profile_name\": \"Older Conservative Investor\",\n",
    "            \"age_numeric\": 5,  # 55-64\n",
    "            \"income_numeric\": 4,  # 50,001-100,000\n",
    "            \"financial_literacy\": 8,\n",
    "            \"risk_tolerance\": 2,\n",
    "            \"horizon_numeric\": 1  # Short-term\n",
    "        },\n",
    "        {\n",
    "            \"profile_name\": \"Middle-Age Balanced Investor\",\n",
    "            \"age_numeric\": 3,  # 35-44\n",
    "            \"income_numeric\": 5,  # >100,000\n",
    "            \"financial_literacy\": 9,\n",
    "            \"risk_tolerance\": 3,\n",
    "            \"horizon_numeric\": 2  # Medium-term\n",
    "        },\n",
    "        {\n",
    "            \"profile_name\": \"Young Low-Income Cautious Investor\",\n",
    "            \"age_numeric\": 1,  # 18-24\n",
    "            \"income_numeric\": 1,  # <15,000\n",
    "            \"financial_literacy\": 4,\n",
    "            \"risk_tolerance\": 2,\n",
    "            \"horizon_numeric\": 2  # Medium-term\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Generate recommendations for each profile\n",
    "    for profile in test_profiles:\n",
    "        print(f\"\\n\\n{'=' * 50}\")\n",
    "        print(f\"Recommendations for: {profile['profile_name']}\")\n",
    "        print(f\"{'=' * 50}\")\n",
    "        print(f\"Age Group: {profile['age_numeric']}\")\n",
    "        print(f\"Income Level: {profile['income_numeric']}\")\n",
    "        print(f\"Financial Literacy: {profile['financial_literacy']}/10\")\n",
    "        print(f\"Risk Tolerance: {profile['risk_tolerance']}/5\")\n",
    "        print(f\"Investment Horizon: {profile['horizon_numeric']} (1=Short, 2=Medium, 3=Long)\")\n",
    "        print(\"\\nTop Recommendations:\")\n",
    "        print(\"------------------\")\n",
    "        \n",
    "        # Get diverse recommendations\n",
    "        recommendations = recommender.recommend_products(profile, top_n=3, diversify=True)\n",
    "        \n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"\\n{i}. {rec['name']} ({rec['category']})\")\n",
    "            print(f\"   Risk Level: {rec['risk_level']}/5\")\n",
    "            print(f\"   Avg. Annual Return: {rec['avg_annual_return']*100:.1f}%\")\n",
    "            print(f\"   Min. Investment: KES {rec['min_investment']:,.0f}\")\n",
    "            print(f\"   Recommendation Score: {rec['score']:.2f}\")\n",
    "            print(\"\\n   Why this recommendation:\")\n",
    "            for explanation in recommender.explain_recommendation(rec, profile):\n",
    "                print(f\"   - {explanation}\")\n",
    "\n",
    "# Test the recommender\n",
    "test_recommender_with_sample_profiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce41a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate recommendation quality using historical data\n",
    "def evaluate_recommendations(recommender, test_data, user_profiles_df, top_n=5):\n",
    "    \"\"\"Evaluate recommendation quality against historical data\"\"\"\n",
    "    # Create actual good recommendations set\n",
    "    actual_good_recs = set(\n",
    "        test_data[test_data['good_recommendation'] == 1][['user_id', 'product_id']]\n",
    "        .apply(tuple, axis=1)\n",
    "    )\n",
    "    \n",
    "    predicted_good_recs = set()\n",
    "    total_users = len(test_data['user_id'].unique())\n",
    "    processed_users = 0\n",
    "    \n",
    "    # Track metrics\n",
    "    precision_at_k = []\n",
    "    recall_at_k = []\n",
    "    \n",
    "    # Evaluate for each user\n",
    "    for user_id in test_data['user_id'].unique():\n",
    "        # Get user profile\n",
    "        user_profile = user_profiles_df[user_profiles_df['user_id'] == user_id].iloc[0].to_dict()\n",
    "        \n",
    "        # Get actual good recommendations for this user\n",
    "        user_actual_good_products = set(\n",
    "            test_data[(test_data['user_id'] == user_id) & (test_data['good_recommendation'] == 1)]['product_id']\n",
    "        )\n",
    "        \n",
    "        # Skip users with no positive examples\n",
    "        if not user_actual_good_products:\n",
    "            continue\n",
    "        \n",
    "        # Get recommendations for this user\n",
    "        recs = recommender.recommend_products(user_profile, top_n=top_n)\n",
    "        recommended_products = {rec['product_id'] for rec in recs}\n",
    "        \n",
    "        # Add to predicted set\n",
    "        for product_id in recommended_products:\n",
    "            predicted_good_recs.add((user_id, product_id))\n",
    "        \n",
    "        # Calculate precision and recall for this user\n",
    "        relevant_and_recommended = len(user_actual_good_products.intersection(recommended_products))\n",
    "        precision = relevant_and_recommended / len(recommended_products) if recommended_products else 0\n",
    "        recall = relevant_and_recommended / len(user_actual_good_products) if user_actual_good_products else 0\n",
    "        \n",
    "        precision_at_k.append(precision)\n",
    "        recall_at_k.append(recall)\n",
    "        \n",
    "        processed_users += 1\n",
    "        if processed_users % 50 == 0:\n",
    "            print(f\"Processed {processed_users}/{total_users} users...\")\n",
    "    \n",
    "    # Calculate average precision and recall\n",
    "    avg_precision = np.mean(precision_at_k)\n",
    "    avg_recall = np.mean(recall_at_k)\n",
    "    f1_score = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0\n",
    "    \n",
    "    # Global precision and recall\n",
    "    true_positives = len(actual_good_recs.intersection(predicted_good_recs))\n",
    "    global_precision = true_positives / len(predicted_good_recs) if predicted_good_recs else 0\n",
    "    global_recall = true_positives / len(actual_good_recs) if actual_good_recs else 0\n",
    "    global_f1 = 2 * (global_precision * global_recall) / (global_precision + global_recall) if (global_precision + global_recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'per_user_metrics': {\n",
    "            'avg_precision': avg_precision,\n",
    "            'avg_recall': avg_recall,\n",
    "            'f1_score': f1_score\n",
    "        },\n",
    "        'global_metrics': {\n",
    "            'precision': global_precision,\n",
    "            'recall': global_recall,\n",
    "            'f1_score': global_f1\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Evaluate the recommendation engine\n",
    "print(\"Evaluating recommendation engine...\")\n",
    "eval_results = evaluate_recommendations(\n",
    "    recommender, \n",
    "    test_data=features_df.iloc[test_idx], \n",
    "    user_profiles_df=user_profiles_processed,\n",
    "    top_n=5\n",
    ")\n",
    "\n",
    "print(\"\\nPer-user Metrics:\")\n",
    "print(f\"Average Precision@5: {eval_results['per_user_metrics']['avg_precision']:.4f}\")\n",
    "print(f\"Average Recall@5: {eval_results['per_user_metrics']['avg_recall']:.4f}\")\n",
    "print(f\"F1 Score: {eval_results['per_user_metrics']['f1_score']:.4f}\")\n",
    "\n",
    "print(\"\\nGlobal Metrics:\")\n",
    "print(f\"Precision: {eval_results['global_metrics']['precision']:.4f}\")\n",
    "print(f\"Recall: {eval_results['global_metrics']['recall']:.4f}\")\n",
    "print(f\"F1 Score: {eval_results['global_metrics']['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fde3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output directory\n",
    "output_dir = '../../ai/models/generated/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save Random Forest model\n",
    "rf_model_path = os.path.join(output_dir, 'rf_recommendation_model.pkl')\n",
    "with open(rf_model_path, 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "print(f\"Saved Random Forest model to {rf_model_path}\")\n",
    "\n",
    "# Save Neural Network model\n",
    "nn_model_path = os.path.join(output_dir, 'nn_recommendation_model')\n",
    "nn_model.save(nn_model_path)\n",
    "print(f\"Saved Neural Network model to {nn_model_path}\")\n",
    "\n",
    "# Save KMeans model\n",
    "kmeans_model_path = os.path.join(output_dir, 'kmeans_product_clusters.pkl')\n",
    "with open(kmeans_model_path, 'wb') as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "print(f\"Saved KMeans model to {kmeans_model_path}\")\n",
    "\n",
    "# Save scalers\n",
    "scalers = {\n",
    "    'feature_scaler': scaler,\n",
    "    'user_scaler': user_scaler,\n",
    "    'product_scaler': product_scaler,\n",
    "    'product_cluster_scaler': product_cluster_scaler\n",
    "}\n",
    "scalers_path = os.path.join(output_dir, 'recommendation_scalers.pkl')\n",
    "with open(scalers_path, 'wb') as f:\n",
    "    pickle.dump(scalers, f)\n",
    "print(f\"Saved scalers to {scalers_path}\")\n",
    "\n",
    "# Save investment products dataset\n",
    "products_path = os.path.join(output_dir, 'investment_products.csv')\n",
    "investment_products_processed.to_csv(products_path, index=False)\n",
    "print(f\"Saved processed investment products to {products_path}\")\n",
    "\n",
    "# Create model metadata\n",
    "model_metadata = {\n",
    "    'version': '1.0.0',\n",
    "    'created_at': datetime.now().isoformat(),\n",
    "    'performance': {\n",
    "        'rf_accuracy': accuracy_score(y_test, y_pred_rf),\n",
    "        'rf_precision': classification_report(y_test, y_pred_rf, output_dict=True)['1']['precision'],\n",
    "        'rf_recall': classification_report(y_test, y_pred_rf, output_dict=True)['1']['recall'],\n",
    "        'nn_accuracy': nn_evaluation[1],\n",
    "        'recommendation_precision': eval_results['global_metrics']['precision'],\n",
    "        'recommendation_recall': eval_results['global_metrics']['recall'],\n",
    "        'recommendation_f1': eval_results['global_metrics']['f1_score']\n",
    "    },\n",
    "    'feature_importance': {\n",
    "        feature: importance for feature, importance in \n",
    "        zip(X.columns, rf_model.feature_importances_)\n",
    "    },\n",
    "    'product_clusters': {\n",
    "        str(i): list(investment_products_processed[investment_products_processed['cluster'] == i]['name'])\n",
    "        for i in range(optimal_k)\n",
    "    },\n",
    "    'files': {\n",
    "        'rf_model': os.path.basename(rf_model_path),\n",
    "        'nn_model': os.path.basename(nn_model_path),\n",
    "        'kmeans_model': os.path.basename(kmeans_model_path),\n",
    "        'scalers': os.path.basename(scalers_path),\n",
    "        'investment_products': os.path.basename(products_path)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save model metadata\n",
    "metadata_path = os.path.join(output_dir, 'recommendation_model_metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=4)\n",
    "print(f\"Saved model metadata to {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f08ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python module for loading and using the recommendation model\n",
    "recommender_module = f'''\n",
    "# investment_recommender.py\n",
    "# Recommendation engine for PesaGuru financial advisory chatbot\n",
    "# Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "class InvestmentRecommender:\n",
    "    \"\"\"Investment recommendation engine for PesaGuru chatbot\"\"\"\n",
    "    \n",
    "    def __init__(self, models_dir):\n",
    "        \"\"\"Initialize with trained models\"\"\"\n",
    "        # Load models\n",
    "        with open(os.path.join(models_dir, 'rf_recommendation_model.pkl'), 'rb') as f:\n",
    "            self.rf_model = pickle.load(f)\n",
    "        \n",
    "        self.nn_model = load_model(os.path.join(models_dir, 'nn_recommendation_model'))\n",
    "        \n",
    "        with open(os.path.join(models_dir, 'kmeans_product_clusters.pkl'), 'rb') as f:\n",
    "            self.product_clusters = pickle.load(f)\n",
    "        \n",
    "        # Load scalers\n",
    "        with open(os.path.join(models_dir, 'recommendation_scalers.pkl'), 'rb') as f:\n",
    "            scalers = pickle.load(f)\n",
    "            self.feature_scaler = scalers['feature_scaler']\n",
    "            self.user_scaler = scalers['user_scaler']\n",
    "            self.product_scaler = scalers['product_scaler']\n",
    "        \n",
    "        # Load products\n",
    "        import pandas as pd\n",
    "        self.products_df = pd.read_csv(os.path.join(models_dir, 'investment_products.csv'))\n",
    "    \n",
    "    def recommend_products(self, user_profile, top_n=5, diversify=True):\n",
    "        \"\"\"Generate personalized investment recommendations\"\"\"\n",
    "        # Extract key user features\n",
    "        user_features = np.array([[\n",
    "            user_profile['age_numeric'], \n",
    "            user_profile['income_numeric'], \n",
    "            user_profile['financial_literacy'],\n",
    "            user_profile['risk_tolerance'],\n",
    "            user_profile['horizon_numeric']\n",
    "        ]])\n",
    "        \n",
    "        # Scale user features\n",
    "        user_features_scaled = self.user_scaler.transform(user_features)\n",
    "        \n",
    "        # Initialize scores dictionary\n",
    "        product_scores = {}\n",
    "        \n",
    "        # Evaluate each product\n",
    "        for _, product in self.products_df.iterrows():\n",
    "            # Extract product features\n",
    "            product_features = np.array([[\n",
    "                product['risk_level'],\n",
    "                product['avg_annual_return'],\n",
    "                product['min_investment'],\n",
    "                product['liquidity_numeric'],\n",
    "                product['horizon_numeric']\n",
    "            ]])\n",
    "            \n",
    "            # Scale product features\n",
    "            product_features_scaled = self.product_scaler.transform(product_features)\n",
    "            \n",
    "            # Combined features for RF model\n",
    "            combined_features = np.array([[\n",
    "                user_profile['risk_tolerance'], \n",
    "                product['risk_level'],\n",
    "                abs(user_profile['risk_tolerance'] - product['risk_level']),  # risk_diff\n",
    "                user_profile['horizon_numeric'],\n",
    "                product['horizon_numeric'],\n",
    "                abs(user_profile['horizon_numeric'] - product['horizon_numeric']),  # horizon_diff\n",
    "                user_profile['age_numeric'],\n",
    "                user_profile['income_numeric'],\n",
    "                user_profile['financial_literacy'],\n",
    "                product['avg_annual_return'],\n",
    "                product['liquidity_numeric'],\n",
    "                product['risk_level'] / max(0.01, product['avg_annual_return']),  # risk_return_ratio\n",
    "                product['min_investment'] / max(1, user_profile['income_numeric']),  # affordability_ratio\n",
    "                50000,  # dummy investment_amount\n",
    "                12      # dummy duration_months\n",
    "            ]])\n",
    "            \n",
    "            # Scale combined features\n",
    "            combined_features_scaled = self.feature_scaler.transform(combined_features)\n",
    "            \n",
    "            # Get predictions from both models\n",
    "            rf_prob = self.rf_model.predict_proba(combined_features_scaled)[0, 1]\n",
    "            nn_prob = self.nn_model.predict([user_features_scaled, product_features_scaled])[0, 0]\n",
    "            \n",
    "            # Combine scores (weighted average)\n",
    "            combined_score = (rf_prob * 0.6) + (nn_prob * 0.4)\n",
    "            \n",
    "            # Store score\n",
    "            product_scores[product['product_id']] = {\n",
    "                'product_id': product['product_id'],\n",
    "                'name': product['name'],\n",
    "                'category': product['category'],\n",
    "                'risk_level': product['risk_level'],\n",
    "                'avg_annual_return': product['avg_annual_return'],\n",
    "                'min_investment': product['min_investment'],\n",
    "                'cluster': product['cluster'],\n",
    "                'score': combined_score\n",
    "            }\n",
    "        \n",
    "        # Get top recommendations\n",
    "        if diversify:\n",
    "            # Group by cluster\n",
    "            cluster_products = {}\n",
    "            for product_id, info in product_scores.items():\n",
    "                cluster = info['cluster']\n",
    "                if cluster not in cluster_products:\n",
    "                    cluster_products[cluster] = []\n",
    "                cluster_products[cluster].append(info)\n",
    "            \n",
    "            # Get top product from each cluster\n",
    "            recommendations = []\n",
    "            for cluster in sorted(cluster_products.keys()):\n",
    "                sorted_cluster_products = sorted(cluster_products[cluster], key=lambda x: x['score'], reverse=True)\n",
    "                if sorted_cluster_products:\n",
    "                    recommendations.append(sorted_cluster_products[0])\n",
    "            \n",
    "            # Sort final recommendations by score\n",
    "            recommendations = sorted(recommendations, key=lambda x: x['score'], reverse=True)[:top_n]\n",
    "        else:\n",
    "            # Simply get top N products by score\n",
    "            recommendations = sorted(\n",
    "                [info for _, info in product_scores.items()], \n",
    "                key=lambda x: x['score'], \n",
    "                reverse=True\n",
    "            )[:top_n]\n",
    "        \n",
    "        return recommendations\n",
    "        \n",
    "    def explain_recommendation(self, recommendation, user_profile):\n",
    "        \"\"\"Generate explanation for why a product was recommended\"\"\"\n",
    "        explanations = []\n",
    "        \n",
    "        # Risk tolerance alignment\n",
    "        risk_diff = abs(user_profile['risk_tolerance'] - recommendation['risk_level'])\n",
    "        if risk_diff <= 1:\n",
    "            explanations.append(f\"This investment aligns well with your risk tolerance level of {user_profile['risk_tolerance']}/5.\")\n",
    "        else:\n",
    "            risk_comparison = \"higher\" if recommendation['risk_level'] > user_profile['risk_tolerance'] else \"lower\"\n",
    "            explanations.append(f\"This investment has a {risk_comparison} risk level ({recommendation['risk_level']}/5) \"\n",
    "                               f\"compared to your risk tolerance ({user_profile['risk_tolerance']}/5).\")\n",
    "        \n",
    "        # Return potential\n",
    "        returns_percent = recommendation['avg_annual_return'] * 100\n",
    "        explanations.append(f\"It has an average annual return of {returns_percent:.1f}%.\")\n",
    "        \n",
    "        # Investment horizon\n",
    "        horizon_map = {1: 'Short-term', 2: 'Medium-term', 3: 'Long-term'}\n",
    "        user_horizon = horizon_map.get(user_profile['horizon_numeric'], 'Medium-term')\n",
    "        product_horizon = horizon_map.get(recommendation.get('horizon_numeric', 2), 'Medium-term')\n",
    "        \n",
    "        if user_horizon == product_horizon:\n",
    "            explanations.append(f\"This aligns with your {user_horizon.lower()} investment horizon.\")\n",
    "        else:\n",
    "            explanations.append(f\"While you indicated a {user_horizon.lower()} investment horizon, \"\n",
    "                              f\"this is a {product_horizon.lower()} investment.\")\n",
    "        \n",
    "        # Affordability\n",
    "        income_map = {1: '<15K', 2: '15K-30K', 3: '30K-50K', 4: '50K-100K', 5: '>100K'}\n",
    "        user_income = income_map.get(user_profile['income_numeric'], 'Medium')\n",
    "        \n",
    "        min_investment = recommendation['min_investment']\n",
    "        if min_investment <= 5000:\n",
    "            explanations.append(f\"With a minimum investment of KES {min_investment:,.0f}, this is accessible \"\n",
    "                              f\"for your income level ({user_income}).\")\n",
    "        elif min_investment > 50000 and user_profile['income_numeric'] < 4:\n",
    "            explanations.append(f\"The minimum investment of KES {min_investment:,.0f} may be significant \"\n",
    "                              f\"for your income level ({user_income}).\")\n",
    "        else:\n",
    "            explanations.append(f\"This requires a minimum investment of KES {min_investment:,.0f}.\")\n",
    "        \n",
    "        # Category specific explanation\n",
    "        category = recommendation['category']\n",
    "        if category == 'Equity':\n",
    "            explanations.append(\"Stocks provide opportunity for growth through capital appreciation and dividends.\")\n",
    "        elif category == 'Government Security':\n",
    "            explanations.append(\"Government securities offer stable returns with minimal risk.\")\n",
    "        elif category == 'Money Market':\n",
    "            explanations.append(\"Money market funds provide liquidity and stable short-term returns.\")\n",
    "        elif category == 'SACCO':\n",
    "            explanations.append(\"SACCOs offer competitive returns and may provide access to loans.\")\n",
    "        elif category == 'Real Estate':\n",
    "            explanations.append(\"Real estate investments can provide both rental income and capital appreciation.\")\n",
    "        \n",
    "        return explanations\n",
    "\n",
    "# Example usage:\n",
    "# recommender = InvestmentRecommender('/path/to/models')\n",
    "# user_profile = {\n",
    "#     'age_numeric': 2,\n",
    "#     'income_numeric': 3,\n",
    "#     'financial_literacy': 7,\n",
    "#     'risk_tolerance': 4,\n",
    "#     'horizon_numeric': 3\n",
    "# }\n",
    "# recommendations = recommender.recommend_products(user_profile)\n",
    "'''\n",
    "\n",
    "# Save the recommender module\n",
    "module_path = os.path.join(output_dir, 'investment_recommender.py')\n",
    "with open(module_path, 'w') as f:\n",
    "    f.write(recommender_module)\n",
    "print(f\"Saved recommender module to {module_path}\")\n",
    "\n",
    "# Create a requirements.txt for the recommendation model\n",
    "requirements = \"\"\"\n",
    "# Requirements for PesaGuru Investment Recommendation Model\n",
    "numpy>=1.19.5\n",
    "pandas>=1.3.0\n",
    "scikit-learn>=1.0.0\n",
    "tensorflow>=2.5.0\n",
    "matplotlib>=3.4.0\n",
    "seaborn>=0.11.0\n",
    "python-dotenv>=0.19.0\n",
    "requests>=2.25.0\n",
    "\"\"\"\n",
    "\n",
    "requirements_path = os.path.join(output_dir, 'requirements.txt')\n",
    "with open(requirements_path, 'w') as f:\n",
    "    f.write(requirements)\n",
    "print(f\"Saved requirements file to {requirements_path}\")\n",
    "\n",
    "# Create a simple README.md file for the model\n",
    "readme = f\"\"\"# PesaGuru Investment Recommendation Model\n",
    "\n",
    "Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Overview\n",
    "This recommendation model provides personalized investment advice for users in Kenya based on their risk profile, financial goals, and market conditions.\n",
    "\n",
    "## Models\n",
    "- **Random Forest Classifier**: Predicts if an investment product is suitable for a given user\n",
    "- **Neural Network**: Scores the compatibility between user profiles and investment products\n",
    "- **KMeans Clustering**: Groups similar investment products for portfolio diversification\n",
    "\n",
    "## Performance Metrics\n",
    "- Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\n",
    "- Precision: {eval_results['global_metrics']['precision']:.4f}\n",
    "- Recall: {eval_results['global_metrics']['recall']:.4f}\n",
    "- F1 Score: {eval_results['global_metrics']['f1_score']:.4f}\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from investment_recommender import InvestmentRecommender\n",
    "\n",
    "# Initialize recommender\n",
    "recommender = InvestmentRecommender('path/to/models')\n",
    "\n",
    "# User profile\n",
    "user_profile = {\n",
    "    'age_numeric': 2,\n",
    "    'income_numeric': 3,\n",
    "    'financial_literacy': 7,\n",
    "    'risk_tolerance': 4,\n",
    "    'horizon_numeric': 3\n",
    "}\n",
    "\n",
    "# Get recommendations\n",
    "recommendations = recommender.recommend_products(user_profile, top_n=3, diversify=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pesaguru",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
