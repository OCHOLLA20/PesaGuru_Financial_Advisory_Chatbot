{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "876516b5-cdc8-4103-9a79-fe256ea52784",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _pywrap_tfe: The specified procedure could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprophet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Prophet\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Deep Learning (if needed)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential, Model, load_model\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout, Input\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\tensorflow\\__init__.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     47\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\autograph\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\tensorflow\\python\\autograph\\core\\ag_ctx.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[0;32m     25\u001b[0m stacks \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\tensorflow\\python\\autograph\\utils\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_managers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_list\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\tensorflow\\python\\autograph\\utils\\context_managers.py:19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Various context managers.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcontrol_dependency_on_returns\u001b[39m(return_value):\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:46\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# pywrap_tensorflow must be imported first to avoid protobuf issues.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# (b/143110113)\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# pylint: disable=invalid-import-order,g-bad-import-order,unused-import\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# pylint: enable=invalid-import-order,g-bad-import-order,unused-import\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf2\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tfe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tfe: The specified procedure could not be found."
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Time Series libraries\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from prophet import Prophet\n",
    "\n",
    "# Deep Learning (if needed)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Set global parameters\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_style('whitegrid')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fe5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_DIR = '../../data/'\n",
    "SYNTHETIC_DATA_PATH = os.path.join(DATA_DIR, 'synthetic_transactions.csv')\n",
    "NSE_DATA_PATH = os.path.join(DATA_DIR, 'nse_historical.csv')\n",
    "MPESA_PATTERNS_PATH = os.path.join(DATA_DIR, 'mpesa_patterns.csv')\n",
    "\n",
    "# Function to check if data exists, otherwise generate synthetic data\n",
    "def get_transaction_data(path=SYNTHETIC_DATA_PATH, n_samples=10000, generate_if_missing=True):\n",
    "    \"\"\"Load transaction data or generate synthetic data if file doesn't exist\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Loading data from {path}\")\n",
    "        return pd.read_csv(path)\n",
    "    elif generate_if_missing:\n",
    "        print(f\"Generating synthetic data...\")\n",
    "        return generate_synthetic_transactions(n_samples, path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Data file not found at {path}\")\n",
    "\n",
    "# Generate synthetic transaction data with known anomalies\n",
    "def generate_synthetic_transactions(n_samples=10000, save_path=None):\n",
    "    \"\"\"Generate synthetic transaction data with embedded anomalies\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create date range\n",
    "    start_date = datetime(2024, 1, 1)\n",
    "    end_date = datetime(2025, 2, 28)\n",
    "    dates = [start_date + timedelta(days=x) for x in range((end_date - start_date).days)]\n",
    "    \n",
    "    # Transaction types\n",
    "    transaction_types = ['deposit', 'withdrawal', 'transfer', 'payment', 'loan_repayment']\n",
    "    \n",
    "    # Generate normal transactions\n",
    "    data = {\n",
    "        'transaction_id': [f'TXN{i:06d}' for i in range(n_samples)],\n",
    "        'date': np.random.choice(dates, n_samples),\n",
    "        'customer_id': np.random.randint(1000, 9999, n_samples),\n",
    "        'transaction_type': np.random.choice(transaction_types, n_samples, \n",
    "                                            p=[0.3, 0.25, 0.2, 0.15, 0.1]),\n",
    "        'amount': np.random.lognormal(mean=8, sigma=1, size=n_samples),  # KES amounts\n",
    "        'location': np.random.choice(['Nairobi', 'Mombasa', 'Kisumu', 'Nakuru', 'Eldoret'], n_samples),\n",
    "        'is_anomaly': np.zeros(n_samples, dtype=int)  # 0 = normal transaction\n",
    "    }\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Format amount to be more realistic (Kenyan Shillings)\n",
    "    df['amount'] = df['amount'].round(2)\n",
    "    \n",
    "    # Generate anomalies (5% of data)\n",
    "    anomaly_count = int(n_samples * 0.05)\n",
    "    anomaly_indices = np.random.choice(n_samples, anomaly_count, replace=False)\n",
    "    \n",
    "    # Anomaly type 1: Unusually large transactions\n",
    "    large_txn_indices = anomaly_indices[:anomaly_count//3]\n",
    "    df.loc[large_txn_indices, 'amount'] = df.loc[large_txn_indices, 'amount'] * np.random.uniform(10, 20, len(large_txn_indices))\n",
    "    df.loc[large_txn_indices, 'is_anomaly'] = 1\n",
    "    \n",
    "    # Anomaly type 2: Unusual transaction frequency\n",
    "    freq_anomaly_customers = df.loc[anomaly_indices[anomaly_count//3:2*anomaly_count//3], 'customer_id'].unique()\n",
    "    for customer in freq_anomaly_customers:\n",
    "        # Add multiple transactions in short time period\n",
    "        customer_idx = df[df['customer_id'] == customer].index[0]\n",
    "        anomaly_date = df.loc[customer_idx, 'date']\n",
    "        \n",
    "        # Mark original transaction as anomaly\n",
    "        df.loc[customer_idx, 'is_anomaly'] = 1\n",
    "    \n",
    "    # Anomaly type 3: Transactions from unusual locations\n",
    "    location_anomaly_indices = anomaly_indices[2*anomaly_count//3:]\n",
    "    df.loc[location_anomaly_indices, 'location'] = 'International'\n",
    "    df.loc[location_anomaly_indices, 'is_anomaly'] = 1\n",
    "    \n",
    "    # Convert date to string for CSV storage\n",
    "    df['date'] = df['date'].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Save to file if path provided\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"Synthetic data saved to {save_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load or generate transaction data\n",
    "transactions_df = get_transaction_data()\n",
    "\n",
    "# Display the first few rows\n",
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a0797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_transaction_data(df):\n",
    "    \"\"\"Preprocess transaction data for anomaly detection\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Convert date to datetime\n",
    "    df_processed['date'] = pd.to_datetime(df_processed['date'])\n",
    "    \n",
    "    # Extract date features\n",
    "    df_processed['day_of_week'] = df_processed['date'].dt.dayofweek\n",
    "    df_processed['day_of_month'] = df_processed['date'].dt.day\n",
    "    df_processed['month'] = df_processed['date'].dt.month\n",
    "    df_processed['year'] = df_processed['date'].dt.year\n",
    "    \n",
    "    # Create transaction frequency features\n",
    "    customer_txn_counts = df_processed.groupby('customer_id').size().reset_index(name='customer_txn_count')\n",
    "    df_processed = pd.merge(df_processed, customer_txn_counts, on='customer_id', how='left')\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    df_processed = pd.get_dummies(df_processed, columns=['transaction_type', 'location'], drop_first=False)\n",
    "    \n",
    "    # For privacy and to focus on patterns, remove transaction_id\n",
    "    if 'transaction_id' in df_processed.columns:\n",
    "        df_processed.drop('transaction_id', axis=1, inplace=True)\n",
    "        \n",
    "    return df_processed\n",
    "\n",
    "# Preprocess the data\n",
    "processed_df = preprocess_transaction_data(transactions_df)\n",
    "\n",
    "# Display the processed data\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d3b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"Engineer additional features for anomaly detection\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_featured = df.copy()\n",
    "    \n",
    "    # Transaction amount statistics per customer\n",
    "    customer_amount_stats = df_featured.groupby('customer_id')['amount'].agg(['mean', 'std', 'min', 'max']).reset_index()\n",
    "    customer_amount_stats.columns = ['customer_id', 'customer_mean_amount', 'customer_std_amount', \n",
    "                                     'customer_min_amount', 'customer_max_amount']\n",
    "    \n",
    "    # Add a small epsilon to std to avoid division by zero\n",
    "    customer_amount_stats['customer_std_amount'] = customer_amount_stats['customer_std_amount'].fillna(0) + 1e-6\n",
    "    \n",
    "    # Merge these statistics back to the main dataframe\n",
    "    df_featured = pd.merge(df_featured, customer_amount_stats, on='customer_id', how='left')\n",
    "    \n",
    "    # Calculate z-score of transaction amount for each customer\n",
    "    df_featured['amount_zscore'] = (df_featured['amount'] - df_featured['customer_mean_amount']) / df_featured['customer_std_amount']\n",
    "    \n",
    "    # Calculate transaction recency (days since first transaction)\n",
    "    customer_first_txn = df_featured.groupby('customer_id')['date'].min().reset_index()\n",
    "    customer_first_txn.columns = ['customer_id', 'first_transaction_date']\n",
    "    df_featured = pd.merge(df_featured, customer_first_txn, on='customer_id', how='left')\n",
    "    df_featured['days_since_first_txn'] = (df_featured['date'] - df_featured['first_transaction_date']).dt.days\n",
    "    \n",
    "    # Drop intermediate columns\n",
    "    df_featured.drop('first_transaction_date', axis=1, inplace=True)\n",
    "    \n",
    "    return df_featured\n",
    "\n",
    "def normalize_features(df, features_to_normalize=None):\n",
    "    \"\"\"Normalize numerical features\"\"\"\n",
    "    df_normalized = df.copy()\n",
    "    \n",
    "    if features_to_normalize is None:\n",
    "        # Default features to normalize\n",
    "        features_to_normalize = ['amount', 'customer_txn_count', 'customer_mean_amount', \n",
    "                                 'customer_std_amount', 'customer_min_amount', 'customer_max_amount',\n",
    "                                 'amount_zscore', 'days_since_first_txn']\n",
    "    \n",
    "    # Filter to include only columns that exist in the dataframe\n",
    "    features_to_normalize = [f for f in features_to_normalize if f in df_normalized.columns]\n",
    "    \n",
    "    if features_to_normalize:\n",
    "        scaler = StandardScaler()\n",
    "        df_normalized[features_to_normalize] = scaler.fit_transform(df_normalized[features_to_normalize])\n",
    "    \n",
    "    return df_normalized, scaler\n",
    "\n",
    "# Engineer features\n",
    "featured_df = engineer_features(processed_df)\n",
    "\n",
    "# Normalize features\n",
    "normalized_df, scaler = normalize_features(featured_df)\n",
    "\n",
    "# Display the enriched dataset\n",
    "normalized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc93e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transaction_distributions(df):\n",
    "    \"\"\"Plot distributions of key transaction features\"\"\"\n",
    "    # Set up the figure\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Transaction amount distribution\n",
    "    sns.histplot(data=df, x='amount', hue='is_anomaly', bins=50, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Transaction Amount Distribution', fontsize=14)\n",
    "    axes[0, 0].set_xlabel('Amount (KES)', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Count', fontsize=12)\n",
    "    \n",
    "    # Transaction type distribution\n",
    "    txn_type_counts = df.groupby(['transaction_type', 'is_anomaly']).size().unstack().fillna(0)\n",
    "    txn_type_counts.plot(kind='bar', stacked=True, ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Transaction Types', fontsize=14)\n",
    "    axes[0, 1].set_xlabel('Transaction Type', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Count', fontsize=12)\n",
    "    \n",
    "    # Transaction by day of week\n",
    "    dow_counts = df.groupby(['day_of_week', 'is_anomaly']).size().unstack().fillna(0)\n",
    "    dow_counts.plot(kind='bar', stacked=True, ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Transactions by Day of Week', fontsize=14)\n",
    "    axes[1, 0].set_xlabel('Day of Week (0=Monday, 6=Sunday)', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Count', fontsize=12)\n",
    "    \n",
    "    # Location distribution\n",
    "    loc_counts = df.groupby(['location', 'is_anomaly']).size().unstack().fillna(0)\n",
    "    loc_counts.plot(kind='bar', stacked=True, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Transactions by Location', fontsize=14)\n",
    "    axes[1, 1].set_xlabel('Location', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Count', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_anomaly_zscore_distribution(df):\n",
    "    \"\"\"Plot Z-score distribution for normal vs anomalous transactions\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot Z-score distributions\n",
    "    sns.kdeplot(data=df[df['is_anomaly']==0], x='amount_zscore', label='Normal Transactions', fill=True)\n",
    "    sns.kdeplot(data=df[df['is_anomaly']==1], x='amount_zscore', label='Anomalous Transactions', fill=True)\n",
    "    \n",
    "    plt.title('Z-score Distribution: Normal vs Anomalous Transactions', fontsize=14)\n",
    "    plt.xlabel('Transaction Amount Z-score', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "# Create and display distribution plots\n",
    "dist_fig = plot_transaction_distributions(featured_df)\n",
    "plt.show()\n",
    "\n",
    "# Plot Z-score distributions\n",
    "zscore_fig = plot_anomaly_zscore_distribution(featured_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8834a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(df):\n",
    "    \"\"\"Plot correlation matrix of numerical features\"\"\"\n",
    "    # Select only numeric columns\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Drop the label column for correlation analysis\n",
    "    if 'is_anomaly' in numeric_df.columns:\n",
    "        numeric_df_no_label = numeric_df.drop('is_anomaly', axis=1)\n",
    "    else:\n",
    "        numeric_df_no_label = numeric_df\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = numeric_df_no_label.corr()\n",
    "    \n",
    "    # Plot the correlation matrix\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', linewidths=0.5)\n",
    "    plt.title('Feature Correlation Matrix', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "def plot_feature_vs_anomaly(df):\n",
    "    \"\"\"Plot relationship between main features and anomaly label\"\"\"\n",
    "    # Calculate correlation with anomaly label\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    anomaly_corr = numeric_df.corr()['is_anomaly'].sort_values(ascending=False)\n",
    "    \n",
    "    # Plot top correlating features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x=anomaly_corr.index[:10], y=anomaly_corr.values[:10])\n",
    "    plt.title('Top Features Correlated with Anomaly Label', fontsize=14)\n",
    "    plt.xlabel('Features', fontsize=12)\n",
    "    plt.ylabel('Correlation Coefficient', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf(), anomaly_corr\n",
    "\n",
    "# Plot correlation matrix\n",
    "corr_fig = plot_correlation_matrix(featured_df)\n",
    "plt.show()\n",
    "\n",
    "# Plot feature vs anomaly correlation\n",
    "anomaly_corr_fig, anomaly_corr = plot_feature_vs_anomaly(featured_df)\n",
    "plt.show()\n",
    "\n",
    "# Display top correlations with anomaly label\n",
    "print(\"Top features correlated with anomalies:\")\n",
    "print(anomaly_corr.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668708cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_zscore(df, zscore_col='amount_zscore', threshold=3.0):\n",
    "    \"\"\"Detect anomalies using Z-score thresholding\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # Mark anomalies based on absolute Z-score exceeding threshold\n",
    "    df_result['zscore_anomaly'] = (abs(df_result[zscore_col]) > threshold).astype(int)\n",
    "    \n",
    "    # Count anomalies detected\n",
    "    anomaly_count = df_result['zscore_anomaly'].sum()\n",
    "    total_count = len(df_result)\n",
    "    \n",
    "    print(f\"Z-score method detected {anomaly_count} anomalies ({anomaly_count/total_count:.2%} of data)\")\n",
    "    \n",
    "    # If ground truth is available, calculate accuracy\n",
    "    if 'is_anomaly' in df_result.columns:\n",
    "        accuracy = (df_result['zscore_anomaly'] == df_result['is_anomaly']).mean()\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            df_result['is_anomaly'], \n",
    "            df_result['zscore_anomaly'], \n",
    "            average='binary'\n",
    "        )\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(df_result['is_anomaly'], df_result['zscore_anomaly'])\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "# Apply Z-score anomaly detection\n",
    "zscore_results = detect_anomalies_zscore(featured_df)\n",
    "\n",
    "# Visualize Z-score anomalies\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(zscore_results['amount'], zscore_results['amount_zscore'], \n",
    "           c=zscore_results['zscore_anomaly'], cmap='coolwarm', alpha=0.7)\n",
    "plt.axhline(y=3, color='r', linestyle='--', alpha=0.5, label='Threshold (+3)')\n",
    "plt.axhline(y=-3, color='r', linestyle='--', alpha=0.5, label='Threshold (-3)')\n",
    "plt.colorbar(label='Anomaly')\n",
    "plt.title('Z-score Anomaly Detection', fontsize=14)\n",
    "plt.xlabel('Transaction Amount (KES)', fontsize=12)\n",
    "plt.ylabel('Z-score', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd72a99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_isolation_forest(df, features=None, contamination=0.05):\n",
    "    \"\"\"Detect anomalies using Isolation Forest algorithm\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    if features is None:\n",
    "        # Default features to use\n",
    "        numerical_cols = df_result.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        features = [col for col in numerical_cols if col not in ['is_anomaly', 'zscore_anomaly', 'customer_id']]\n",
    "    \n",
    "    # Initialize and fit the model\n",
    "    iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "    iso_forest.fit(df_result[features])\n",
    "    \n",
    "    # Predict anomalies\n",
    "    df_result['isolation_forest_score'] = iso_forest.decision_function(df_result[features])\n",
    "    df_result['isolation_forest_anomaly'] = (iso_forest.predict(df_result[features]) == -1).astype(int)\n",
    "    \n",
    "    # Count anomalies detected\n",
    "    anomaly_count = df_result['isolation_forest_anomaly'].sum()\n",
    "    total_count = len(df_result)\n",
    "    \n",
    "    print(f\"Isolation Forest detected {anomaly_count} anomalies ({anomaly_count/total_count:.2%} of data)\")\n",
    "    \n",
    "    # If ground truth is available, calculate accuracy\n",
    "    if 'is_anomaly' in df_result.columns:\n",
    "        accuracy = (df_result['isolation_forest_anomaly'] == df_result['is_anomaly']).mean()\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            df_result['is_anomaly'], \n",
    "            df_result['isolation_forest_anomaly'], \n",
    "            average='binary'\n",
    "        )\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(df_result['is_anomaly'], df_result['isolation_forest_anomaly'])\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "    \n",
    "    return df_result, iso_forest\n",
    "\n",
    "# Apply Isolation Forest anomaly detection\n",
    "iforest_results, iforest_model = detect_anomalies_isolation_forest(featured_df)\n",
    "\n",
    "# Visualize Isolation Forest anomalies\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(iforest_results['amount'], iforest_results['customer_mean_amount'], \n",
    "           c=iforest_results['isolation_forest_anomaly'], cmap='coolwarm', alpha=0.7)\n",
    "plt.colorbar(label='Anomaly')\n",
    "plt.title('Isolation Forest Anomaly Detection', fontsize=14)\n",
    "plt.xlabel('Transaction Amount (KES)', fontsize=12)\n",
    "plt.ylabel('Customer Average Transaction Amount (KES)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90af0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_lof(df, features=None, n_neighbors=20, contamination=0.05):\n",
    "    \"\"\"Detect anomalies using Local Outlier Factor algorithm\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    if features is None:\n",
    "        # Default features to use\n",
    "        numerical_cols = df_result.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        features = [col for col in numerical_cols if col not in ['is_anomaly', 'zscore_anomaly', \n",
    "                                                                'isolation_forest_anomaly', 'isolation_forest_score', 'customer_id']]\n",
    "    \n",
    "    # Initialize and fit the model\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)\n",
    "    \n",
    "    # Predict anomalies\n",
    "    df_result['lof_anomaly'] = (lof.fit_predict(df_result[features]) == -1).astype(int)\n",
    "    \n",
    "    # Get LOF scores (negative of outlier factor)\n",
    "    lof_scores = -lof.negative_outlier_factor_\n",
    "    df_result['lof_score'] = lof_scores\n",
    "    \n",
    "    # Count anomalies detected\n",
    "    anomaly_count = df_result['lof_anomaly'].sum()\n",
    "    total_count = len(df_result)\n",
    "    \n",
    "    print(f\"LOF detected {anomaly_count} anomalies ({anomaly_count/total_count:.2%} of data)\")\n",
    "    \n",
    "    # If ground truth is available, calculate accuracy\n",
    "    if 'is_anomaly' in df_result.columns:\n",
    "        accuracy = (df_result['lof_anomaly'] == df_result['is_anomaly']).mean()\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            df_result['is_anomaly'], \n",
    "            df_result['lof_anomaly'], \n",
    "            average='binary'\n",
    "        )\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(df_result['is_anomaly'], df_result['lof_anomaly'])\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "# Apply LOF anomaly detection\n",
    "lof_results = detect_anomalies_lof(featured_df)\n",
    "\n",
    "# Visualize LOF anomalies\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(lof_results['amount'], lof_results['lof_score'], \n",
    "           c=lof_results['lof_anomaly'], cmap='coolwarm', alpha=0.7)\n",
    "plt.colorbar(label='Anomaly')\n",
    "plt.title('Local Outlier Factor (LOF) Anomaly Detection', fontsize=14)\n",
    "plt.xlabel('Transaction Amount (KES)', fontsize=12)\n",
    "plt.ylabel('LOF Score', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7545503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(input_dim, encoding_dim=10, activation='relu'):\n",
    "    \"\"\"Create a simple autoencoder model\"\"\"\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Encoder\n",
    "    encoded = Dense(encoding_dim * 2, activation=activation)(input_layer)\n",
    "    encoded = Dense(encoding_dim, activation=activation)(encoded)\n",
    "    \n",
    "    # Decoder\n",
    "    decoded = Dense(encoding_dim * 2, activation=activation)(encoded)\n",
    "    decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "    \n",
    "    # Autoencoder model\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    \n",
    "    # Encoder model\n",
    "    encoder = Model(input_layer, encoded)\n",
    "    \n",
    "    # Compile model\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    return autoencoder, encoder\n",
    "\n",
    "def detect_anomalies_autoencoder(df, features=None, encoding_dim=10, epochs=50, batch_size=32, threshold_percentile=95):\n",
    "    \"\"\"Detect anomalies using Autoencoder reconstruction error\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    if features is None:\n",
    "        # Default features to use\n",
    "        numerical_cols = df_result.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        features = [col for col in numerical_cols if col not in ['is_anomaly', 'zscore_anomaly', \n",
    "                                                               'isolation_forest_anomaly', 'isolation_forest_score',\n",
    "                                                               'lof_anomaly', 'lof_score', 'customer_id']]\n",
    "    \n",
    "    # Split data for training (use only normal transactions for training if ground truth is available)\n",
    "    if 'is_anomaly' in df_result.columns:\n",
    "        train_data = df_result[df_result['is_anomaly'] == 0][features].values\n",
    "    else:\n",
    "        # If no ground truth, use all data (less effective, but still works)\n",
    "        train_data = df_result[features].values\n",
    "    \n",
    "    # Create and train autoencoder\n",
    "    autoencoder, encoder = create_autoencoder(input_dim=len(features), encoding_dim=encoding_dim)\n",
    "    \n",
    "    # Train the model\n",
    "    history = autoencoder.fit(\n",
    "        train_data, train_data,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        validation_split=0.1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Autoencoder Training History', fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Get reconstruction error\n",
    "    test_data = df_result[features].values\n",
    "    reconstructions = autoencoder.predict(test_data)\n",
    "    mse = np.mean(np.power(test_data - reconstructions, 2), axis=1)\n",
    "    df_result['autoencoder_error'] = mse\n",
    "    \n",
    "    # Set threshold based on percentile of reconstruction error\n",
    "    threshold = np.percentile(mse, threshold_percentile)\n",
    "    df_result['autoencoder_anomaly'] = (df_result['autoencoder_error'] > threshold).astype(int)\n",
    "    \n",
    "    # Count anomalies detected\n",
    "    anomaly_count = df_result['autoencoder_anomaly'].sum()\n",
    "    total_count = len(df_result)\n",
    "    \n",
    "    print(f\"Autoencoder detected {anomaly_count} anomalies ({anomaly_count/total_count:.2%} of data)\")\n",
    "    print(f\"Reconstruction error threshold: {threshold:.6f}\")\n",
    "    \n",
    "    # If ground truth is available, calculate accuracy\n",
    "    if 'is_anomaly' in df_result.columns:\n",
    "        accuracy = (df_result['autoencoder_anomaly'] == df_result['is_anomaly']).mean()\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            df_result['is_anomaly'], \n",
    "            df_result['autoencoder_anomaly'], \n",
    "            average='binary'\n",
    "        )\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(df_result['is_anomaly'], df_result['autoencoder_anomaly'])\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "    \n",
    "    return df_result, autoencoder, encoder\n",
    "\n",
    "# Apply Autoencoder anomaly detection\n",
    "autoencoder_results, autoencoder_model, encoder_model = detect_anomalies_autoencoder(featured_df)\n",
    "\n",
    "# Visualize Autoencoder anomalies\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(autoencoder_results['amount'], autoencoder_results['autoencoder_error'], \n",
    "           c=autoencoder_results['autoencoder_anomaly'], cmap='coolwarm', alpha=0.7)\n",
    "plt.axhline(y=autoencoder_results['autoencoder_error'].quantile(0.95), color='r', \n",
    "           linestyle='--', alpha=0.5, label='Threshold (95th percentile)')\n",
    "plt.colorbar(label='Anomaly')\n",
    "plt.title('Autoencoder Anomaly Detection', fontsize=14)\n",
    "plt.xlabel('Transaction Amount (KES)', fontsize=12)\n",
    "plt.ylabel('Reconstruction Error', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c7efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_nse_data(n_days=365, save_path=None):\n",
    "    \"\"\"Generate synthetic Nairobi Securities Exchange (NSE) data with anomalies\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create date range\n",
    "    end_date = datetime(2025, 3, 1)\n",
    "    start_date = end_date - timedelta(days=n_days)\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='B')  # Business days only\n",
    "    \n",
    "    # Kenyan stock tickers - major companies on NSE\n",
    "    tickers = ['SCOM', 'EQTY', 'KCB', 'COOP', 'SBIC', 'BAT', 'EABL']\n",
    "    \n",
    "    # Generate data for multiple stocks\n",
    "    all_stock_data = []\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        # Base parameters\n",
    "        initial_price = np.random.uniform(50, 500)  # Initial stock price (KES)\n",
    "        daily_volatility = np.random.uniform(0.01, 0.03)  # Daily volatility\n",
    "        drift = np.random.uniform(0.0001, 0.0005)  # Small upward drift\n",
    "        \n",
    "        # Generate prices using random walk with drift\n",
    "        prices = [initial_price]\n",
    "        returns = np.random.normal(drift, daily_volatility, len(dates) - 1)\n",
    "        \n",
    "        for ret in returns:\n",
    "            prices.append(prices[-1] * (1 + ret))\n",
    "        \n",
    "        # Generate volume\n",
    "        base_volume = np.random.uniform(100000, 2000000)  # Base volume\n",
    "        volumes = np.random.lognormal(mean=np.log(base_volume), sigma=0.5, size=len(dates))\n",
    "        \n",
    "        # Introduce anomalies\n",
    "        is_anomaly = np.zeros(len(dates))\n",
    "        \n",
    "        # Anomaly type 1: Price jumps (5-10 instances)\n",
    "        num_price_anomalies = np.random.randint(5, 11)\n",
    "        price_anomaly_indices = np.random.choice(range(len(dates)), num_price_anomalies, replace=False)\n",
    "        \n",
    "        for idx in price_anomaly_indices:\n",
    "            # Create a price jump/drop (±10-20%)\n",
    "            direction = np.random.choice([-1, 1])\n",
    "            magnitude = np.random.uniform(0.1, 0.2)\n",
    "            prices[idx] = prices[idx] * (1 + direction * magnitude)\n",
    "            is_anomaly[idx] = 1\n",
    "        \n",
    "        # Anomaly type 2: Volume spikes (3-7 instances)\n",
    "        num_volume_anomalies = np.random.randint(3, 8)\n",
    "        volume_anomaly_indices = np.random.choice(\n",
    "            [i for i in range(len(dates)) if i not in price_anomaly_indices],\n",
    "            num_volume_anomalies, replace=False\n",
    "        )\n",
    "        \n",
    "        for idx in volume_anomaly_indices:\n",
    "            # Create a volume spike (3-10x normal)\n",
    "            volumes[idx] = volumes[idx] * np.random.uniform(3, 10)\n",
    "            is_anomaly[idx] = 1\n",
    "        \n",
    "        # Create dataframe for this stock\n",
    "        stock_data = pd.DataFrame({\n",
    "            'date': dates,\n",
    "            'ticker': ticker,\n",
    "            'open': prices,\n",
    "            'high': [p * (1 + np.random.uniform(0, 0.02)) for p in prices],\n",
    "            'low': [p * (1 - np.random.uniform(0, 0.02)) for p in prices],\n",
    "            'close': [p * (1 + np.random.uniform(-0.01, 0.01)) for p in prices],\n",
    "            'volume': volumes,\n",
    "            'is_anomaly': is_anomaly\n",
    "        })\n",
    "        \n",
    "        all_stock_data.append(stock_data)\n",
    "    \n",
    "    # Combine all stocks into one dataframe\n",
    "    nse_data = pd.concat(all_stock_data, ignore_index=True)\n",
    "    \n",
    "    # Save to file if path provided\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        nse_data.to_csv(save_path, index=False)\n",
    "        print(f\"Synthetic NSE data saved to {save_path}\")\n",
    "    \n",
    "    return nse_data\n",
    "\n",
    "# Generate or load NSE data\n",
    "if os.path.exists(NSE_DATA_PATH):\n",
    "    nse_data = pd.read_csv(NSE_DATA_PATH)\n",
    "    print(f\"Loaded NSE data from {NSE_DATA_PATH}\")\n",
    "else:\n",
    "    nse_data = generate_synthetic_nse_data(save_path=NSE_DATA_PATH)\n",
    "    print(f\"Generated synthetic NSE data\")\n",
    "\n",
    "# Convert date to datetime\n",
    "nse_data['date'] = pd.to_datetime(nse_data['date'])\n",
    "\n",
    "# Display a sample of the data\n",
    "nse_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7589e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stock_data(df, ticker='SCOM'):\n",
    "    \"\"\"Plot stock price and volume with anomalies highlighted\"\"\"\n",
    "    # Filter data for the selected ticker\n",
    "    stock_df = df[df['ticker'] == ticker].sort_values('date')\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "    \n",
    "    # Plot price data\n",
    "    ax1.plot(stock_df['date'], stock_df['close'], label='Close Price', color='blue')\n",
    "    ax1.scatter(stock_df[stock_df['is_anomaly'] == 1]['date'], \n",
    "              stock_df[stock_df['is_anomaly'] == 1]['close'], \n",
    "              color='red', s=50, label='Anomaly')\n",
    "    ax1.set_title(f'{ticker} Stock Price (KES)', fontsize=16)\n",
    "    ax1.set_ylabel('Price (KES)', fontsize=14)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot volume data\n",
    "    ax2.bar(stock_df['date'], stock_df['volume'], color='blue', alpha=0.5)\n",
    "    ax2.scatter(stock_df[stock_df['is_anomaly'] == 1]['date'], \n",
    "              stock_df[stock_df['is_anomaly'] == 1]['volume'], \n",
    "              color='red', s=50, label='Anomaly')\n",
    "    ax2.set_title(f'{ticker} Trading Volume', fontsize=16)\n",
    "    ax2.set_xlabel('Date', fontsize=14)\n",
    "    ax2.set_ylabel('Volume', fontsize=14)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot example stock data\n",
    "safcom_fig = plot_stock_data(nse_data, 'SCOM')\n",
    "plt.show()\n",
    "\n",
    "equity_fig = plot_stock_data(nse_data, 'EQTY')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58fb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_arima(stock_df, price_col='close', confidence=0.95):\n",
    "    \"\"\"Detect anomalies in stock prices using ARIMA model\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_result = stock_df.copy()\n",
    "    \n",
    "    # Prepare time series data\n",
    "    df_result = df_result.sort_values('date')\n",
    "    ts_data = df_result[price_col]\n",
    "    \n",
    "    # Fit ARIMA model - find optimal order using AIC\n",
    "    best_aic = float('inf')\n",
    "    best_order = None\n",
    "    best_model = None\n",
    "    \n",
    "    # Try different ARIMA parameters (simplified version for demonstration)\n",
    "    for p in range(0, 3):\n",
    "        for d in range(0, 2):\n",
    "            for q in range(0, 3):\n",
    "                try:\n",
    "                    model = ARIMA(ts_data, order=(p, d, q))\n",
    "                    model_fit = model.fit()\n",
    "                    aic = model_fit.aic\n",
    "                    \n",
    "                    if aic < best_aic:\n",
    "                        best_aic = aic\n",
    "                        best_order = (p, d, q)\n",
    "                        best_model = model_fit\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    if best_model is None:\n",
    "        print(\"Could not fit ARIMA model. Using default parameters.\")\n",
    "        model = ARIMA(ts_data, order=(1, 1, 1))\n",
    "        best_model = model.fit()\n",
    "        best_order = (1, 1, 1)\n",
    "    \n",
    "    print(f\"Best ARIMA order: {best_order}, AIC: {best_aic:.2f}\")\n",
    "    \n",
    "    # Get predictions and confidence intervals\n",
    "    predictions = best_model.fittedvalues\n",
    "    resid = best_model.resid\n",
    "    sigma = np.std(resid)\n",
    "    \n",
    "    # Calculate confidence intervals\n",
    "    from scipy.stats import norm\n",
    "    z_score = norm.ppf(1 - (1 - confidence) / 2)\n",
    "    lower_bound = predictions - z_score * sigma\n",
    "    upper_bound = predictions + z_score * sigma\n",
    "    \n",
    "    # Detect anomalies\n",
    "    df_result['arima_prediction'] = predictions\n",
    "    df_result['arima_residual'] = resid\n",
    "    df_result['arima_lower_bound'] = lower_bound\n",
    "    df_result['arima_upper_bound'] = upper_bound\n",
    "    df_result['arima_anomaly'] = ((df_result[price_col] < df_result['arima_lower_bound']) | \n",
    "                                 (df_result[price_col] > df_result['arima_upper_bound'])).astype(int)\n",
    "    \n",
    "    # Count anomalies detected\n",
    "    anomaly_count = df_result['arima_anomaly'].sum()\n",
    "    total_count = len(df_result)\n",
    "    \n",
    "    print(f\"ARIMA model detected {anomaly_count} anomalies ({anomaly_count/total_count:.2%} of data)\")\n",
    "    \n",
    "    # If ground truth is available, calculate accuracy\n",
    "    if 'is_anomaly' in df_result.columns:\n",
    "        accuracy = (df_result['arima_anomaly'] == df_result['is_anomaly']).mean()\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            df_result['is_anomaly'], \n",
    "            df_result['arima_anomaly'], \n",
    "            average='binary'\n",
    "        )\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(df_result['is_anomaly'], df_result['arima_anomaly'])\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "    \n",
    "    return df_result, best_model\n",
    "\n",
    "# Run ARIMA anomaly detection on SCOM stock\n",
    "scom_data = nse_data[nse_data['ticker'] == 'SCOM'].sort_values('date')\n",
    "scom_arima_results, scom_arima_model = detect_anomalies_arima(scom_data)\n",
    "\n",
    "# Visualize ARIMA anomaly detection results\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(scom_arima_results['date'], scom_arima_results['close'], label='Actual Price', color='blue')\n",
    "plt.plot(scom_arima_results['date'], scom_arima_results['arima_prediction'], label='ARIMA Prediction', color='green', alpha=0.7)\n",
    "plt.fill_between(scom_arima_results['date'], \n",
    "                scom_arima_results['arima_lower_bound'], \n",
    "                scom_arima_results['arima_upper_bound'], \n",
    "                color='green', alpha=0.1, label='Confidence Interval')\n",
    "plt.scatter(scom_arima_results[scom_arima_results['arima_anomaly'] == 1]['date'],\n",
    "           scom_arima_results[scom_arima_results['arima_anomaly'] == 1]['close'],\n",
    "           color='red', s=50, label='ARIMA Detected Anomaly')\n",
    "plt.scatter(scom_arima_results[scom_arima_results['is_anomaly'] == 1]['date'],\n",
    "           scom_arima_results[scom_arima_results['is_anomaly'] == 1]['close'],\n",
    "           marker='X', color='purple', s=100, label='True Anomaly')\n",
    "plt.title('ARIMA Anomaly Detection - SCOM Stock Price', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Price (KES)', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7fbb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_methods(df, methods_cols, ground_truth='is_anomaly'):\n",
    "    \"\"\"Compare different anomaly detection methods\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for method in methods_cols:\n",
    "        if method not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Calculate metrics\n",
    "        accuracy = (df[method] == df[ground_truth]).mean()\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            df[ground_truth], \n",
    "            df[method], \n",
    "            average='binary'\n",
    "        )\n",
    "        \n",
    "        # Count detected anomalies\n",
    "        anomaly_count = df[method].sum()\n",
    "        total_count = len(df)\n",
    "        anomaly_percent = anomaly_count / total_count\n",
    "        \n",
    "        # Create result row\n",
    "        results.append({\n",
    "            'Method': method.replace('_anomaly', ''),\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1,\n",
    "            'Anomalies Detected': anomaly_count,\n",
    "            'Anomaly %': anomaly_percent\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def plot_method_comparison(results_df):\n",
    "    \"\"\"Plot comparison of different anomaly detection methods\"\"\"\n",
    "    # Set up metrics to compare\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Set position for bars\n",
    "    x = np.arange(len(results_df['Method']))\n",
    "    width = 0.2\n",
    "    \n",
    "    # Plot bars for each metric\n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.bar(x + (i - 1.5) * width, results_df[metric], width, label=metric)\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Method', fontsize=14)\n",
    "    plt.ylabel('Score', fontsize=14)\n",
    "    plt.title('Comparison of Anomaly Detection Methods', fontsize=16)\n",
    "    plt.xticks(x, results_df['Method'])\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "# Compare methods for transaction data\n",
    "transaction_methods = [\n",
    "    'zscore_anomaly', \n",
    "    'isolation_forest_anomaly', \n",
    "    'lof_anomaly', \n",
    "    'autoencoder_anomaly'\n",
    "]\n",
    "\n",
    "transaction_comparison = compare_methods(featured_df, transaction_methods)\n",
    "print(\"Transaction Data Anomaly Detection Method Comparison:\")\n",
    "display(transaction_comparison)\n",
    "\n",
    "# Plot comparison\n",
    "txn_comparison_fig = plot_method_comparison(transaction_comparison)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5884b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble_detector(df, methods, voting_threshold=2):\n",
    "    \"\"\"Create an ensemble anomaly detector by voting from multiple methods\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # Check which methods exist in the dataframe\n",
    "    valid_methods = [method for method in methods if method in df_result.columns]\n",
    "    \n",
    "    if len(valid_methods) == 0:\n",
    "        raise ValueError(\"No valid anomaly detection methods provided\")\n",
    "    \n",
    "    # Sum the votes from each method\n",
    "    df_result['ensemble_votes'] = df_result[valid_methods].sum(axis=1)\n",
    "    \n",
    "    # Mark as anomaly if votes exceed threshold\n",
    "    df_result['ensemble_anomaly'] = (df_result['ensemble_votes'] >= voting_threshold).astype(int)\n",
    "    \n",
    "    # Count anomalies detected\n",
    "    anomaly_count = df_result['ensemble_anomaly'].sum()\n",
    "    total_count = len(df_result)\n",
    "    \n",
    "    print(f\"Ensemble method detected {anomaly_count} anomalies ({anomaly_count/total_count:.2%} of data)\")\n",
    "    print(f\"Voting threshold: {voting_threshold} out of {len(valid_methods)} methods\")\n",
    "    print(f\"Methods used: {', '.join([m.replace('_anomaly', '') for m in valid_methods])}\")\n",
    "    \n",
    "    # If ground truth is available, calculate accuracy\n",
    "    if 'is_anomaly' in df_result.columns:\n",
    "        accuracy = (df_result['ensemble_anomaly'] == df_result['is_anomaly']).mean()\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            df_result['is_anomaly'], \n",
    "            df_result['ensemble_anomaly'], \n",
    "            average='binary'\n",
    "        )\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(df_result['is_anomaly'], df_result['ensemble_anomaly'])\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "# Create ensemble detector for transaction data\n",
    "ensemble_results = create_ensemble_detector(\n",
    "    featured_df, \n",
    "    ['zscore_anomaly', 'isolation_forest_anomaly', 'lof_anomaly', 'autoencoder_anomaly'],\n",
    "    voting_threshold=2\n",
    ")\n",
    "\n",
    "# Add ensemble to comparison\n",
    "ensemble_comparison = compare_methods(\n",
    "    ensemble_results, \n",
    "    transaction_methods + ['ensemble_anomaly']\n",
    ")\n",
    "\n",
    "print(\"\\nTransaction Data Anomaly Detection Method Comparison (with Ensemble):\")\n",
    "display(ensemble_comparison)\n",
    "\n",
    "# Plot comparison with ensemble\n",
    "ensemble_comparison_fig = plot_method_comparison(ensemble_comparison)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6bd985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.models import load_model\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from prophet import Prophet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define paths\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "MODELS_DIR = os.path.join(BASE_DIR, 'models')\n",
    "\n",
    "class AnomalyDetectionService:\n",
    "    \"\"\"Service for detecting anomalies in financial data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the service and load models\"\"\"\n",
    "        self.models = {}\n",
    "        self.load_models()\n",
    "        \n",
    "    def load_models(self):\n",
    "        \"\"\"Load all trained anomaly detection models\"\"\"\n",
    "        # Load Isolation Forest model\n",
    "        iforest_path = os.path.join(MODELS_DIR, 'isolation_forest_model.joblib')\n",
    "        if os.path.exists(iforest_path):\n",
    "            self.models['isolation_forest'] = joblib.load(iforest_path)\n",
    "        \n",
    "        # Load Autoencoder model\n",
    "        autoencoder_path = os.path.join(MODELS_DIR, 'autoencoder_model.h5')\n",
    "        if os.path.exists(autoencoder_path):\n",
    "            self.models['autoencoder'] = load_model(autoencoder_path)\n",
    "        \n",
    "        # Load ARIMA parameters\n",
    "        arima_params_path = os.path.join(MODELS_DIR, 'arima_model_params.json')\n",
    "        if os.path.exists(arima_params_path):\n",
    "            with open(arima_params_path, 'r') as f:\n",
    "                self.models['arima_params'] = json.load(f)\n",
    "        \n",
    "        # Load Prophet model\n",
    "        prophet_path = os.path.join(MODELS_DIR, 'prophet_model.json')\n",
    "        if os.path.exists(prophet_path):\n",
    "            self.models['prophet'] = Prophet.deserialize_model(prophet_path)\n",
    "    \n",
    "    def preprocess_transaction(self, transaction_data):\n",
    "        \"\"\"Preprocess a single transaction or batch of transactions\"\"\"\n",
    "        # Convert to DataFrame if it's a dictionary\n",
    "        if isinstance(transaction_data, dict):\n",
    "            df = pd.DataFrame([transaction_data])\n",
    "        else:\n",
    "            df = pd.DataFrame(transaction_data)\n",
    "        \n",
    "        # Ensure date is datetime\n",
    "        if 'date' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        # Extract date features\n",
    "        if 'date' in df.columns:\n",
    "            df['day_of_week'] = df['date'].dt.dayofweek\n",
    "            df['day_of_month'] = df['date'].dt.day\n",
    "            df['month'] = df['date'].dt.month\n",
    "            df['year'] = df['date'].dt.year\n",
    "        \n",
    "        # One-hot encode categorical features if present\n",
    "        for col in ['transaction_type', 'location']:\n",
    "            if col in df.columns and df[col].dtype == 'object':\n",
    "                df = pd.get_dummies(df, columns=[col], drop_first=False)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def detect_transaction_anomalies(self, transaction_data, methods=None, zscore_threshold=3.0):\n",
    "        \"\"\"Detect anomalies in transaction data\"\"\"\n",
    "        # Set default methods if not specified\n",
    "        if methods is None:\n",
    "            methods = ['zscore', 'isolation_forest']\n",
    "        \n",
    "        # Preprocess data\n",
    "        df = self.preprocess_transaction(transaction_data)\n",
    "        \n",
    "        # Calculate Z-score if needed\n",
    "        if 'zscore' in methods and 'amount' in df.columns:\n",
    "            # Get customer stats or use overall stats\n",
    "            if 'customer_id' in df.columns and len(df['customer_id'].unique()) > 1:\n",
    "                # Calculate per customer\n",
    "                customer_stats = df.groupby('customer_id')['amount'].agg(['mean', 'std']).reset_index()\n",
    "                df = pd.merge(df, customer_stats, on='customer_id', how='left')\n",
    "                # Handle zero std\n",
    "                df['std'] = df['std'].fillna(0) + 1e-6\n",
    "                df['amount_zscore'] = (df['amount'] - df['mean']) / df['std']\n",
    "            else:\n",
    "                # Use overall stats\n",
    "                mean = df['amount'].mean()\n",
    "                std = df['amount'].std() + 1e-6\n",
    "                df['amount_zscore'] = (df['amount'] - mean) / std\n",
    "                \n",
    "            # Detect anomalies based on Z-score\n",
    "            df['zscore_anomaly'] = (abs(df['amount_zscore']) > zscore_threshold).astype(int)\n",
    "        \n",
    "        # Use Isolation Forest if available\n",
    "        if 'isolation_forest' in methods and 'isolation_forest' in self.models:\n",
    "            # Select features for isolation forest\n",
    "            num_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            features = [col for col in num_features if col not in ['customer_id', 'zscore_anomaly']]\n",
    "            \n",
    "            if features:\n",
    "                # Make prediction\n",
    "                try:\n",
    "                    df['isolation_forest_anomaly'] = (\n",
    "                        self.models['isolation_forest'].predict(df[features]) == -1\n",
    "                    ).astype(int)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in isolation forest prediction: {e}\")\n",
    "                    df['isolation_forest_anomaly'] = 0\n",
    "        \n",
    "        # Use Autoencoder if available\n",
    "        if 'autoencoder' in methods and 'autoencoder' in self.models:\n",
    "            # Select features for autoencoder\n",
    "            num_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            features = [col for col in num_features if col not in ['customer_id', 'zscore_anomaly', 'isolation_forest_anomaly']]\n",
    "            \n",
    "            if features:\n",
    "                # Make prediction\n",
    "                try:\n",
    "                    reconstructions = self.models['autoencoder'].predict(df[features])\n",
    "                    mse = np.mean(np.power(df[features].values - reconstructions, 2), axis=1)\n",
    "                    df['autoencoder_error'] = mse\n",
    "                    threshold = np.percentile(mse, 95)  # 95th percentile\n",
    "                    df['autoencoder_anomaly'] = (df['autoencoder_error'] > threshold).astype(int)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in autoencoder prediction: {e}\")\n",
    "                    df['autoencoder_anomaly'] = 0\n",
    "        \n",
    "        # Create ensemble\n",
    "        anomaly_cols = [col for col in df.columns if col.endswith('_anomaly')]\n",
    "        if len(anomaly_cols) > 1:\n",
    "            df['ensemble_votes'] = df[anomaly_cols].sum(axis=1)\n",
    "            df['ensemble_anomaly'] = (df['ensemble_votes'] >= max(1, len(anomaly_cols) // 2)).astype(int)\n",
    "        elif len(anomaly_cols) == 1:\n",
    "            df['ensemble_anomaly'] = df[anomaly_cols[0]]\n",
    "            df['ensemble_votes'] = df[anomaly_cols[0]]\n",
    "        else:\n",
    "            df['ensemble_anomaly'] = 0\n",
    "            df['ensemble_votes'] = 0\n",
    "        \n",
    "        # Prepare result\n",
    "        result = {\n",
    "            'is_anomaly': bool(df['ensemble_anomaly'].iloc[0]) if len(df) == 1 else df['ensemble_anomaly'].tolist(),\n",
    "            'anomaly_score': float(df['ensemble_votes'].iloc[0]) if len(df) == 1 else df['ensemble_votes'].tolist(),\n",
    "            'methods_used': methods,\n",
    "            'details': {}\n",
    "        }\n",
    "        \n",
    "        # Add method-specific details\n",
    "        for method in methods:\n",
    "            method_col = f\"{method}_anomaly\"\n",
    "            if method_col in df.columns:\n",
    "                result['details'][method] = bool(df[method_col].iloc[0]) if len(df) == 1 else df[method_col].tolist()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def detect_market_anomalies(self, market_data, price_col='close', method='arima', confidence=0.95):\n",
    "        \"\"\"Detect anomalies in market price data\"\"\"\n",
    "        # Convert to DataFrame if it's a dictionary\n",
    "        if isinstance(market_data, dict):\n",
    "            df = pd.DataFrame([market_data])\n",
    "        else:\n",
    "            df = pd.DataFrame(market_data)\n",
    "        \n",
    "        # Ensure date is datetime\n",
    "        if 'date' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        # Sort by date\n",
    "        df = df.sort_values('date')\n",
    "        \n",
    "        # ARIMA method\n",
    "        if method == 'arima' and price_col in df.columns:\n",
    "            try:\n",
    "                # Get ARIMA parameters if available\n",
    "                if 'arima_params' in self.models:\n",
    "                    order = tuple(self.models['arima_params']['order'])\n",
    "                else:\n",
    "                    order = (1, 1, 1)  # Default\n",
    "                \n",
    "                # Fit ARIMA model\n",
    "                model = ARIMA(df[price_col], order=order)\n",
    "                model_fit = model.fit()\n",
    "                \n",
    "                # Get predictions and confidence intervals\n",
    "                predictions = model_fit.fittedvalues\n",
    "                resid = model_fit.resid\n",
    "                sigma = np.std(resid)\n",
    "                \n",
    "                # Calculate confidence intervals\n",
    "                from scipy.stats import norm\n",
    "                z_score = norm.ppf(1 - (1 - confidence) / 2)\n",
    "                lower_bound = predictions - z_score * sigma\n",
    "                upper_bound = predictions + z_score * sigma\n",
    "                \n",
    "                # Detect anomalies\n",
    "                df['arima_prediction'] = predictions\n",
    "                df['arima_lower_bound'] = lower_bound\n",
    "                df['arima_upper_bound'] = upper_bound\n",
    "                df['arima_anomaly'] = ((df[price_col] < df['arima_lower_bound']) | \n",
    "                                     (df[price_col] > df['arima_upper_bound'])).astype(int)\n",
    "                \n",
    "                # Prepare result\n",
    "                result = {\n",
    "                    'is_anomaly': bool(df['arima_anomaly'].iloc[-1]) if len(df) >= 1 else False,\n",
    "                    'anomaly_dates': df.loc[df['arima_anomaly'] == 1, 'date'].dt.strftime('%Y-%m-%d').tolist(),\n",
    "                    'anomaly_prices': df.loc[df['arima_anomaly'] == 1, price_col].tolist(),\n",
    "                    'latest_prediction': float(df['arima_prediction'].iloc[-1]) if len(df) >= 1 else None,\n",
    "                    'latest_lower_bound': float(df['arima_lower_bound'].iloc[-1]) if len(df) >= 1 else None,\n",
    "                    'latest_upper_bound': float(df['arima_upper_bound'].iloc[-1]) if len(df) >= 1 else None,\n",
    "                    'method': 'arima'\n",
    "                }\n",
    "                \n",
    "                return result\n",
    "            except Exception as e:\n",
    "                print(f\"Error in ARIMA anomaly detection: {e}\")\n",
    "                return {'is_anomaly': False, 'error': str(e), 'method': 'arima'}\n",
    "        \n",
    "        # Prophet method\n",
    "        elif method == 'prophet' and price_col in df.columns and 'date' in df.columns:\n",
    "            try:\n",
    "                # Prepare data for Prophet\n",
    "                prophet_df = df[['date', price_col]].rename(columns={'date': 'ds', price_col: 'y'})\n",
    "                \n",
    "                # Use loaded model or create new one\n",
    "                if 'prophet' in self.models:\n",
    "                    model = self.models['prophet']\n",
    "                else:\n",
    "                    model = Prophet(interval_width=confidence, daily_seasonality=True)\n",
    "                    model.fit(prophet_df)\n",
    "                \n",
    "                # Make predictions\n",
    "                forecast = model.predict(prophet_df[['ds']])\n",
    "                \n",
    "                # Merge predictions back to original data\n",
    "                forecast_columns = ['ds', 'yhat', 'yhat_lower', 'yhat_upper']\n",
    "                df = pd.merge(df, forecast[forecast_columns], left_on='date', right_on='ds')\n",
    "                df.drop('ds', axis=1, inplace=True)\n",
    "                \n",
    "                # Detect anomalies\n",
    "                df['prophet_anomaly'] = ((df[price_col] < df['yhat_lower']) | \n",
    "                                       (df[price_col] > df['yhat_upper'])).astype(int)\n",
    "                \n",
    "                # Prepare result\n",
    "                result = {\n",
    "                    'is_anomaly': bool(df['prophet_anomaly'].iloc[-1]) if len(df) >= 1 else False,\n",
    "                    'anomaly_dates': df.loc[df['prophet_anomaly'] == 1, 'date'].dt.strftime('%Y-%m-%d').tolist(),\n",
    "                    'anomaly_prices': df.loc[df['prophet_anomaly'] == 1, price_col].tolist(),\n",
    "                    'latest_prediction': float(df['yhat'].iloc[-1]) if len(df) >= 1 else None,\n",
    "                    'latest_lower_bound': float(df['yhat_lower'].iloc[-1]) if len(df) >= 1 else None,\n",
    "                    'latest_upper_bound': float(df['yhat_upper'].iloc[-1]) if len(df) >= 1 else None,\n",
    "                    'method': 'prophet'\n",
    "                }\n",
    "                \n",
    "                return result\n",
    "            except Exception as e:\n",
    "                print(f\"Error in Prophet anomaly detection: {e}\")\n",
    "                return {'is_anomaly': False, 'error': str(e), 'method': 'prophet'}\n",
    "        \n",
    "        else:\n",
    "            return {'is_anomaly': False, 'error': f\"Invalid method or missing columns\", 'method': method}\n",
    "\n",
    "    def check_unusual_patterns(self, df, column, window=5, threshold=2.0):\n",
    "        \"\"\"Detect unusual patterns in time series data using rolling statistics\"\"\"\n",
    "        if len(df) < window * 2:\n",
    "            return np.zeros(len(df), dtype=int)\n",
    "        \n",
    "        # Calculate rolling mean and std\n",
    "        rolling_mean = df[column].rolling(window=window).mean()\n",
    "        rolling_std = df[column].rolling(window=window).std()\n",
    "        \n",
    "        # Fill NaN values\n",
    "        rolling_mean = rolling_mean.fillna(df[column].mean())\n",
    "        rolling_std = rolling_std.fillna(df[column].std())\n",
    "        \n",
    "        # Detect anomalies using z-score\n",
    "        z_scores = (df[column] - rolling_mean) / (rolling_std + 1e-6)\n",
    "        anomalies = (abs(z_scores) > threshold).astype(int)\n",
    "        \n",
    "        return anomalies\n",
    "\n",
    "    def analyze_user_transaction_history(self, transactions, user_id, recent_window=30):\n",
    "        \"\"\"Analyze a user's transaction history to detect changes in behavior\"\"\"\n",
    "        if not transactions or len(transactions) < 5:\n",
    "            return {\"status\": \"insufficient_data\", \"message\": \"Not enough transaction history for analysis\"}\n",
    "        \n",
    "        # Convert to DataFrame if it's a list of dictionaries\n",
    "        if isinstance(transactions[0], dict):\n",
    "            df = pd.DataFrame(transactions)\n",
    "        else:\n",
    "            df = transactions\n",
    "        \n",
    "        # Filter for specific user if provided\n",
    "        if user_id is not None and 'user_id' in df.columns:\n",
    "            df = df[df['user_id'] == user_id]\n",
    "        \n",
    "        # Ensure date is datetime\n",
    "        if 'date' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            \n",
    "        # Sort by date\n",
    "        if 'date' in df.columns:\n",
    "            df = df.sort_values('date')\n",
    "        \n",
    "        # Calculate behavioral metrics\n",
    "        results = {}\n",
    "        \n",
    "        # 1. Transaction frequency\n",
    "        if 'date' in df.columns:\n",
    "            # Get dates as list\n",
    "            dates = pd.Series(df['date'])\n",
    "            # Calculate gaps between transactions\n",
    "            gaps = dates.diff().dt.total_seconds() / 86400  # Convert to days\n",
    "            \n",
    "            # Get recent activity\n",
    "            if len(df) >= recent_window:\n",
    "                recent_df = df.iloc[-recent_window:]\n",
    "                older_df = df.iloc[:-recent_window]\n",
    "                \n",
    "                # Compare transaction rates\n",
    "                if len(older_df) > 0:\n",
    "                    older_rate = len(older_df) / (older_df['date'].max() - older_df['date'].min()).days\n",
    "                    recent_rate = len(recent_df) / max(1, (recent_df['date'].max() - recent_df['date'].min()).days)\n",
    "                    \n",
    "                    # If recent rate is 2x older rate, flag as unusual\n",
    "                    rate_change = recent_rate / (older_rate + 1e-6)\n",
    "                    results['transaction_frequency'] = {\n",
    "                        'is_unusual': rate_change > 2.0 or rate_change < 0.5,\n",
    "                        'recent_rate': recent_rate,\n",
    "                        'historical_rate': older_rate,\n",
    "                        'change_factor': rate_change\n",
    "                    }\n",
    "        \n",
    "        # 2. Transaction amounts\n",
    "        if 'amount' in df.columns:\n",
    "            # Calculate amount statistics\n",
    "            avg_amount = df['amount'].mean()\n",
    "            std_amount = df['amount'].std()\n",
    "            \n",
    "            # Check recent transactions for unusual amounts\n",
    "            if len(df) >= recent_window:\n",
    "                recent_amounts = df['amount'].iloc[-recent_window:]\n",
    "                unusual_amounts = (abs(recent_amounts - avg_amount) > 2 * std_amount).sum()\n",
    "                \n",
    "                results['transaction_amounts'] = {\n",
    "                    'is_unusual': unusual_amounts > recent_window * 0.2,  # If >20% of recent transactions are unusual\n",
    "                    'unusual_count': unusual_amounts,\n",
    "                    'average_amount': avg_amount,\n",
    "                    'std_amount': std_amount\n",
    "                }\n",
    "        \n",
    "        # 3. Location patterns\n",
    "        if 'location' in df.columns:\n",
    "            # Get common locations\n",
    "            location_counts = df['location'].value_counts()\n",
    "            common_locations = set(location_counts[location_counts > 1].index)\n",
    "            \n",
    "            # Check if recent transactions are in unusual locations\n",
    "            if len(df) >= recent_window and len(common_locations) > 0:\n",
    "                recent_locations = set(df['location'].iloc[-recent_window:])\n",
    "                new_locations = recent_locations - common_locations\n",
    "                \n",
    "                results['location_patterns'] = {\n",
    "                    'is_unusual': len(new_locations) > 0,\n",
    "                    'new_locations': list(new_locations),\n",
    "                    'common_locations': list(common_locations)\n",
    "                }\n",
    "        \n",
    "        # Determine overall status\n",
    "        unusual_markers = sum(1 for k, v in results.items() if isinstance(v, dict) and v.get('is_unusual', False))\n",
    "        results['overall_status'] = {\n",
    "            'is_unusual': unusual_markers > 0,\n",
    "            'unusual_markers': unusual_markers,\n",
    "            'total_markers': len(results),\n",
    "            'confidence': unusual_markers / max(1, len(results))\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Instantiate service for usage\n",
    "anomaly_service = AnomalyDetectionService()\n",
    "\n",
    "def detect_transaction_anomaly(transaction):\n",
    "    \"\"\"API function to detect anomalies in a transaction\"\"\"\n",
    "    return anomaly_service.detect_transaction_anomalies(transaction)\n",
    "\n",
    "def detect_market_anomaly(market_data, ticker=None):\n",
    "    \"\"\"API function to detect anomalies in market data\"\"\"\n",
    "    if ticker:\n",
    "        market_data = [d for d in market_data if d.get('ticker') == ticker]\n",
    "    return anomaly_service.detect_market_anomalies(market_data)\n",
    "\n",
    "def analyze_user_behavior(transactions, user_id=None):\n",
    "    \"\"\"API function to analyze user transaction behavior for anomalies\"\"\"\n",
    "    return anomaly_service.analyze_user_transaction_history(transactions, user_id)\n",
    "\n",
    "def get_anomaly_description(anomaly_result):\n",
    "    \"\"\"Generate human-readable description for an anomaly\"\"\"\n",
    "    if not anomaly_result['is_anomaly']:\n",
    "        return \"No anomalies detected.\"\n",
    "    \n",
    "    descriptions = []\n",
    "    \n",
    "    # Transaction anomalies\n",
    "    if 'details' in anomaly_result:\n",
    "        if anomaly_result['details'].get('zscore', False):\n",
    "            descriptions.append(\"Unusual transaction amount (significantly different from historical patterns)\")\n",
    "        \n",
    "        if anomaly_result['details'].get('isolation_forest', False):\n",
    "            descriptions.append(\"Transaction with unusual combination of features\")\n",
    "        \n",
    "        if anomaly_result['details'].get('autoencoder', False):\n",
    "            descriptions.append(\"Transaction pattern differs from normal behavior\")\n",
    "    \n",
    "    # Market anomalies\n",
    "    if 'method' in anomaly_result:\n",
    "        if anomaly_result['method'] == 'arima' or anomaly_result['method'] == 'prophet':\n",
    "            if anomaly_result.get('anomaly_dates', []):\n",
    "                latest_anomaly = anomaly_result['anomaly_dates'][-1]\n",
    "                descriptions.append(f\"Unusual price movement detected on {latest_anomaly}\")\n",
    "    \n",
    "    if not descriptions:\n",
    "        descriptions.append(\"Potential anomaly detected (unspecified reason)\")\n",
    "    \n",
    "    return \" \".join(descriptions)\n",
    "\n",
    "def generate_user_advice(anomaly_result):\n",
    "    \"\"\"Generate advice for users based on detected anomalies\"\"\"\n",
    "    if not anomaly_result.get('is_anomaly', False):\n",
    "        return \"Your financial activity appears normal. Continue monitoring your accounts regularly.\"\n",
    "    \n",
    "    advice = []\n",
    "    \n",
    "    # Transaction anomalies\n",
    "    if 'details' in anomaly_result:\n",
    "        if anomaly_result['details'].get('zscore', False):\n",
    "            advice.append(\"This transaction is unusually large or small compared to your typical patterns. \"\n",
    "                         \"Ensure it was authorized by you.\")\n",
    "        \n",
    "        if anomaly_result['details'].get('isolation_forest', False) or anomaly_result['details'].get('autoencoder', False):\n",
    "            advice.append(\"This transaction has unusual characteristics. \"\n",
    "                         \"Verify the transaction details and contact your bank if you don't recognize it.\")\n",
    "    \n",
    "    # Market anomalies\n",
    "    if 'method' in anomaly_result:\n",
    "        if anomaly_result['method'] == 'arima' or anomaly_result['method'] == 'prophet':\n",
    "            advice.append(\"The market is showing unusual movement. \"\n",
    "                         \"Consider waiting for stability before making investment decisions.\")\n",
    "    \n",
    "    # Behavioral anomalies\n",
    "    if 'overall_status' in anomaly_result:\n",
    "        if anomaly_result['overall_status'].get('is_unusual', False):\n",
    "            if anomaly_result.get('transaction_frequency', {}).get('is_unusual', False):\n",
    "                advice.append(\"Your transaction frequency has changed significantly. \"\n",
    "                             \"Review your recent account activity.\")\n",
    "            \n",
    "            if anomaly_result.get('transaction_amounts', {}).get('is_unusual', False):\n",
    "                advice.append(\"Your recent transaction amounts are different from your usual patterns. \"\n",
    "                             \"Ensure all transactions are authorized.\")\n",
    "            \n",
    "            if anomaly_result.get('location_patterns', {}).get('is_unusual', False):\n",
    "                advice.append(\"Transactions from new or unusual locations detected. \"\n",
    "                             \"Verify these transactions and consider updating your security settings.\")\n",
    "    \n",
    "    if not advice:\n",
    "        advice.append(\"An unusual pattern has been detected. Review your recent activities as a precaution.\")\n",
    "    \n",
    "    # Add general security advice\n",
    "    advice.append(\"\\nGeneral security tips: \"\n",
    "                 \"Enable two-factor authentication, regularly update your passwords, \"\n",
    "                 \"and monitor your accounts for unauthorized activity.\")\n",
    "    \n",
    "    return \" \".join(advice)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example transaction\n",
    "    example_transaction = {\n",
    "        'date': '2025-03-01',\n",
    "        'customer_id': 1234,\n",
    "        'transaction_type': 'withdrawal',\n",
    "        'amount': 50000,  # Unusually large amount\n",
    "        'location': 'Nairobi'\n",
    "    }\n",
    "    \n",
    "    # Detect anomaly\n",
    "    result = detect_transaction_anomaly(example_transaction)\n",
    "    \n",
    "    # Print result\n",
    "    print(f\"Is anomaly: {result['is_anomaly']}\")\n",
    "    print(f\"Description: {get_anomaly_description(result)}\")\n",
    "    print(f\"Advice: {generate_user_advice(result)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pesaguru",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
