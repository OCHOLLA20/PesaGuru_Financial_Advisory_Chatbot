{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02f8fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Optimization libraries\n",
    "import scipy.optimize as sco\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# API requests\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "\n",
    "# For model persistence\n",
    "import pickle\n",
    "\n",
    "# Warning suppression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48ef88cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch NSE stock data from Yahoo Finance\n",
    "def fetch_nse_data(tickers, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch historical stock data for NSE-listed companies\n",
    "    \n",
    "    Parameters:\n",
    "    - tickers (list): List of NSE stock symbols with '.NR' suffix for Nairobi exchange\n",
    "    - start_date (str): Start date in YYYY-MM-DD format\n",
    "    - end_date (str): End date in YYYY-MM-DD format\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with historical stock prices\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Yahoo Finance requires headers to prevent blocking\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        # Create empty DataFrame to store all stock data\n",
    "        all_data = pd.DataFrame()\n",
    "        \n",
    "        for ticker in tickers:\n",
    "            # Convert dates to Unix timestamp for Yahoo Finance API\n",
    "            start_timestamp = int(datetime.strptime(start_date, '%Y-%m-%d').timestamp())\n",
    "            end_timestamp = int(datetime.strptime(end_date, '%Y-%m-%d').timestamp())\n",
    "            \n",
    "            # Construct URL\n",
    "            url = f\"https://query1.finance.yahoo.com/v7/finance/download/{ticker}?period1={start_timestamp}&period2={end_timestamp}&interval=1d&events=history\"\n",
    "            \n",
    "            # Fetch data\n",
    "            response = requests.get(url, headers=headers)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Create DataFrame from CSV content\n",
    "                stock_data = pd.read_csv(pd.io.common.StringIO(response.text))\n",
    "                stock_data['Symbol'] = ticker\n",
    "                \n",
    "                # Append to main DataFrame\n",
    "                if all_data.empty:\n",
    "                    all_data = stock_data\n",
    "                else:\n",
    "                    all_data = pd.concat([all_data, stock_data])\n",
    "            else:\n",
    "                print(f\"Failed to fetch data for {ticker}: Status code {response.status_code}\")\n",
    "        \n",
    "        # Convert date to datetime\n",
    "        all_data['Date'] = pd.to_datetime(all_data['Date'])\n",
    "        \n",
    "        return all_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching stock data: {str(e)}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "971861d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample NSE data if API fetch fails\n",
    "def load_sample_nse_data():\n",
    "    \"\"\"\n",
    "    Load sample NSE stock data for demonstration purposes\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with sample historical stock prices\n",
    "    \"\"\"\n",
    "    # Create sample data for key NSE stocks with realistic values\n",
    "    # We'll generate synthetic data based on real NSE stock behavior\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    # Define stock parameters (based on historical NSE performance)\n",
    "    stocks = {\n",
    "        'SCOM.NR': {'mean': 0.0005, 'std': 0.015, 'start_price': 42.75},  # Safaricom\n",
    "        'EQTY.NR': {'mean': 0.0003, 'std': 0.012, 'start_price': 51.00},  # Equity Group\n",
    "        'KCB.NR': {'mean': 0.0002, 'std': 0.014, 'start_price': 45.50},   # KCB Group\n",
    "        'COOP.NR': {'mean': 0.0003, 'std': 0.013, 'start_price': 16.00},  # Cooperative Bank\n",
    "        'EABL.NR': {'mean': 0.0001, 'std': 0.011, 'start_price': 170.00}, # East African Breweries\n",
    "        'BAT.NR': {'mean': -0.0001, 'std': 0.010, 'start_price': 800.00}, # BAT Kenya\n",
    "        'ABSA.NR': {'mean': 0.0002, 'std': 0.012, 'start_price': 11.75},  # Absa Bank Kenya\n",
    "        'JUB.NR': {'mean': 0.0004, 'std': 0.014, 'start_price': 305.00}   # Jubilee Holdings\n",
    "    }\n",
    "    \n",
    "    # Generate dates (2 years of trading days - approximately 504 days)\n",
    "    end_date = datetime.now()\n",
    "    dates = [end_date - timedelta(days=i) for i in range(504)]\n",
    "    dates.reverse()  # Sort chronologically\n",
    "    dates = [date for date in dates if date.weekday() < 5]  # Keep only weekdays\n",
    "    \n",
    "    # Create empty DataFrame\n",
    "    all_data = pd.DataFrame()\n",
    "    \n",
    "    # Generate data for each stock\n",
    "    for ticker, params in stocks.items():\n",
    "        # Generate returns using random walk with drift\n",
    "        returns = np.random.normal(params['mean'], params['std'], size=len(dates))\n",
    "        \n",
    "        # Convert returns to prices\n",
    "        price = params['start_price']\n",
    "        prices = [price]\n",
    "        \n",
    "        for ret in returns[1:]:\n",
    "            price = price * (1 + ret)\n",
    "            prices.append(price)\n",
    "        \n",
    "        # Create DataFrame for this stock\n",
    "        stock_data = pd.DataFrame({\n",
    "            'Date': dates,\n",
    "            'Open': prices,\n",
    "            'High': [p * (1 + abs(np.random.normal(0, 0.005))) for p in prices],\n",
    "            'Low': [p * (1 - abs(np.random.normal(0, 0.005))) for p in prices],\n",
    "            'Close': prices,\n",
    "            'Adj Close': prices,\n",
    "            'Volume': [int(np.random.normal(500000, 100000)) for _ in range(len(dates))],\n",
    "            'Symbol': ticker\n",
    "        })\n",
    "        \n",
    "        # Append to main DataFrame\n",
    "        if all_data.empty:\n",
    "            all_data = stock_data\n",
    "        else:\n",
    "            all_data = pd.concat([all_data, stock_data])\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3ac80a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping daily_equities data...\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\urllib3\\connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\http\\client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\urllib3\\util\\retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\urllib3\\util\\util.py:38\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\urllib3\\connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\http\\client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m data_categories\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_nse_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnse_data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[4], line 29\u001b[0m, in \u001b[0;36mscrape_nse_data\u001b[1;34m(category, start_date, end_date)\u001b[0m\n\u001b[0;32m     27\u001b[0m formatted_date \u001b[38;5;241m=\u001b[39m current_date\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdata_categories[category]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?date=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformatted_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 29\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHEADERS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     31\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\requests\\sessions.py:724\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[0;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m [resp \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m gen]\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\requests\\sessions.py:724\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[0;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m [resp \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m gen]\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\requests\\sessions.py:265\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 265\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m    266\u001b[0m         req,\n\u001b[0;32m    267\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    268\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    269\u001b[0m         verify\u001b[38;5;241m=\u001b[39mverify,\n\u001b[0;32m    270\u001b[0m         cert\u001b[38;5;241m=\u001b[39mcert,\n\u001b[0;32m    271\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    272\u001b[0m         allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madapter_kwargs,\n\u001b[0;32m    274\u001b[0m     )\n\u001b[0;32m    276\u001b[0m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcookies, prepared_request, resp\u001b[38;5;241m.\u001b[39mraw)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\pesaguru\\lib\\site-packages\\requests\\adapters.py:682\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    686\u001b[0m         \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "BASE_URL = \"https://www.nse.co.ke/dataservices/historical-data/\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# Define the different categories to scrape\n",
    "data_categories = {\n",
    "    \"daily_equities\": \"daily-equities-pricelist\",\n",
    "    \"daily_bonds\": \"daily-bonds-pricelist\",\n",
    "    \"weekly_equities\": \"weekly-equities-pricelist\",\n",
    "    \"monthly_bulletin\": \"monthly-bulletin\",\n",
    "}\n",
    "\n",
    "# Date range from 2015 to 2025\n",
    "start_date = datetime.date(2015, 1, 1)\n",
    "end_date = datetime.date(2025, 12, 31)\n",
    "\n",
    "# Function to scrape data\n",
    "def scrape_nse_data(category, start_date, end_date):\n",
    "    current_date = start_date\n",
    "    all_data = []\n",
    "    while current_date <= end_date:\n",
    "        formatted_date = current_date.strftime(\"%Y-%m-%d\")\n",
    "        url = f\"{BASE_URL}{data_categories[category]}?date={formatted_date}\"\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            tables = soup.find_all(\"table\")\n",
    "            for table in tables:\n",
    "                df = pd.read_html(str(table))[0]\n",
    "                df[\"Date\"] = formatted_date\n",
    "                all_data.append(df)\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for {formatted_date}: {response.status_code}\")\n",
    "        current_date += datetime.timedelta(days=1)\n",
    "    return pd.concat(all_data, ignore_index=True) if all_data else None\n",
    "\n",
    "# Create a directory for storing data\n",
    "if not os.path.exists(\"nse_data\"): \n",
    "    os.makedirs(\"nse_data\")\n",
    "\n",
    "# Scrape and save data\n",
    "for category in data_categories.keys():\n",
    "    print(f\"Scraping {category} data...\")\n",
    "    df = scrape_nse_data(category, start_date, end_date)\n",
    "    if df is not None:\n",
    "        file_path = f\"nse_data/{category}.csv\"\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Saved {category} data to {file_path}\")\n",
    "    else:\n",
    "        print(f\"No data found for {category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6aa8f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 15:33:46,506 - INFO - NSE Scraper initialized with 30 stocks\n",
      "2025-03-13 15:33:46,508 - INFO - Starting NSE data scraping at 2025-03-13 15:33:46\n",
      "2025-03-13 15:33:46,510 - INFO - Scraping market statistics from https://www.nse.co.ke/dataservices/market-statistics/\n",
      "2025-03-13 15:33:47,505 - INFO - Found 6 tables on market statistics page\n",
      "2025-03-13 15:33:47,524 - INFO - Saved table 1 with shape (16, 3) to nse_data/market_statistics/table_1_20250313_153346.csv\n",
      "2025-03-13 15:33:47,532 - INFO - Saved table 2 with shape (0, 8) to nse_data/market_statistics/table_2_20250313_153346.csv\n",
      "2025-03-13 15:33:47,539 - INFO - Saved table 3 with shape (4, 6) to nse_data/market_statistics/table_3_20250313_153346.csv\n",
      "2025-03-13 15:33:47,545 - INFO - Saved table 4 with shape (1, 6) to nse_data/market_statistics/table_4_20250313_153346.csv\n",
      "2025-03-13 15:33:47,554 - INFO - Saved table 5 with shape (2, 3) to nse_data/market_statistics/table_5_20250313_153346.csv\n",
      "2025-03-13 15:33:47,558 - INFO - Saved table 6 with shape (1, 3) to nse_data/market_statistics/table_6_20250313_153346.csv\n",
      "2025-03-13 15:33:47,559 - INFO - Scraping historical data links from https://www.nse.co.ke/dataservices/historical-data/\n",
      "2025-03-13 15:33:48,576 - INFO - Found 0 download links\n",
      "2025-03-13 15:33:48,577 - INFO - Scraping data from myStocks Kenya\n",
      "2025-03-13 15:33:48,579 - INFO - Scraping EQTY from myStocks Kenya\n",
      "2025-03-13 15:33:48,945 - INFO - Successfully scraped EQTY data\n",
      "2025-03-13 15:33:50,956 - INFO - Scraping KCB from myStocks Kenya\n",
      "2025-03-13 15:33:51,293 - INFO - Successfully scraped KCB data\n",
      "2025-03-13 15:33:53,296 - INFO - Scraping COOP from myStocks Kenya\n",
      "2025-03-13 15:33:53,779 - INFO - Successfully scraped COOP data\n",
      "2025-03-13 15:33:55,795 - INFO - Scraping ABSA from myStocks Kenya\n",
      "2025-03-13 15:33:56,098 - INFO - Successfully scraped ABSA data\n",
      "2025-03-13 15:33:58,112 - INFO - Scraping SCBK from myStocks Kenya\n",
      "2025-03-13 15:33:58,469 - INFO - Successfully scraped SCBK data\n",
      "2025-03-13 15:34:00,475 - INFO - Scraping DTK from myStocks Kenya\n",
      "2025-03-13 15:34:00,779 - INFO - Successfully scraped DTK data\n",
      "2025-03-13 15:34:02,781 - INFO - Scraping IMH from myStocks Kenya\n",
      "2025-03-13 15:34:03,071 - INFO - Successfully scraped IMH data\n",
      "2025-03-13 15:34:05,073 - INFO - Scraping NCBA from myStocks Kenya\n",
      "2025-03-13 15:34:05,374 - INFO - Successfully scraped NCBA data\n",
      "2025-03-13 15:34:07,378 - INFO - Scraping SCOM from myStocks Kenya\n",
      "2025-03-13 15:34:07,709 - INFO - Successfully scraped SCOM data\n",
      "2025-03-13 15:34:09,722 - INFO - Scraping EABL from myStocks Kenya\n",
      "2025-03-13 15:34:10,046 - INFO - Successfully scraped EABL data\n",
      "2025-03-13 15:34:12,050 - INFO - Scraping BAT from myStocks Kenya\n",
      "2025-03-13 15:34:12,352 - INFO - Successfully scraped BAT data\n",
      "2025-03-13 15:34:14,362 - INFO - Scraping BAMB from myStocks Kenya\n",
      "2025-03-13 15:34:14,817 - INFO - Successfully scraped BAMB data\n",
      "2025-03-13 15:34:16,841 - INFO - Scraping KEGN from myStocks Kenya\n",
      "2025-03-13 15:34:17,142 - INFO - Successfully scraped KEGN data\n",
      "2025-03-13 15:34:19,146 - INFO - Scraping KPLC from myStocks Kenya\n",
      "2025-03-13 15:34:19,443 - INFO - Successfully scraped KPLC data\n",
      "2025-03-13 15:34:21,458 - INFO - Scraping JUB from myStocks Kenya\n",
      "2025-03-13 15:34:21,806 - INFO - Successfully scraped JUB data\n",
      "2025-03-13 15:34:23,808 - INFO - Scraping BRIT from myStocks Kenya\n",
      "2025-03-13 15:34:24,115 - INFO - Successfully scraped BRIT data\n",
      "2025-03-13 15:34:26,121 - INFO - Scraping CIC from myStocks Kenya\n",
      "2025-03-13 15:34:26,472 - INFO - Successfully scraped CIC data\n",
      "2025-03-13 15:34:28,485 - INFO - Scraping CTUM from myStocks Kenya\n",
      "2025-03-13 15:34:28,782 - INFO - Successfully scraped CTUM data\n",
      "2025-03-13 15:34:30,791 - INFO - Scraping NBV from myStocks Kenya\n",
      "2025-03-13 15:34:31,085 - INFO - Successfully scraped NBV data\n",
      "2025-03-13 15:34:33,087 - INFO - Scraping HAFR from myStocks Kenya\n",
      "2025-03-13 15:34:33,435 - INFO - Successfully scraped HAFR data\n",
      "2025-03-13 15:34:35,449 - INFO - Scraping TOTL from myStocks Kenya\n",
      "2025-03-13 15:34:35,761 - INFO - Successfully scraped TOTL data\n",
      "2025-03-13 15:34:37,765 - INFO - Scraping KENO from myStocks Kenya\n",
      "2025-03-13 15:34:38,253 - INFO - Successfully scraped KENO data\n",
      "2025-03-13 15:34:40,263 - INFO - Scraping KUKZ from myStocks Kenya\n",
      "2025-03-13 15:34:40,579 - INFO - Successfully scraped KUKZ data\n",
      "2025-03-13 15:34:42,682 - INFO - Scraping SASN from myStocks Kenya\n",
      "2025-03-13 15:34:43,616 - INFO - Successfully scraped SASN data\n",
      "2025-03-13 15:34:45,711 - INFO - Scraping KAPC from myStocks Kenya\n",
      "2025-03-13 15:34:46,008 - INFO - Successfully scraped KAPC data\n",
      "2025-03-13 15:34:48,011 - INFO - Scraping SIC from myStocks Kenya\n",
      "2025-03-13 15:34:48,195 - ERROR - Error scraping SIC from myStocks: 404 Client Error: Object \"SIC\" not found for url: https://live.mystocks.co.ke/stock=SIC\n",
      "2025-03-13 15:34:48,196 - INFO - Scraping SCAN from myStocks Kenya\n",
      "2025-03-13 15:34:48,586 - INFO - Successfully scraped SCAN data\n",
      "2025-03-13 15:34:50,590 - INFO - Scraping NMG from myStocks Kenya\n",
      "2025-03-13 15:34:50,924 - INFO - Successfully scraped NMG data\n",
      "2025-03-13 15:34:52,933 - INFO - Scraping LKL from myStocks Kenya\n",
      "2025-03-13 15:34:53,312 - INFO - Successfully scraped LKL data\n",
      "2025-03-13 15:34:55,324 - INFO - Scraping UMME from myStocks Kenya\n",
      "2025-03-13 15:34:55,684 - INFO - Successfully scraped UMME data\n",
      "2025-03-13 15:34:57,711 - INFO - Saved myStocks data to nse_data/mystocks/stock_data_20250313_153346.csv\n",
      "2025-03-13 15:34:57,717 - INFO - Scraping NSE indices data from Investing.com\n",
      "2025-03-13 15:34:57,723 - INFO - Scraping NSE_20 from https://www.investing.com/indices/kenya-nse-20-historical-data\n",
      "2025-03-13 15:35:00,565 - WARNING - No data table found for NSE_20\n",
      "2025-03-13 15:35:03,575 - INFO - Scraping NASI from https://www.investing.com/indices/nairobi-all-share-historical-data\n",
      "2025-03-13 15:35:05,534 - WARNING - No data table found for NASI\n",
      "2025-03-13 15:35:08,540 - INFO - Collecting information about NSE datasets from Mendeley Data\n",
      "2025-03-13 15:35:08,542 - INFO - Getting metadata for NSE_Stocks_2022 from https://data.mendeley.com/datasets/jmcdmnyh2s\n",
      "2025-03-13 15:35:10,901 - INFO - Successfully collected metadata for NSE_Stocks_2022\n",
      "2025-03-13 15:35:13,916 - INFO - Getting metadata for NSE_Stocks_2023_2024 from https://data.mendeley.com/datasets/ss5pfw8xnk/1\n",
      "2025-03-13 15:35:15,804 - INFO - Successfully collected metadata for NSE_Stocks_2023_2024\n",
      "2025-03-13 15:35:18,811 - INFO - Saved Mendeley dataset information to nse_data/mendeley/dataset_info_20250313_153346.csv\n",
      "2025-03-13 15:35:18,812 - INFO - Note: To download the actual datasets, please visit the Mendeley Data website\n",
      "2025-03-13 15:35:18,814 - INFO - Scraping completed! All data saved to c:\\xampp\\htdocs\\PesaGuru\\notebooks\\predictive_modeling\\nse_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping completed successfully!\n",
      "Data saved to: c:\\xampp\\htdocs\\PesaGuru\\notebooks\\predictive_modeling\\nse_data\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"nse_scraper.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class NSEDataScraper:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the NSE data scraper with necessary parameters\"\"\"\n",
    "        self.base_dir = 'nse_data'\n",
    "        os.makedirs(self.base_dir, exist_ok=True)\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Define NSE stock symbols from the provided data\n",
    "        self.stock_symbols = [\n",
    "            'EQTY', 'KCB', 'COOP', 'ABSA', 'SCBK', 'DTK', 'IMH', 'NCBA',\n",
    "            'SCOM', 'EABL', 'BAT', 'BAMB', 'KEGN', 'KPLC', 'JUB', 'BRIT',\n",
    "            'CIC', 'CTUM', 'NBV', 'HAFR', 'TOTL', 'KENO', 'KUKZ', 'SASN',\n",
    "            'KAPC', 'SIC', 'SCAN', 'NMG', 'LKL', 'UMME'\n",
    "        ]\n",
    "        \n",
    "        # Company codes with NSE suffix\n",
    "        self.nse_codes = [f\"{symbol}.NR\" for symbol in self.stock_symbols]\n",
    "        \n",
    "        # HTTP headers to mimic browser behavior\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Referer': 'https://www.google.com/'\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"NSE Scraper initialized with {len(self.stock_symbols)} stocks\")\n",
    "\n",
    "    def scrape_nse_market_statistics(self):\n",
    "        \"\"\"Scrape market statistics from the NSE official website\"\"\"\n",
    "        url = \"https://www.nse.co.ke/dataservices/market-statistics/\"\n",
    "        logger.info(f\"Scraping market statistics from {url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all tables on the page\n",
    "            tables = soup.find_all('table')\n",
    "            logger.info(f\"Found {len(tables)} tables on market statistics page\")\n",
    "            \n",
    "            # Process and store each table\n",
    "            for i, table in enumerate(tables):\n",
    "                try:\n",
    "                    df = pd.read_html(str(table))[0]\n",
    "                    \n",
    "                    # Create directory if it doesn't exist\n",
    "                    output_dir = f\"{self.base_dir}/market_statistics\"\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    \n",
    "                    # Save to CSV\n",
    "                    file_path = f\"{output_dir}/table_{i+1}_{self.timestamp}.csv\"\n",
    "                    df.to_csv(file_path, index=False)\n",
    "                    logger.info(f\"Saved table {i+1} with shape {df.shape} to {file_path}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing table {i+1}: {e}\")\n",
    "            \n",
    "            return True\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Error fetching market statistics: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def scrape_nse_historical_data(self):\n",
    "        \"\"\"Scrape historical data links from the NSE website\"\"\"\n",
    "        url = \"https://www.nse.co.ke/dataservices/historical-data/\"\n",
    "        logger.info(f\"Scraping historical data links from {url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all download links\n",
    "            download_links = []\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link.get('href', '')\n",
    "                if href.endswith(('.xlsx', '.xls', '.csv', '.pdf')) or 'download' in href.lower():\n",
    "                    download_links.append({\n",
    "                        'text': link.text.strip() or \"Unnamed Link\",\n",
    "                        'href': href,\n",
    "                        'date_scraped': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    })\n",
    "            \n",
    "            logger.info(f\"Found {len(download_links)} download links\")\n",
    "            \n",
    "            # Save links to CSV\n",
    "            if download_links:\n",
    "                links_df = pd.DataFrame(download_links)\n",
    "                output_dir = f\"{self.base_dir}/historical_data\"\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                file_path = f\"{output_dir}/download_links_{self.timestamp}.csv\"\n",
    "                links_df.to_csv(file_path, index=False)\n",
    "                logger.info(f\"Saved download links to {file_path}\")\n",
    "                \n",
    "                # Now download the files\n",
    "                self.download_files(links_df, f\"{output_dir}/files\")\n",
    "            \n",
    "            return True\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Error fetching historical data links: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def download_files(self, links_df, output_dir):\n",
    "        \"\"\"Download files from the extracted links\"\"\"\n",
    "        if links_df.empty:\n",
    "            logger.warning(\"No links provided for download\")\n",
    "            return\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        logger.info(f\"Downloading {len(links_df)} files to {output_dir}\")\n",
    "        \n",
    "        for i, row in links_df.iterrows():\n",
    "            try:\n",
    "                url = row['href']\n",
    "                # Handle relative URLs\n",
    "                if not url.startswith(('http://', 'https://')):\n",
    "                    url = f\"https://www.nse.co.ke{'' if url.startswith('/') else '/'}{url}\"\n",
    "                \n",
    "                logger.info(f\"Downloading file {i+1}/{len(links_df)}: {url}\")\n",
    "                response = requests.get(url, headers=self.headers, timeout=60)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Extract filename from URL or generate one\n",
    "                if '/' in url:\n",
    "                    filename = url.split('/')[-1]\n",
    "                    # Clean up filename\n",
    "                    filename = re.sub(r'[\\\\/*?:\"<>|]', \"_\", filename)\n",
    "                    if not filename or len(filename) < 3:\n",
    "                        filename = f\"file_{i+1}_{self.timestamp}.dat\"\n",
    "                else:\n",
    "                    filename = f\"file_{i+1}_{self.timestamp}.dat\"\n",
    "                \n",
    "                # Save the file\n",
    "                with open(f\"{output_dir}/{filename}\", 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                logger.info(f\"Successfully downloaded {filename}\")\n",
    "                \n",
    "                # Be polite to the server\n",
    "                time.sleep(2)\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error downloading file {i+1}: {e}\")\n",
    "    \n",
    "    def scrape_mystocks_data(self):\n",
    "        \"\"\"Scrape stock data from myStocks Kenya website\"\"\"\n",
    "        logger.info(\"Scraping data from myStocks Kenya\")\n",
    "        \n",
    "        all_stock_data = []\n",
    "        \n",
    "        for symbol in self.stock_symbols:\n",
    "            url = f\"https://live.mystocks.co.ke/stock={symbol}\"\n",
    "            logger.info(f\"Scraping {symbol} from myStocks Kenya\")\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(url, headers=self.headers, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Extract stock price\n",
    "                price = None\n",
    "                price_elem = soup.select_one('.roundbox .price')\n",
    "                if price_elem:\n",
    "                    price = price_elem.text.strip()\n",
    "                \n",
    "                # Extract price change\n",
    "                change = None\n",
    "                change_elem = soup.select_one('.roundbox .change')\n",
    "                if change_elem:\n",
    "                    change = change_elem.text.strip()\n",
    "                \n",
    "                # Extract volume\n",
    "                volume = None\n",
    "                volume_elem = soup.select_one('.roundbox .volume')\n",
    "                if volume_elem:\n",
    "                    volume = volume_elem.text.strip()\n",
    "                \n",
    "                # Find stock details table\n",
    "                details = {}\n",
    "                details_table = soup.select_one('table.infotable')\n",
    "                if details_table:\n",
    "                    rows = details_table.select('tr')\n",
    "                    for row in rows:\n",
    "                        cells = row.select('td')\n",
    "                        if len(cells) >= 2:\n",
    "                            key = cells[0].text.strip().rstrip(':')\n",
    "                            value = cells[1].text.strip()\n",
    "                            details[key] = value\n",
    "                \n",
    "                stock_data = {\n",
    "                    'Symbol': symbol,\n",
    "                    'Price': price,\n",
    "                    'Change': change,\n",
    "                    'Volume': volume,\n",
    "                    'Timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    **details\n",
    "                }\n",
    "                \n",
    "                all_stock_data.append(stock_data)\n",
    "                logger.info(f\"Successfully scraped {symbol} data\")\n",
    "                \n",
    "                # Be polite to the server\n",
    "                time.sleep(2)\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error scraping {symbol} from myStocks: {e}\")\n",
    "        \n",
    "        # Save the collected data\n",
    "        if all_stock_data:\n",
    "            df = pd.DataFrame(all_stock_data)\n",
    "            output_dir = f\"{self.base_dir}/mystocks\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            file_path = f\"{output_dir}/stock_data_{self.timestamp}.csv\"\n",
    "            df.to_csv(file_path, index=False)\n",
    "            logger.info(f\"Saved myStocks data to {file_path}\")\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            logger.warning(\"No stock data collected from myStocks Kenya\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def scrape_investing_data(self):\n",
    "        \"\"\"Scrape NSE indices data from Investing.com\"\"\"\n",
    "        logger.info(\"Scraping NSE indices data from Investing.com\")\n",
    "        \n",
    "        indices = [\n",
    "            {\n",
    "                'name': 'NSE_20',\n",
    "                'url': 'https://www.investing.com/indices/kenya-nse-20-historical-data'\n",
    "            },\n",
    "            {\n",
    "                'name': 'NASI',\n",
    "                'url': 'https://www.investing.com/indices/nairobi-all-share-historical-data'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for index in indices:\n",
    "            logger.info(f\"Scraping {index['name']} from {index['url']}\")\n",
    "            \n",
    "            try:\n",
    "                # Custom headers for Investing.com\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "                    'Accept-Language': 'en-US,en;q=0.9',\n",
    "                    'Referer': 'https://www.investing.com/'\n",
    "                }\n",
    "                \n",
    "                response = requests.get(index['url'], headers=headers, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Parse HTML\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Find the historical data table\n",
    "                table = soup.find('table', {'id': 'curr_table'})\n",
    "                \n",
    "                if table:\n",
    "                    # Extract historical data\n",
    "                    df = pd.read_html(str(table))[0]\n",
    "                    \n",
    "                    # Save to CSV\n",
    "                    output_dir = f\"{self.base_dir}/investing\"\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    file_path = f\"{output_dir}/{index['name']}_{self.timestamp}.csv\"\n",
    "                    df.to_csv(file_path, index=False)\n",
    "                    logger.info(f\"Saved {index['name']} data with {len(df)} rows to {file_path}\")\n",
    "                else:\n",
    "                    logger.warning(f\"No data table found for {index['name']}\")\n",
    "                \n",
    "                # Be polite to the server\n",
    "                time.sleep(3)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error scraping {index['name']} from Investing.com: {e}\")\n",
    "    \n",
    "    def scrape_mendeley_datasets(self):\n",
    "        \"\"\"Get metadata about NSE datasets from Mendeley Data\"\"\"\n",
    "        logger.info(\"Collecting information about NSE datasets from Mendeley Data\")\n",
    "        \n",
    "        datasets = [\n",
    "            {\n",
    "                'name': 'NSE_Stocks_2022',\n",
    "                'url': 'https://data.mendeley.com/datasets/jmcdmnyh2s'\n",
    "            },\n",
    "            {\n",
    "                'name': 'NSE_Stocks_2023_2024',\n",
    "                'url': 'https://data.mendeley.com/datasets/ss5pfw8xnk/1'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        dataset_info = []\n",
    "        \n",
    "        for dataset in datasets:\n",
    "            logger.info(f\"Getting metadata for {dataset['name']} from {dataset['url']}\")\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(dataset['url'], headers=self.headers, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Extract dataset metadata\n",
    "                title_elem = soup.select_one('h1.title')\n",
    "                title = title_elem.text.strip() if title_elem else dataset['name']\n",
    "                \n",
    "                # Extract description\n",
    "                desc_elem = soup.select_one('.description')\n",
    "                description = desc_elem.text.strip() if desc_elem else \"No description available\"\n",
    "                \n",
    "                # Extract file information\n",
    "                files = []\n",
    "                file_elems = soup.select('.file-list-item')\n",
    "                for file_elem in file_elems:\n",
    "                    file_name_elem = file_elem.select_one('.name')\n",
    "                    file_name = file_name_elem.text.strip() if file_name_elem else \"Unknown file\"\n",
    "                    files.append(file_name)\n",
    "                \n",
    "                dataset_info.append({\n",
    "                    'Dataset': dataset['name'],\n",
    "                    'Title': title,\n",
    "                    'URL': dataset['url'],\n",
    "                    'Description': description,\n",
    "                    'Files': \", \".join(files) if files else \"File list not available\",\n",
    "                    'Timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                })\n",
    "                \n",
    "                logger.info(f\"Successfully collected metadata for {dataset['name']}\")\n",
    "                \n",
    "                # Be polite to the server\n",
    "                time.sleep(3)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error collecting metadata for {dataset['name']}: {e}\")\n",
    "                dataset_info.append({\n",
    "                    'Dataset': dataset['name'],\n",
    "                    'Title': dataset['name'],\n",
    "                    'URL': dataset['url'],\n",
    "                    'Description': \"Error collecting metadata\",\n",
    "                    'Files': \"Not available\",\n",
    "                    'Timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                })\n",
    "        \n",
    "        # Save dataset information\n",
    "        if dataset_info:\n",
    "            df = pd.DataFrame(dataset_info)\n",
    "            output_dir = f\"{self.base_dir}/mendeley\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            file_path = f\"{output_dir}/dataset_info_{self.timestamp}.csv\"\n",
    "            df.to_csv(file_path, index=False)\n",
    "            logger.info(f\"Saved Mendeley dataset information to {file_path}\")\n",
    "            \n",
    "            # Note: Direct download may require authentication,\n",
    "            # so we're just collecting metadata here\n",
    "            logger.info(\"Note: To download the actual datasets, please visit the Mendeley Data website\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run all scraping functions\"\"\"\n",
    "        logger.info(f\"Starting NSE data scraping at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Create the main directory\n",
    "        os.makedirs(self.base_dir, exist_ok=True)\n",
    "        \n",
    "        # Scrape NSE official website\n",
    "        self.scrape_nse_market_statistics()\n",
    "        self.scrape_nse_historical_data()\n",
    "        \n",
    "        # Scrape additional sources\n",
    "        self.scrape_mystocks_data()\n",
    "        self.scrape_investing_data()\n",
    "        self.scrape_mendeley_datasets()\n",
    "        \n",
    "        logger.info(f\"Scraping completed! All data saved to {os.path.abspath(self.base_dir)}\")\n",
    "        return os.path.abspath(self.base_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = NSEDataScraper()\n",
    "    output_path = scraper.run()\n",
    "    print(f\"\\nScraping completed successfully!\\nData saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70fafc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample NSE data if API fetch fails\n",
    "def load_sample_nse_data():\n",
    "    \"\"\"\n",
    "    Load sample NSE stock data for demonstration purposes\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with sample historical stock prices\n",
    "    \"\"\"\n",
    "    # Create sample data for key NSE stocks with realistic values\n",
    "    # We'll generate synthetic data based on real NSE stock behavior\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    # Define stock parameters (based on historical NSE performance)\n",
    "    stocks = {\n",
    "        'SCOM.NR': {'mean': 0.0005, 'std': 0.015, 'start_price': 42.75},  # Safaricom\n",
    "        'EQTY.NR': {'mean': 0.0003, 'std': 0.012, 'start_price': 51.00},  # Equity Group\n",
    "        'KCB.NR': {'mean': 0.0002, 'std': 0.014, 'start_price': 45.50},   # KCB Group\n",
    "        'COOP.NR': {'mean': 0.0003, 'std': 0.013, 'start_price': 16.00},  # Cooperative Bank\n",
    "        'EABL.NR': {'mean': 0.0001, 'std': 0.011, 'start_price': 170.00}, # East African Breweries\n",
    "        'BAT.NR': {'mean': -0.0001, 'std': 0.010, 'start_price': 800.00}, # BAT Kenya\n",
    "        'ABSA.NR': {'mean': 0.0002, 'std': 0.012, 'start_price': 11.75},  # Absa Bank Kenya\n",
    "        'JUB.NR': {'mean': 0.0004, 'std': 0.014, 'start_price': 305.00}   # Jubilee Holdings\n",
    "    }\n",
    "    \n",
    "    # Generate dates (2 years of trading days - approximately 504 days)\n",
    "    end_date = datetime.now()\n",
    "    dates = [end_date - timedelta(days=i) for i in range(504)]\n",
    "    dates.reverse()  # Sort chronologically\n",
    "    dates = [date for date in dates if date.weekday() < 5]  # Keep only weekdays\n",
    "    \n",
    "    # Create empty DataFrame\n",
    "    all_data = pd.DataFrame()\n",
    "    \n",
    "    # Generate data for each stock\n",
    "    for ticker, params in stocks.items():\n",
    "        # Generate returns using random walk with drift\n",
    "        returns = np.random.normal(params['mean'], params['std'], size=len(dates))\n",
    "        \n",
    "        # Convert returns to prices\n",
    "        price = params['start_price']\n",
    "        prices = [price]\n",
    "        \n",
    "        for ret in returns[1:]:\n",
    "            price = price * (1 + ret)\n",
    "            prices.append(price)\n",
    "        \n",
    "        # Create DataFrame for this stock\n",
    "        stock_data = pd.DataFrame({\n",
    "            'Date': dates,\n",
    "            'Open': prices,\n",
    "            'High': [p * (1 + abs(np.random.normal(0, 0.005))) for p in prices],\n",
    "            'Low': [p * (1 - abs(np.random.normal(0, 0.005))) for p in prices],\n",
    "            'Close': prices,\n",
    "            'Adj Close': prices,\n",
    "            'Volume': [int(np.random.normal(500000, 100000)) for _ in range(len(dates))],\n",
    "            'Symbol': ticker\n",
    "        })\n",
    "        \n",
    "        # Append to main DataFrame\n",
    "        if all_data.empty:\n",
    "            all_data = stock_data\n",
    "        else:\n",
    "            all_data = pd.concat([all_data, stock_data])\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b12715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Returns:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Symbol</th>\n",
       "      <th>ABSA.NR</th>\n",
       "      <th>BAT.NR</th>\n",
       "      <th>COOP.NR</th>\n",
       "      <th>EABL.NR</th>\n",
       "      <th>EQTY.NR</th>\n",
       "      <th>JUB.NR</th>\n",
       "      <th>KCB.NR</th>\n",
       "      <th>SCOM.NR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-10-30 14:28:08.729639</th>\n",
       "      <td>-0.009045</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>-0.001094</td>\n",
       "      <td>0.005783</td>\n",
       "      <td>0.014382</td>\n",
       "      <td>-0.017907</td>\n",
       "      <td>-0.029490</td>\n",
       "      <td>-0.001574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-31 14:28:08.729639</th>\n",
       "      <td>0.013920</td>\n",
       "      <td>0.008033</td>\n",
       "      <td>0.016164</td>\n",
       "      <td>-0.010695</td>\n",
       "      <td>0.002472</td>\n",
       "      <td>0.004231</td>\n",
       "      <td>-0.008296</td>\n",
       "      <td>0.010215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-01 14:28:08.729639</th>\n",
       "      <td>0.002418</td>\n",
       "      <td>-0.008539</td>\n",
       "      <td>-0.013697</td>\n",
       "      <td>-0.015353</td>\n",
       "      <td>-0.015262</td>\n",
       "      <td>0.022336</td>\n",
       "      <td>0.006608</td>\n",
       "      <td>0.023345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-02 14:28:08.729639</th>\n",
       "      <td>0.003226</td>\n",
       "      <td>-0.017537</td>\n",
       "      <td>-0.009613</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.005096</td>\n",
       "      <td>0.008787</td>\n",
       "      <td>-0.038265</td>\n",
       "      <td>-0.003012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-03 14:28:08.729639</th>\n",
       "      <td>-0.023444</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.013772</td>\n",
       "      <td>-0.003407</td>\n",
       "      <td>-0.007516</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>-0.006796</td>\n",
       "      <td>-0.003012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Symbol                       ABSA.NR    BAT.NR   COOP.NR   EABL.NR   EQTY.NR  \\\n",
       "Date                                                                           \n",
       "2023-10-30 14:28:08.729639 -0.009045  0.000550 -0.001094  0.005783  0.014382   \n",
       "2023-10-31 14:28:08.729639  0.013920  0.008033  0.016164 -0.010695  0.002472   \n",
       "2023-11-01 14:28:08.729639  0.002418 -0.008539 -0.013697 -0.015353 -0.015262   \n",
       "2023-11-02 14:28:08.729639  0.003226 -0.017537 -0.009613  0.001279  0.005096   \n",
       "2023-11-03 14:28:08.729639 -0.023444  0.001468  0.013772 -0.003407 -0.007516   \n",
       "\n",
       "Symbol                        JUB.NR    KCB.NR   SCOM.NR  \n",
       "Date                                                      \n",
       "2023-10-30 14:28:08.729639 -0.017907 -0.029490 -0.001574  \n",
       "2023-10-31 14:28:08.729639  0.004231 -0.008296  0.010215  \n",
       "2023-11-01 14:28:08.729639  0.022336  0.006608  0.023345  \n",
       "2023-11-02 14:28:08.729639  0.008787 -0.038265 -0.003012  \n",
       "2023-11-03 14:28:08.729639  0.002859 -0.006796 -0.003012  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to preprocess stock data\n",
    "def preprocess_stock_data(data):\n",
    "    \"\"\"\n",
    "    Preprocess stock data for portfolio optimization\n",
    "    \n",
    "    Parameters:\n",
    "    - data (DataFrame): Raw stock data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with daily returns\n",
    "    - DataFrame with monthly returns\n",
    "    \"\"\"\n",
    "    # Make a copy of the data\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Pivot the data to have dates as index and tickers as columns\n",
    "    pivot_df = df.pivot(index='Date', columns='Symbol', values='Adj Close')\n",
    "    \n",
    "    # Calculate daily returns\n",
    "    daily_returns = pivot_df.pct_change().dropna()\n",
    "    \n",
    "    # Calculate monthly returns (resample to month end)\n",
    "    monthly_returns = pivot_df.resample('M').last().pct_change().dropna()\n",
    "    \n",
    "    return daily_returns, monthly_returns\n",
    "\n",
    "# Preprocess the stock data\n",
    "daily_returns, monthly_returns = preprocess_stock_data(stock_data)\n",
    "\n",
    "# Display first few rows of daily returns\n",
    "print(\"Daily Returns:\")\n",
    "daily_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8739d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and visualize correlation matrix\n",
    "correlation_matrix = daily_returns.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, linewidths=0.5)\n",
    "plt.title('Correlation Matrix of NSE Stocks', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and visualize annualized returns and volatility\n",
    "# We'll use daily data for more accuracy\n",
    "mean_returns = daily_returns.mean() * 252  # Annualize (252 trading days)\n",
    "volatility = daily_returns.std() * np.sqrt(252)  # Annualized volatility\n",
    "\n",
    "# Create DataFrame with return and risk metrics\n",
    "risk_return = pd.DataFrame({\n",
    "    'Annualized Return': mean_returns,\n",
    "    'Annualized Volatility': volatility\n",
    "})\n",
    "\n",
    "# Display risk-return metrics\n",
    "print(\"Risk-Return Metrics:\")\n",
    "risk_return.sort_values('Annualized Return', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253ee3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot risk-return scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(risk_return['Annualized Volatility'], risk_return['Annualized Return'], s=100)\n",
    "\n",
    "# Add labels for each stock\n",
    "for i, ticker in enumerate(risk_return.index):\n",
    "    plt.annotate(ticker.replace('.NR', ''), \n",
    "                 (risk_return['Annualized Volatility'][i], risk_return['Annualized Return'][i]),\n",
    "                 xytext=(10, 5), textcoords='offset points', fontsize=12)\n",
    "\n",
    "plt.xlabel('Annualized Volatility (Risk)', fontsize=14)\n",
    "plt.ylabel('Annualized Return', fontsize=14)\n",
    "plt.title('Risk-Return Profile of NSE Stocks', fontsize=16)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f34049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for portfolio calculations\n",
    "\n",
    "def portfolio_performance(weights, mean_returns, cov_matrix):\n",
    "    \"\"\"\n",
    "    Calculate portfolio return and volatility\n",
    "    \n",
    "    Parameters:\n",
    "    - weights: Portfolio weights\n",
    "    - mean_returns: Expected returns\n",
    "    - cov_matrix: Covariance matrix\n",
    "    \n",
    "    Returns:\n",
    "    - returns, volatility, sharpe_ratio\n",
    "    \"\"\"\n",
    "    returns = np.sum(mean_returns * weights) * 252  # Annualized return\n",
    "    volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix * 252, weights)))  # Annualized volatility\n",
    "    sharpe_ratio = returns / volatility  # Sharpe ratio (assuming 0% risk-free rate)\n",
    "    return returns, volatility, sharpe_ratio\n",
    "\n",
    "def negative_sharpe(weights, mean_returns, cov_matrix):\n",
    "    \"\"\"\n",
    "    Return negative Sharpe ratio (for minimization)\n",
    "    \"\"\"\n",
    "    returns, volatility, sharpe = portfolio_performance(weights, mean_returns, cov_matrix)\n",
    "    return -sharpe\n",
    "\n",
    "def portfolio_volatility(weights, mean_returns, cov_matrix):\n",
    "    \"\"\"\n",
    "    Return portfolio volatility (for minimization)\n",
    "    \"\"\"\n",
    "    return portfolio_performance(weights, mean_returns, cov_matrix)[1]\n",
    "\n",
    "def portfolio_return(weights, mean_returns, cov_matrix):\n",
    "    \"\"\"\n",
    "    Return portfolio return (for minimization)\n",
    "    \"\"\"\n",
    "    return portfolio_performance(weights, mean_returns, cov_matrix)[0]\n",
    "\n",
    "def max_sharpe_ratio(mean_returns, cov_matrix, num_assets):\n",
    "    \"\"\"\n",
    "    Find portfolio with maximum Sharpe ratio\n",
    "    \"\"\"\n",
    "    # Initial guess (equal weights)\n",
    "    args = (mean_returns, cov_matrix)\n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    bounds = tuple((0, 1) for _ in range(num_assets))\n",
    "    \n",
    "    # Optimize\n",
    "    result = sco.minimize(negative_sharpe, num_assets * [1./num_assets], args=args,\n",
    "                        method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def min_volatility(mean_returns, cov_matrix, num_assets):\n",
    "    \"\"\"\n",
    "    Find portfolio with minimum volatility\n",
    "    \"\"\"\n",
    "    # Initial guess (equal weights)\n",
    "    args = (mean_returns, cov_matrix)\n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    bounds = tuple((0, 1) for _ in range(num_assets))\n",
    "    \n",
    "    # Optimize\n",
    "    result = sco.minimize(portfolio_volatility, num_assets * [1./num_assets], args=args,\n",
    "                        method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def efficient_frontier(mean_returns, cov_matrix, num_assets, returns_range):\n",
    "    \"\"\"\n",
    "    Calculate efficient frontier for a range of target returns\n",
    "    \"\"\"\n",
    "    efficient_portfolios = []\n",
    "    \n",
    "    for target_return in returns_range:\n",
    "        args = (mean_returns, cov_matrix)\n",
    "        constraints = (\n",
    "            {'type': 'eq', 'fun': lambda x: portfolio_return(x, mean_returns, cov_matrix) - target_return},\n",
    "            {'type': 'eq', 'fun': lambda x: np.sum(x) - 1}\n",
    "        )\n",
    "        bounds = tuple((0, 1) for _ in range(num_assets))\n",
    "        \n",
    "        # Find minimum volatility for target return\n",
    "        result = sco.minimize(portfolio_volatility, num_assets * [1./num_assets], args=args,\n",
    "                            method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "        \n",
    "        if result['success']:\n",
    "            efficient_portfolios.append({\n",
    "                'return': target_return,\n",
    "                'volatility': result['fun'],\n",
    "                'weights': result['x']\n",
    "            })\n",
    "    \n",
    "    return efficient_portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75064a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate efficient frontier\n",
    "\n",
    "# Get mean returns and covariance matrix\n",
    "mean_returns_daily = daily_returns.mean()\n",
    "cov_matrix = daily_returns.cov()\n",
    "num_assets = len(mean_returns_daily)\n",
    "\n",
    "# Find portfolio with maximum Sharpe ratio\n",
    "max_sharpe_result = max_sharpe_ratio(mean_returns_daily, cov_matrix, num_assets)\n",
    "max_sharpe_weights = max_sharpe_result['x']\n",
    "max_sharpe_returns, max_sharpe_volatility, max_sharpe = portfolio_performance(max_sharpe_weights, mean_returns_daily, cov_matrix)\n",
    "\n",
    "# Find portfolio with minimum volatility\n",
    "min_vol_result = min_volatility(mean_returns_daily, cov_matrix, num_assets)\n",
    "min_vol_weights = min_vol_result['x']\n",
    "min_vol_returns, min_vol_volatility, min_vol_sharpe = portfolio_performance(min_vol_weights, mean_returns_daily, cov_matrix)\n",
    "\n",
    "# Print results\n",
    "print(\"Maximum Sharpe Ratio Portfolio:\")\n",
    "print(f\"Annual Return: {max_sharpe_returns:.4f}\")\n",
    "print(f\"Annual Volatility: {max_sharpe_volatility:.4f}\")\n",
    "print(f\"Sharpe Ratio: {max_sharpe:.4f}\")\n",
    "print(\"\\nWeights:\")\n",
    "for i, ticker in enumerate(daily_returns.columns):\n",
    "    print(f\"{ticker}: {max_sharpe_weights[i]:.4f}\")\n",
    "\n",
    "print(\"\\nMinimum Volatility Portfolio:\")\n",
    "print(f\"Annual Return: {min_vol_returns:.4f}\")\n",
    "print(f\"Annual Volatility: {min_vol_volatility:.4f}\")\n",
    "print(f\"Sharpe Ratio: {min_vol_sharpe:.4f}\")\n",
    "print(\"\\nWeights:\")\n",
    "for i, ticker in enumerate(daily_returns.columns):\n",
    "    print(f\"{ticker}: {min_vol_weights[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e11e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate efficient frontier\n",
    "target_returns = np.linspace(min_vol_returns, max_sharpe_returns, 30)\n",
    "efficient_portfolios = efficient_frontier(mean_returns_daily, cov_matrix, num_assets, target_returns)\n",
    "\n",
    "# Extract results\n",
    "ef_returns = [p['return'] for p in efficient_portfolios]\n",
    "ef_volatility = [p['volatility'] for p in efficient_portfolios]\n",
    "\n",
    "# Generate random portfolios for visualization\n",
    "num_random_portfolios = 5000\n",
    "random_returns = []\n",
    "random_volatility = []\n",
    "\n",
    "for _ in range(num_random_portfolios):\n",
    "    # Generate random weights\n",
    "    weights = np.random.random(num_assets)\n",
    "    weights /= np.sum(weights)\n",
    "    \n",
    "    # Calculate portfolio performance\n",
    "    returns, volatility, _ = portfolio_performance(weights, mean_returns_daily, cov_matrix)\n",
    "    random_returns.append(returns)\n",
    "    random_volatility.append(volatility)\n",
    "\n",
    "# Plot efficient frontier\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot random portfolios\n",
    "plt.scatter(random_volatility, random_returns, c='lightgray', marker='.', s=10, alpha=0.3, label='Random Portfolios')\n",
    "\n",
    "# Plot efficient frontier\n",
    "plt.plot(ef_volatility, ef_returns, 'b-', linewidth=3, label='Efficient Frontier')\n",
    "\n",
    "# Plot maximum Sharpe ratio portfolio\n",
    "plt.scatter(max_sharpe_volatility, max_sharpe_returns, marker='*', color='red', s=300, label='Maximum Sharpe Ratio')\n",
    "\n",
    "# Plot minimum volatility portfolio\n",
    "plt.scatter(min_vol_volatility, min_vol_returns, marker='o', color='green', s=200, label='Minimum Volatility')\n",
    "\n",
    "# Plot individual assets\n",
    "for i, ticker in enumerate(daily_returns.columns):\n",
    "    asset_vol = volatility[i]\n",
    "    asset_ret = mean_returns[i]\n",
    "    plt.scatter(asset_vol, asset_ret, marker='x', s=100, label=ticker)\n",
    "    plt.annotate(ticker.replace('.NR', ''), (asset_vol, asset_ret), xytext=(10, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "plt.title('Efficient Frontier with NSE Stocks', fontsize=16)\n",
    "plt.xlabel('Annualized Volatility', fontsize=14)\n",
    "plt.ylabel('Annualized Return', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f769892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recommend portfolio based on risk profile\n",
    "def recommend_portfolio(risk_profile, mean_returns, cov_matrix, tickers):\n",
    "    \"\"\"\n",
    "    Recommend a portfolio based on user's risk profile\n",
    "    \n",
    "    Parameters:\n",
    "    - risk_profile: 'conservative', 'moderate', 'aggressive'\n",
    "    - mean_returns: Mean returns Series\n",
    "    - cov_matrix: Covariance matrix\n",
    "    - tickers: List of stock tickers\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with portfolio details\n",
    "    \"\"\"\n",
    "    num_assets = len(mean_returns)\n",
    "    \n",
    "    # Get minimum volatility and maximum Sharpe ratio portfolios\n",
    "    min_vol_result = min_volatility(mean_returns, cov_matrix, num_assets)\n",
    "    min_vol_weights = min_vol_result['x']\n",
    "    min_vol_returns, min_vol_volatility, _ = portfolio_performance(min_vol_weights, mean_returns, cov_matrix)\n",
    "    \n",
    "    max_sharpe_result = max_sharpe_ratio(mean_returns, cov_matrix, num_assets)\n",
    "    max_sharpe_weights = max_sharpe_result['x']\n",
    "    max_sharpe_returns, max_sharpe_volatility, _ = portfolio_performance(max_sharpe_weights, mean_returns, cov_matrix)\n",
    "    \n",
    "    # Define target return based on risk profile\n",
    "    if risk_profile.lower() == 'conservative':\n",
    "        # Conservative investors prioritize capital preservation\n",
    "        target_return = min_vol_returns  # Use minimum volatility portfolio\n",
    "        weights = min_vol_weights\n",
    "        expected_return = min_vol_returns\n",
    "        expected_volatility = min_vol_volatility\n",
    "    \n",
    "    elif risk_profile.lower() == 'aggressive':\n",
    "        # Aggressive investors prioritize maximum returns\n",
    "        target_return = max_sharpe_returns  # Use maximum Sharpe ratio portfolio\n",
    "        weights = max_sharpe_weights\n",
    "        expected_return = max_sharpe_returns\n",
    "        expected_volatility = max_sharpe_volatility\n",
    "    \n",
    "    elif risk_profile.lower() == 'moderate':\n",
    "        # Moderate investors seek balance between risk and return\n",
    "        # Target return halfway between min volatility and max Sharpe\n",
    "        target_return = (min_vol_returns + max_sharpe_returns) / 2\n",
    "        \n",
    "        # Find portfolio with this target return\n",
    "        args = (mean_returns, cov_matrix)\n",
    "        constraints = (\n",
    "            {'type': 'eq', 'fun': lambda x: portfolio_return(x, mean_returns, cov_matrix) - target_return},\n",
    "            {'type': 'eq', 'fun': lambda x: np.sum(x) - 1}\n",
    "        )\n",
    "        bounds = tuple((0, 1) for _ in range(num_assets))\n",
    "        \n",
    "        result = sco.minimize(portfolio_volatility, num_assets * [1./num_assets], args=args,\n",
    "                            method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "        \n",
    "        weights = result['x']\n",
    "        expected_return, expected_volatility, _ = portfolio_performance(weights, mean_returns, cov_matrix)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Risk profile must be 'conservative', 'moderate', or 'aggressive'\")\n",
    "    \n",
    "    # Create portfolio recommendation\n",
    "    portfolio = {\n",
    "        'risk_profile': risk_profile,\n",
    "        'expected_annual_return': expected_return,\n",
    "        'expected_annual_volatility': expected_volatility,\n",
    "        'allocations': []\n",
    "    }\n",
    "    \n",
    "    # Add allocations with minimum 0.5% threshold to avoid tiny positions\n",
    "    for i, ticker in enumerate(tickers):\n",
    "        weight = weights[i]\n",
    "        if weight >= 0.005:  # 0.5% minimum allocation\n",
    "            portfolio['allocations'].append({\n",
    "                'ticker': ticker.replace('.NR', ''),  # Remove suffix for display\n",
    "                'weight': weight,\n",
    "                'amount_per_10k': weight * 10000  # Amount to invest per KES 10,000\n",
    "            })\n",
    "    \n",
    "    # Sort allocations by weight (descending)\n",
    "    portfolio['allocations'] = sorted(portfolio['allocations'], key=lambda x: x['weight'], reverse=True)\n",
    "    \n",
    "    return portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4870f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4c4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate portfolio recommendations for different risk profiles\n",
    "risk_profiles = ['Conservative', 'Moderate', 'Aggressive']\n",
    "portfolio_recommendations = {}\n",
    "\n",
    "for profile in risk_profiles:\n",
    "    portfolio = recommend_portfolio(profile, mean_returns_daily, cov_matrix, daily_returns.columns)\n",
    "    portfolio_recommendations[profile] = portfolio\n",
    "\n",
    "# Display portfolio recommendations\n",
    "for profile, portfolio in portfolio_recommendations.items():\n",
    "    print(f\"\\n{profile} Portfolio Recommendation:\")\n",
    "    print(f\"Expected Annual Return: {portfolio['expected_annual_return']:.2%}\")\n",
    "    print(f\"Expected Annual Volatility: {portfolio['expected_annual_volatility']:.2%}\")\n",
    "    print(\"\\nAllocations:\")\n",
    "    for allocation in portfolio['allocations']:\n",
    "        print(f\"{allocation['ticker']}: {allocation['weight']:.2%} (KES {allocation['amount_per_10k']:.0f} per KES 10,000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276c8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visualization of asset allocations for different risk profiles\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 8))\n",
    "\n",
    "for i, profile in enumerate(risk_profiles):\n",
    "    portfolio = portfolio_recommendations[profile]\n",
    "    \n",
    "    # Extract data for plotting\n",
    "    labels = [alloc['ticker'] for alloc in portfolio['allocations']]\n",
    "    sizes = [alloc['weight'] for alloc in portfolio['allocations']]\n",
    "    \n",
    "    # Create pie chart\n",
    "    axes[i].pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, shadow=False)\n",
    "    axes[i].set_title(f\"{profile} Portfolio\\nReturn: {portfolio['expected_annual_return']:.2%}, Risk: {portfolio['expected_annual_volatility']:.2%}\", fontsize=14)\n",
    "    axes[i].axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "\n",
    "plt.suptitle('Portfolio Allocations by Risk Profile', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5604c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a portfolio optimization class for export\n",
    "class PortfolioOptimizer:\n",
    "    \"\"\"\n",
    "    Portfolio Optimization for Kenyan market\n",
    "    \"\"\"\n",
    "    def __init__(self, mean_returns, cov_matrix, tickers):\n",
    "        self.mean_returns = mean_returns\n",
    "        self.cov_matrix = cov_matrix\n",
    "        self.tickers = tickers\n",
    "        self.num_assets = len(tickers)\n",
    "    \n",
    "    def portfolio_performance(self, weights):\n",
    "        \"\"\"\n",
    "        Calculate portfolio performance metrics\n",
    "        \"\"\"\n",
    "        returns = np.sum(self.mean_returns * weights) * 252\n",
    "        volatility = np.sqrt(np.dot(weights.T, np.dot(self.cov_matrix * 252, weights)))\n",
    "        sharpe_ratio = returns / volatility\n",
    "        return returns, volatility, sharpe_ratio\n",
    "    \n",
    "    def optimize_portfolio(self, risk_profile):\n",
    "        \"\"\"\n",
    "        Optimize portfolio based on risk profile\n",
    "        \"\"\"\n",
    "        return recommend_portfolio(risk_profile, self.mean_returns, self.cov_matrix, self.tickers)\n",
    "    \n",
    "    def generate_efficient_frontier(self, num_points=30):\n",
    "        \"\"\"\n",
    "        Generate efficient frontier\n",
    "        \"\"\"\n",
    "        # Get min vol and max Sharpe portfolios\n",
    "        min_vol_result = min_volatility(self.mean_returns, self.cov_matrix, self.num_assets)\n",
    "        min_vol_weights = min_vol_result['x']\n",
    "        min_vol_returns, min_vol_volatility, _ = portfolio_performance(min_vol_weights, self.mean_returns, self.cov_matrix)\n",
    "        \n",
    "        max_sharpe_result = max_sharpe_ratio(self.mean_returns, self.cov_matrix, self.num_assets)\n",
    "        max_sharpe_weights = max_sharpe_result['x']\n",
    "        max_sharpe_returns, max_sharpe_volatility, _ = portfolio_performance(max_sharpe_weights, self.mean_returns, self.cov_matrix)\n",
    "        \n",
    "        # Calculate efficient frontier\n",
    "        target_returns = np.linspace(min_vol_returns, max_sharpe_returns * 1.2, num_points)\n",
    "        efficient_portfolios = efficient_frontier(self.mean_returns, self.cov_matrix, self.num_assets, target_returns)\n",
    "        \n",
    "        return {\n",
    "            'min_vol': {'return': min_vol_returns, 'volatility': min_vol_volatility, 'weights': min_vol_weights},\n",
    "            'max_sharpe': {'return': max_sharpe_returns, 'volatility': max_sharpe_volatility, 'weights': max_sharpe_weights},\n",
    "            'efficient_frontier': efficient_portfolios\n",
    "        }\n",
    "\n",
    "# Create and export the model\n",
    "optimizer = PortfolioOptimizer(mean_returns_daily, cov_matrix, daily_returns.columns)\n",
    "\n",
    "# Save the model\n",
    "output_path = '../models/portfolio_optimizer.pkl'\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(optimizer, f)\n",
    "\n",
    "print(f\"Portfolio optimization model saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef42d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use the model in the PesaGuru chatbot\n",
    "def sample_chatbot_integration():\n",
    "    \"\"\"\n",
    "    Demonstrate how the PesaGuru chatbot would use the portfolio optimizer\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    with open('../models/portfolio_optimizer.pkl', 'rb') as f:\n",
    "        optimizer = pickle.load(f)\n",
    "    \n",
    "    # Example user input\n",
    "    user_risk_profile = 'moderate'  # From user questionnaire\n",
    "    investment_amount = 100000  # KES 100,000\n",
    "    \n",
    "    # Get portfolio recommendation\n",
    "    portfolio = optimizer.optimize_portfolio(user_risk_profile)\n",
    "    \n",
    "    # Format response for chatbot\n",
    "    response = {\n",
    "        'message': f\"Based on your {user_risk_profile} risk profile, here's your recommended investment portfolio:\",\n",
    "        'expected_return': f\"{portfolio['expected_annual_return']:.2%} per year\",\n",
    "        'expected_risk': f\"{portfolio['expected_annual_volatility']:.2%} annual volatility\",\n",
    "        'allocations': []\n",
    "    }\n",
    "    \n",
    "    # Calculate allocation amounts\n",
    "    for allocation in portfolio['allocations']:\n",
    "        amount = allocation['weight'] * investment_amount\n",
    "        response['allocations'].append({\n",
    "            'ticker': allocation['ticker'],\n",
    "            'percentage': f\"{allocation['weight']:.2%}\",\n",
    "            'amount': f\"KES {amount:,.0f}\"\n",
    "        })\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Show example response\n",
    "chatbot_response = sample_chatbot_integration()\n",
    "print(chatbot_response['message'])\n",
    "print(f\"Expected return: {chatbot_response['expected_return']}\")\n",
    "print(f\"Expected risk: {chatbot_response['expected_risk']}\")\n",
    "print(\"\\nRecommended allocations:\")\n",
    "for alloc in chatbot_response['allocations']:\n",
    "    print(f\"{alloc['ticker']}: {alloc['percentage']} ({alloc['amount']})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pesaguru",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
