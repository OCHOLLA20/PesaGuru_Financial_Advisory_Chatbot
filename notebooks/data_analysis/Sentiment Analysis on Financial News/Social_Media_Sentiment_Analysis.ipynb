{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee56cf83-3d57-46e4-9807-87d928708809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Load spaCy model for English\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except:\n",
    "    # If the model isn't installed, download it\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# For saving outputs\n",
    "OUTPUT_DIR = \"../../../outputs/sentiment_analysis\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# API configuration\n",
    "API_CONFIG = {\n",
    "    \"twitter\": {\n",
    "        \"rapidapi_key\": \"29a112a0f4mshdb1b2aa2ac46841p1b3131jsn23bae608f9ab\"\n",
    "    },\n",
    "    \"reddit\": {\n",
    "         \"rapidapi_key\": \"29a112a0f4mshdb1b2aa2ac46841p1b3131jsn23bae608f9ab\"\n",
    "    },\n",
    "    \"news\": {\n",
    "        \"rapidapi_key\": \"29a112a0f4mshdb1b2aa2ac46841p1b3131jsn23bae608f9ab\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c91e2ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_twitter_data(query, count=100, lang=\"en\"):\n",
    "    \"\"\"\n",
    "    Fetch tweets related to a specific query using RapidAPI\n",
    "    \n",
    "    Args:\n",
    "        query (str): Search query (e.g., \"#SCOM\", \"Safaricom stock\")\n",
    "        count (int): Number of tweets to retrieve\n",
    "        lang (str): Language code\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing tweets and metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = \"https://twitter154.p.rapidapi.com/search/search\"\n",
    "        \n",
    "        querystring = {\n",
    "            \"query\": query,\n",
    "            \"section\": \"top\",\n",
    "            \"min_retweets\": \"5\",\n",
    "            \"min_likes\": \"5\", \n",
    "            \"limit\": str(count),\n",
    "            \"language\": lang\n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            \"X-RapidAPI-Key\": API_CONFIG[\"twitter\"][\"rapidapi_key\"],\n",
    "            \"X-RapidAPI-Host\": \"twitter154.p.rapidapi.com\"\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, params=querystring)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            # Extract relevant fields\n",
    "            tweets = []\n",
    "            for result in data.get('results', []):\n",
    "                tweet = {\n",
    "                    'id': result.get('tweet_id'),\n",
    "                    'text': result.get('text'),\n",
    "                    'created_at': result.get('creation_date'),\n",
    "                    'user': result.get('user', {}).get('username'),\n",
    "                    'likes': result.get('favorite_count'),\n",
    "                    'retweets': result.get('retweet_count')\n",
    "                }\n",
    "                tweets.append(tweet)\n",
    "                \n",
    "            df = pd.DataFrame(tweets)\n",
    "            df['source'] = 'twitter'\n",
    "            df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "            \n",
    "            print(f\"Retrieved {len(df)} tweets for query: '{query}'\")\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"Error fetching Twitter data: {response.status_code}\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Twitter API call: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_reddit_data(subreddit, time_filter=\"week\", limit=100):\n",
    "    \"\"\"\n",
    "    Fetch posts from a specific subreddit\n",
    "    \n",
    "    Args:\n",
    "        subreddit (str): Subreddit name (e.g., \"investing\", \"stocks\")\n",
    "        time_filter (str): Time filter for posts (hour, day, week, month, year, all)\n",
    "        limit (int): Maximum number of posts to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing Reddit posts and metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = f\"https://reddit-api-five.vercel.app/api/search?q=&subreddit={subreddit}&sort=top&time={time_filter}&limit={limit}\"\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            posts = data.get('posts', [])\n",
    "            \n",
    "            # Extract relevant fields\n",
    "            reddit_posts = []\n",
    "            for post in posts:\n",
    "                post_data = {\n",
    "                    'id': post.get('id'),\n",
    "                    'title': post.get('title'),\n",
    "                    'text': post.get('selftext', ''),\n",
    "                    'created_at': datetime.fromtimestamp(post.get('created_utc', 0)),\n",
    "                    'score': post.get('score'),\n",
    "                    'url': post.get('url'),\n",
    "                    'num_comments': post.get('num_comments')\n",
    "                }\n",
    "                \n",
    "                # Combine title and text for sentiment analysis\n",
    "                post_data['full_text'] = post_data['title'] + ' ' + post_data['text']\n",
    "                reddit_posts.append(post_data)\n",
    "                \n",
    "            df = pd.DataFrame(reddit_posts)\n",
    "            df['source'] = 'reddit'\n",
    "            \n",
    "            print(f\"Retrieved {len(df)} posts from r/{subreddit}\")\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"Error fetching Reddit data: {response.status_code}\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Reddit API call: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_financial_news(query, limit=10):\n",
    "    \"\"\"\n",
    "    Fetch financial news articles about a specific topic\n",
    "    \n",
    "    Args:\n",
    "        query (str): Search query (e.g., \"Safaricom\", \"Kenya banking sector\")\n",
    "        limit (int): Maximum number of articles to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing news articles\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = \"https://news-api14.p.rapidapi.com/top-headlines\"\n",
    "        \n",
    "        querystring = {\n",
    "            \"country\": \"us,gb,ke\",\n",
    "            \"language\": \"en\",\n",
    "            \"pageSize\": str(limit),\n",
    "            \"category\": \"business\",\n",
    "            \"q\": query\n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            \"X-RapidAPI-Key\": API_CONFIG[\"news\"][\"rapidapi_key\"],\n",
    "            \"X-RapidAPI-Host\": \"news-api14.p.rapidapi.com\"\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, params=querystring)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            articles = data.get('articles', [])\n",
    "            \n",
    "            # Extract relevant fields\n",
    "            news_articles = []\n",
    "            for article in articles:\n",
    "                article_data = {\n",
    "                    'title': article.get('title'),\n",
    "                    'description': article.get('description', ''),\n",
    "                    'content': article.get('content', ''),\n",
    "                    'url': article.get('url'),\n",
    "                    'source': article.get('source', {}).get('name'),\n",
    "                    'published_at': article.get('publishedAt')\n",
    "                }\n",
    "                \n",
    "                # Combine title, description and content for sentiment analysis\n",
    "                article_data['text'] = ' '.join([\n",
    "                    article_data['title'], \n",
    "                    article_data['description'], \n",
    "                    article_data['content']\n",
    "                ])\n",
    "                \n",
    "                news_articles.append(article_data)\n",
    "                \n",
    "            df = pd.DataFrame(news_articles)\n",
    "            df['source'] = 'news'\n",
    "            df['created_at'] = pd.to_datetime(df['published_at'])\n",
    "            \n",
    "            print(f\"Retrieved {len(df)} news articles for query: '{query}'\")\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"Error fetching News data: {response.status_code}\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in News API call: {str(e)}\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f710583",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text for sentiment analysis\"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string if not already\n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Remove user mentions and hashtags (keep hashtag text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text.lower()\n",
    "\n",
    "def remove_stopwords(text, additional_stopwords=None):\n",
    "    \"\"\"Remove stopwords from text\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Add financial-specific stopwords\n",
    "    financial_stopwords = {\n",
    "        'stock', 'stocks', 'market', 'markets', 'price', 'prices',\n",
    "        'trade', 'trading', 'investor', 'investors', 'investment',\n",
    "        'investments', 'share', 'shares'\n",
    "    }\n",
    "    \n",
    "    stop_words.update(financial_stopwords)\n",
    "    \n",
    "    if additional_stopwords:\n",
    "        stop_words.update(set(additional_stopwords))\n",
    "    \n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Lemmatize text to get base words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "    \n",
    "    return ' '.join(lemmatized_text)\n",
    "\n",
    "def preprocess_text(text, remove_stop=True, lemmatize=True):\n",
    "    \"\"\"Full preprocessing pipeline for text data\"\"\"\n",
    "    if pd.isna(text) or not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Remove stopwords if requested\n",
    "    if remove_stop:\n",
    "        cleaned_text = remove_stopwords(cleaned_text)\n",
    "    \n",
    "    # Lemmatize if requested\n",
    "    if lemmatize:\n",
    "        cleaned_text = lemmatize_text(cleaned_text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def preprocess_dataframe(df, text_column):\n",
    "    \"\"\"Preprocess text data in a DataFrame column\"\"\"\n",
    "    df['clean_text'] = df[text_column].apply(preprocess_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2466814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vader_sentiment(text):\n",
    "    \"\"\"\n",
    "    Get sentiment scores using VADER (Valence Aware Dictionary and Sentiment Reasoner)\n",
    "    VADER is particularly useful for social media text\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not text:\n",
    "        return {'compound': 0, 'pos': 0, 'neu': 0, 'neg': 0}\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    \n",
    "    return sentiment_scores\n",
    "\n",
    "def get_textblob_sentiment(text):\n",
    "    \"\"\"Get sentiment using TextBlob\"\"\"\n",
    "    if pd.isna(text) or not text:\n",
    "        return {'polarity': 0, 'subjectivity': 0}\n",
    "    \n",
    "    analysis = TextBlob(text)\n",
    "    return {\n",
    "        'polarity': analysis.sentiment.polarity,\n",
    "        'subjectivity': analysis.sentiment.subjectivity\n",
    "    }\n",
    "\n",
    "def classify_sentiment(compound_score):\n",
    "    \"\"\"Classify sentiment based on compound score\"\"\"\n",
    "    if compound_score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "def analyze_sentiment(df, text_column='clean_text'):\n",
    "    \"\"\"\n",
    "    Analyze sentiment of text in a DataFrame column using multiple methods\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing text data\n",
    "        text_column (str): Column containing preprocessed text\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with added sentiment columns\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Initialize new columns\n",
    "    result_df['vader_compound'] = 0.0\n",
    "    result_df['vader_pos'] = 0.0\n",
    "    result_df['vader_neu'] = 0.0\n",
    "    result_df['vader_neg'] = 0.0\n",
    "    result_df['textblob_polarity'] = 0.0\n",
    "    result_df['textblob_subjectivity'] = 0.0\n",
    "    result_df['sentiment_label'] = 'neutral'\n",
    "    \n",
    "    # Apply sentiment analysis to each row\n",
    "    for idx, row in tqdm(result_df.iterrows(), total=len(result_df), desc=\"Analyzing sentiment\"):\n",
    "        text = row[text_column]\n",
    "        \n",
    "        # VADER sentiment\n",
    "        vader_scores = get_vader_sentiment(text)\n",
    "        result_df.at[idx, 'vader_compound'] = vader_scores['compound']\n",
    "        result_df.at[idx, 'vader_pos'] = vader_scores['pos']\n",
    "        result_df.at[idx, 'vader_neu'] = vader_scores['neu']\n",
    "        result_df.at[idx, 'vader_neg'] = vader_scores['neg']\n",
    "        \n",
    "        # TextBlob sentiment\n",
    "        textblob_scores = get_textblob_sentiment(text)\n",
    "        result_df.at[idx, 'textblob_polarity'] = textblob_scores['polarity']\n",
    "        result_df.at[idx, 'textblob_subjectivity'] = textblob_scores['subjectivity']\n",
    "        \n",
    "        # Classify sentiment based on VADER compound score\n",
    "        result_df.at[idx, 'sentiment_label'] = classify_sentiment(vader_scores['compound'])\n",
    "    \n",
    "    # Calculate the sentiment score (combined from both methods)\n",
    "    # Normalize both to range -1 to 1, and take average\n",
    "    result_df['sentiment_score'] = (result_df['vader_compound'] + result_df['textblob_polarity']) / 2\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def calculate_sentiment_stats(df):\n",
    "    \"\"\"Calculate summary statistics for sentiment analysis\"\"\"\n",
    "    stats = {\n",
    "        'total_count': len(df),\n",
    "        'positive_count': sum(df['sentiment_label'] == 'positive'),\n",
    "        'neutral_count': sum(df['sentiment_label'] == 'neutral'),\n",
    "        'negative_count': sum(df['sentiment_label'] == 'negative'),\n",
    "        'positive_percentage': (sum(df['sentiment_label'] == 'positive') / len(df)) * 100 if len(df) > 0 else 0,\n",
    "        'neutral_percentage': (sum(df['sentiment_label'] == 'neutral') / len(df)) * 100 if len(df) > 0 else 0,\n",
    "        'negative_percentage': (sum(df['sentiment_label'] == 'negative') / len(df)) * 100 if len(df) > 0 else 0,\n",
    "        'average_compound': df['vader_compound'].mean(),\n",
    "        'average_polarity': df['textblob_polarity'].mean(),\n",
    "        'average_sentiment_score': df['sentiment_score'].mean()\n",
    "    }\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49273505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentiment_distribution(df, title=\"Sentiment Distribution\", figsize=(12, 6)):\n",
    "    \"\"\"Plot the distribution of sentiment labels\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Count the sentiment labels\n",
    "    sentiment_counts = df['sentiment_label'].value_counts()\n",
    "    \n",
    "    # Create a bar chart\n",
    "    ax = sentiment_counts.plot(kind='bar', color=['green', 'gray', 'red'])\n",
    "    \n",
    "    # Add percentage labels on top of bars\n",
    "    for i, count in enumerate(sentiment_counts):\n",
    "        percentage = (count / len(df)) * 100\n",
    "        ax.text(i, count + (max(sentiment_counts) * 0.02), f\"{percentage:.1f}%\", \n",
    "                ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Sentiment', fontsize=14)\n",
    "    plt.ylabel('Count', fontsize=14)\n",
    "    plt.xticks(rotation=0, fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt\n",
    "\n",
    "def plot_sentiment_timeline(df, date_column='created_at', title=\"Sentiment Over Time\"):\n",
    "    \"\"\"Plot sentiment trends over time\"\"\"\n",
    "    # Ensure datetime format\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    \n",
    "    # Group by date and sentiment\n",
    "    df['date'] = df[date_column].dt.date\n",
    "    sentiment_by_date = df.groupby(['date', 'sentiment_label']).size().unstack().fillna(0)\n",
    "    \n",
    "    # Calculate daily totals and percentages\n",
    "    sentiment_by_date['total'] = sentiment_by_date.sum(axis=1)\n",
    "    \n",
    "    for sentiment in ['positive', 'neutral', 'negative']:\n",
    "        if sentiment in sentiment_by_date.columns:\n",
    "            sentiment_by_date[f'{sentiment}_pct'] = (sentiment_by_date[sentiment] / sentiment_by_date['total']) * 100\n",
    "    \n",
    "    # Plot the percentages over time\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    if 'positive_pct' in sentiment_by_date.columns:\n",
    "        plt.plot(sentiment_by_date.index, sentiment_by_date['positive_pct'], 'g-', label='Positive')\n",
    "    \n",
    "    if 'neutral_pct' in sentiment_by_date.columns:\n",
    "        plt.plot(sentiment_by_date.index, sentiment_by_date['neutral_pct'], 'b-', label='Neutral')\n",
    "    \n",
    "    if 'negative_pct' in sentiment_by_date.columns:\n",
    "        plt.plot(sentiment_by_date.index, sentiment_by_date['negative_pct'], 'r-', label='Negative')\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=14)\n",
    "    plt.ylabel('Percentage (%)', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt\n",
    "\n",
    "def generate_wordcloud(df, column='clean_text', title=\"Word Cloud\", figsize=(12, 8)):\n",
    "    \"\"\"Generate a word cloud from text data\"\"\"\n",
    "    # Combine all text into a single string\n",
    "    text = ' '.join(df[column].dropna())\n",
    "    \n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, \n",
    "        height=500, \n",
    "        background_color='white',\n",
    "        max_words=150,\n",
    "        colormap='viridis',\n",
    "        collocations=False\n",
    "    ).generate(text)\n",
    "    \n",
    "    # Plot the word cloud\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt\n",
    "\n",
    "def generate_sentiment_wordcloud(df, sentiment_type='positive', figsize=(12, 8)):\n",
    "    \"\"\"Generate a word cloud for a specific sentiment type\"\"\"\n",
    "    # Filter data by sentiment\n",
    "    filtered_df = df[df['sentiment_label'] == sentiment_type]\n",
    "    \n",
    "    # Generate title based on sentiment\n",
    "    title = f\"Most Common Words in {sentiment_type.capitalize()} Posts\"\n",
    "    \n",
    "    # Generate and return the word cloud\n",
    "    return generate_wordcloud(filtered_df, column='clean_text', title=title, figsize=figsize)\n",
    "\n",
    "def plot_source_sentiment(df, title=\"Sentiment Distribution by Source\", figsize=(14, 8)):\n",
    "    \"\"\"Plot sentiment distribution across different data sources\"\"\"\n",
    "    # Check if there's more than one source\n",
    "    if len(df['source'].unique()) <= 1:\n",
    "        print(\"Only one source present in data, skipping source comparison plot.\")\n",
    "        return None\n",
    "    \n",
    "    # Group by source and sentiment\n",
    "    source_sentiment = df.groupby(['source', 'sentiment_label']).size().unstack().fillna(0)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    source_sentiment_pct = source_sentiment.div(source_sentiment.sum(axis=1), axis=0) * 100\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = source_sentiment_pct.plot(kind='bar', stacked=True, \n",
    "                                   color=['red', 'gray', 'green'])\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, source in enumerate(source_sentiment_pct.index):\n",
    "        previous_height = 0\n",
    "        for j, sentiment in enumerate(source_sentiment_pct.columns):\n",
    "            height = source_sentiment_pct.loc[source, sentiment]\n",
    "            if height > 5:  # Only show percentage if it's large enough\n",
    "                ax.text(i, previous_height + (height/2), f\"{height:.1f}%\", \n",
    "                        ha='center', va='center', fontsize=10, color='black')\n",
    "            previous_height += height\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Source', fontsize=14)\n",
    "    plt.ylabel('Percentage (%)', fontsize=14)\n",
    "    plt.xticks(rotation=0, fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(title='Sentiment')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt\n",
    "\n",
    "def plot_top_keywords(df, column='clean_text', n=20, figsize=(12, 8)):\n",
    "    \"\"\"Plot the most frequent keywords in the dataset\"\"\"\n",
    "    # Initialize the TF-IDF vectorizer\n",
    "    tfidf = TfidfVectorizer(max_features=n, stop_words='english')\n",
    "    \n",
    "    # Fit and transform the text data\n",
    "    tfidf_matrix = tfidf.fit_transform(df[column].dropna())\n",
    "    \n",
    "    # Get feature names and TF-IDF scores\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "    \n",
    "    # Create a DataFrame with feature names and scores\n",
    "    keywords_df = pd.DataFrame({\n",
    "        'keyword': feature_names,\n",
    "        'tfidf_score': tfidf_scores\n",
    "    })\n",
    "    \n",
    "    # Sort by TF-IDF score in descending order\n",
    "    keywords_df = keywords_df.sort_values('tfidf_score', ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.barplot(x='tfidf_score', y='keyword', data=keywords_df, palette='viridis')\n",
    "    plt.title(f\"Top {n} Keywords by TF-IDF Score\", fontsize=16)\n",
    "    plt.xlabel('TF-IDF Score', fontsize=14)\n",
    "    plt.ylabel('Keyword', fontsize=14)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt, keywords_df\n",
    "\n",
    "def plot_sentiment_comparison(df1, df2, label1, label2, title=\"Sentiment Comparison\", figsize=(12, 6)):\n",
    "    \"\"\"Compare sentiment distribution between two datasets\"\"\"\n",
    "    # Calculate sentiment percentages for each dataset\n",
    "    def get_sentiment_percentages(df):\n",
    "        total = len(df)\n",
    "        return {\n",
    "            'positive': (sum(df['sentiment_label'] == 'positive') / total) * 100 if total > 0 else 0,\n",
    "            'neutral': (sum(df['sentiment_label'] == 'neutral') / total) * 100 if total > 0 else 0,\n",
    "            'negative': (sum(df['sentiment_label'] == 'negative') / total) * 100 if total > 0 else 0\n",
    "        }\n",
    "    \n",
    "    pct1 = get_sentiment_percentages(df1)\n",
    "    pct2 = get_sentiment_percentages(df2)\n",
    "    \n",
    "    # Create a DataFrame for plotting\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Sentiment': ['Positive', 'Neutral', 'Negative'],\n",
    "        label1: [pct1['positive'], pct1['neutral'], pct1['negative']],\n",
    "        label2: [pct2['positive'], pct2['neutral'], pct2['negative']]\n",
    "    })\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    x = np.arange(len(comparison_df['Sentiment']))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, comparison_df[label1], width, label=label1, color='cornflowerblue')\n",
    "    plt.bar(x + width/2, comparison_df[label2], width, label=label2, color='lightcoral')\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, v in enumerate(comparison_df[label1]):\n",
    "        plt.text(i - width/2, v + 1, f\"{v:.1f}%\", ha='center', va='bottom')\n",
    "    \n",
    "    for i, v in enumerate(comparison_df[label2]):\n",
    "        plt.text(i + width/2, v + 1, f\"{v:.1f}%\", ha='center', va='bottom')\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Sentiment', fontsize=14)\n",
    "    plt.ylabel('Percentage (%)', fontsize=14)\n",
    "    plt.xticks(x, comparison_df['Sentiment'])\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb598320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sentiment_shift(current_df, historical_df, threshold=15):\n",
    "    \"\"\"\n",
    "    Detect significant shifts in sentiment compared to historical data\n",
    "    \n",
    "    Args:\n",
    "        current_df (pandas.DataFrame): Current sentiment data\n",
    "        historical_df (pandas.DataFrame): Historical sentiment data\n",
    "        threshold (float): Percentage change threshold to trigger an alert\n",
    "        \n",
    "    Returns:\n",
    "        dict: Alert information if threshold exceeded, or None\n",
    "    \"\"\"\n",
    "    # Calculate current sentiment percentages\n",
    "    current_stats = calculate_sentiment_stats(current_df)\n",
    "    \n",
    "    # Calculate historical sentiment percentages\n",
    "    historical_stats = calculate_sentiment_stats(historical_df)\n",
    "    \n",
    "    # Calculate changes\n",
    "    positive_change = current_stats['positive_percentage'] - historical_stats['positive_percentage']\n",
    "    negative_change = current_stats['negative_percentage'] - historical_stats['negative_percentage']\n",
    "    \n",
    "    # Determine if alert should be triggered\n",
    "    alert = None\n",
    "    \n",
    "    if abs(positive_change) >= threshold:\n",
    "        direction = \"increase\" if positive_change > 0 else \"decrease\"\n",
    "        alert = {\n",
    "            \"type\": f\"positive_sentiment_{direction}\",\n",
    "            \"change\": abs(positive_change),\n",
    "            \"message\": f\"Positive sentiment has {direction}d by {abs(positive_change):.2f}% \"\n",
    "                       f\"(Current: {current_stats['positive_percentage']:.2f}%, \"\n",
    "                       f\"Historical: {historical_stats['positive_percentage']:.2f}%)\"\n",
    "        }\n",
    "    \n",
    "    elif abs(negative_change) >= threshold:\n",
    "        direction = \"increase\" if negative_change > 0 else \"decrease\"\n",
    "        alert = {\n",
    "            \"type\": f\"negative_sentiment_{direction}\",\n",
    "            \"change\": abs(negative_change),\n",
    "            \"message\": f\"Negative sentiment has {direction}d by {abs(negative_change):.2f}% \"\n",
    "                       f\"(Current: {current_stats['negative_percentage']:.2f}%, \"\n",
    "                       f\"Historical: {historical_stats['negative_percentage']:.2f}%)\"\n",
    "        }\n",
    "    \n",
    "    return alert\n",
    "\n",
    "def generate_sentiment_report(df, topic, time_period=\"last 7 days\"):\n",
    "    \"\"\"\n",
    "    Generate a summary report of sentiment analysis\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame with sentiment analysis results\n",
    "        topic (str): Topic or stock being analyzed\n",
    "        time_period (str): Time period of the data\n",
    "        \n",
    "    Returns:\n",
    "        str: Markdown-formatted report\n",
    "    \"\"\"\n",
    "    # Calculate stats\n",
    "    stats = calculate_sentiment_stats(df)\n",
    "    \n",
    "    # Calculate most common positive and negative keywords\n",
    "    positive_df = df[df['sentiment_label'] == 'positive']\n",
    "    negative_df = df[df['sentiment_label'] == 'negative']\n",
    "    \n",
    "    # Extract keywords using TF-IDF\n",
    "    tfidf = TfidfVectorizer(max_features=10, stop_words='english')\n",
    "    \n",
    "    # Get top positive keywords if enough data\n",
    "    top_positive_keywords = []\n",
    "    if len(positive_df) >= 5:\n",
    "        tfidf_matrix = tfidf.fit_transform(positive_df['clean_text'].dropna())\n",
    "        feature_names = tfidf.get_feature_names_out()\n",
    "        tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "        top_positive_keywords = [feature_names[i] for i in tfidf_scores.argsort()[::-1][:5]]\n",
    "    \n",
    "    # Get top negative keywords if enough data\n",
    "    top_negative_keywords = []\n",
    "    if len(negative_df) >= 5:\n",
    "        tfidf_matrix = tfidf.fit_transform(negative_df['clean_text'].dropna())\n",
    "        feature_names = tfidf.get_feature_names_out()\n",
    "        tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "        top_negative_keywords = [feature_names[i] for i in tfidf_scores.argsort()[::-1][:5]]\n",
    "    \n",
    "    # Generate report\n",
    "    report = f\"\"\"# Sentiment Analysis Report: {topic}\n",
    "## Overview ({time_period})\n",
    "\n",
    "- **Total Items Analyzed**: {stats['total_count']}\n",
    "- **Data Sources**: {', '.join(df['source'].unique())}\n",
    "- **Time Range**: {df['created_at'].min().date()} to {df['created_at'].max().date()}\n",
    "\n",
    "## Sentiment Distribution\n",
    "\n",
    "- **Positive**: {stats['positive_count']} ({stats['positive_percentage']:.2f}%)\n",
    "- **Neutral**: {stats['neutral_count']} ({stats['neutral_percentage']:.2f}%)\n",
    "- **Negative**: {stats['negative_count']} ({stats['negative_percentage']:.2f}%)\n",
    "\n",
    "## Average Sentiment Scores\n",
    "\n",
    "- **VADER Compound Score**: {stats['average_compound']:.4f} (Range: -1 to +1)\n",
    "- **TextBlob Polarity**: {stats['average_polarity']:.4f} (Range: -1 to +1)\n",
    "- **Combined Sentiment Score**: {stats['average_sentiment_score']:.4f}\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add overall sentiment assessment\n",
    "    if stats['average_sentiment_score'] >= 0.25:\n",
    "        report += \"- **Overall Sentiment**: Strongly Positive ✅✅\\n\"\n",
    "    elif stats['average_sentiment_score'] >= 0.05:\n",
    "        report += \"- **Overall Sentiment**: Positive ✅\\n\"\n",
    "    elif stats['average_sentiment_score'] <= -0.25:\n",
    "        report += \"- **Overall Sentiment**: Strongly Negative ❌❌\\n\"\n",
    "    elif stats['average_sentiment_score'] <= -0.05:\n",
    "        report += \"- **Overall Sentiment**: Negative ❌\\n\"\n",
    "    else:\n",
    "        report += \"- **Overall Sentiment**: Neutral ⚖️\\n\"\n",
    "    \n",
    "    # Add top keywords\n",
    "    if top_positive_keywords:\n",
    "        report += f\"- **Top Positive Keywords**: {', '.join(top_positive_keywords)}\\n\"\n",
    "    \n",
    "    if top_negative_keywords:\n",
    "        report += f\"- **Top Negative Keywords**: {', '.join(top_negative_keywords)}\\n\"\n",
    "    \n",
    "    # Add source breakdown if multiple sources\n",
    "    if len(df['source'].unique()) > 1:\n",
    "        report += \"\\n## Sentiment by Source\\n\\n\"\n",
    "        for source in df['source'].unique():\n",
    "            source_df = df[df['source'] == source]\n",
    "            source_stats = calculate_sentiment_stats(source_df)\n",
    "            report += f\"- **{source.capitalize()}**: \"\n",
    "            report += f\"Positive: {source_stats['positive_percentage']:.2f}%, \"\n",
    "            report += f\"Neutral: {source_stats['neutral_percentage']:.2f}%, \"\n",
    "            report += f\"Negative: {source_stats['negative_percentage']:.2f}%\\n\"\n",
    "    \n",
    "    # Add sentiment over time insight if enough data points\n",
    "    if len(df['created_at'].dt.date.unique()) >= 3:\n",
    "        report += \"\\n## Sentiment Trends\\n\\n\"\n",
    "        \n",
    "        latest_date = df['created_at'].max().date()\n",
    "        earliest_date = df['created_at'].min().date()\n",
    "        \n",
    "        # Compare earliest vs latest day\n",
    "        early_df = df[df['created_at'].dt.date == earliest_date]\n",
    "        late_df = df[df['created_at'].dt.date == latest_date]\n",
    "        \n",
    "        if len(early_df) > 0 and len(late_df) > 0:\n",
    "            early_stats = calculate_sentiment_stats(early_df)\n",
    "            late_stats = calculate_sentiment_stats(late_df)\n",
    "            \n",
    "            positive_change = late_stats['positive_percentage'] - early_stats['positive_percentage']\n",
    "            \n",
    "            if abs(positive_change) >= 10:\n",
    "                direction = \"increased\" if positive_change > 0 else \"decreased\"\n",
    "                report += f\"- Positive sentiment has **{direction}** by {abs(positive_change):.2f}% \"\n",
    "                report += f\"from {earliest_date} to {latest_date}.\\n\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "def save_sentiment_data(df, filename=\"sentiment_data\", format=\"csv\"):\n",
    "    \"\"\"Save sentiment analysis results to a file\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_path = os.path.join(OUTPUT_DIR, f\"{filename}_{timestamp}.{format}\")\n",
    "    \n",
    "    if format.lower() == \"csv\":\n",
    "        df.to_csv(file_path, index=False)\n",
    "    elif format.lower() == \"json\":\n",
    "        df.to_json(file_path, orient=\"records\", date_format=\"iso\")\n",
    "    elif format.lower() == \"excel\":\n",
    "        df.to_excel(file_path, index=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported format: {format}\")\n",
    "    \n",
    "    print(f\"Saved sentiment data to {file_path}\")\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6263a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_stock_sentiment(stock_code, stock_name=None, days=7):\n",
    "    \"\"\"\n",
    "    Analyze sentiment for a specific stock across multiple data sources\n",
    "    \n",
    "    Args:\n",
    "        stock_code (str): Stock code (e.g., \"SCOM\" for Safaricom)\n",
    "        stock_name (str, optional): Stock name for better search results\n",
    "        days (int): Number of days to look back\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Combined sentiment analysis results\n",
    "    \"\"\"\n",
    "    if stock_name is None:\n",
    "        stock_name = stock_code\n",
    "    \n",
    "    print(f\"Analyzing sentiment for {stock_name} ({stock_code}) over the past {days} days...\")\n",
    "    \n",
    "    # Build search queries\n",
    "    twitter_query = f\"${stock_code} OR #{stock_code} OR {stock_name} stock\"\n",
    "    reddit_subreddits = [\"investing\", \"stocks\", \"StockMarket\"]\n",
    "    news_query = f\"{stock_name} stock OR {stock_code}\"\n",
    "    \n",
    "    # Collect data from different sources\n",
    "    twitter_data = get_twitter_data(twitter_query, count=100)\n",
    "    \n",
    "    # Collect Reddit data from multiple subreddits\n",
    "    reddit_dfs = []\n",
    "    for subreddit in reddit_subreddits:\n",
    "        df = get_reddit_data(subreddit, time_filter=\"week\" if days <= 7 else \"month\")\n",
    "        # Filter for posts containing the stock code or name\n",
    "        if not df.empty:\n",
    "            df = df[df['full_text'].str.contains(stock_code, case=False) | \n",
    "                    df['full_text'].str.contains(stock_name, case=False)]\n",
    "            reddit_dfs.append(df)\n",
    "    \n",
    "    reddit_data = pd.concat(reddit_dfs, ignore_index=True) if reddit_dfs else pd.DataFrame()\n",
    "    \n",
    "    news_data = get_financial_news(news_query, limit=20)\n",
    "    \n",
    "    # Combine data from all sources\n",
    "    dfs = []\n",
    "    if not twitter_data.empty:\n",
    "        twitter_data['text_to_analyze'] = twitter_data['text']\n",
    "        dfs.append(twitter_data)\n",
    "    \n",
    "    if not reddit_data.empty:\n",
    "        reddit_data['text_to_analyze'] = reddit_data['full_text']\n",
    "        dfs.append(reddit_data)\n",
    "    \n",
    "    if not news_data.empty:\n",
    "        news_data['text_to_analyze'] = news_data['text']\n",
    "        dfs.append(news_data)\n",
    "    \n",
    "    # If no data was collected, return empty DataFrame\n",
    "    if not dfs:\n",
    "        print(f\"No data found for {stock_name} ({stock_code})\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Combine all data\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Preprocess text\n",
    "    combined_df = preprocess_dataframe(combined_df, 'text_to_analyze')\n",
    "    \n",
    "    # Analyze sentiment\n",
    "    sentiment_df = analyze_sentiment(combined_df)\n",
    "    \n",
    "    # Filter by date if requested\n",
    "    if days > 0:\n",
    "        cutoff_date = datetime.now() - timedelta(days=days)\n",
    "        sentiment_df = sentiment_df[sentiment_df['created_at'] >= cutoff_date]\n",
    "    \n",
    "    # Add metadata\n",
    "    sentiment_df['stock_code'] = stock_code\n",
    "    sentiment_df['stock_name'] = stock_name\n",
    "    sentiment_df['analysis_date'] = datetime.now()\n",
    "    \n",
    "    print(f\"Sentiment analysis complete for {stock_name} ({stock_code})\")\n",
    "    print(f\"Total items analyzed: {len(sentiment_df)}\")\n",
    "    \n",
    "    return sentiment_df\n",
    "\n",
    "def analyze_multiple_stocks(stock_list):\n",
    "    \"\"\"\n",
    "    Analyze sentiment for multiple stocks\n",
    "    \n",
    "    Args:\n",
    "        stock_list (list): List of dictionaries with 'code' and 'name' keys\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of DataFrames with sentiment analysis results for each stock\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for stock in stock_list:\n",
    "        stock_code = stock['code']\n",
    "        stock_name = stock['name']\n",
    "        \n",
    "        print(f\"\\nAnalyzing {stock_name} ({stock_code})...\")\n",
    "        \n",
    "        # Analyze sentiment for this stock\n",
    "        sentiment_df = analyze_stock_sentiment(stock_code, stock_name)\n",
    "        \n",
    "        # Store results if we got data\n",
    "        if not sentiment_df.empty:\n",
    "            results[stock_code] = sentiment_df\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_stock_sentiment(stock1, stock2):\n",
    "    \"\"\"\n",
    "    Compare sentiment between two stocks\n",
    "    \n",
    "    Args:\n",
    "        stock1 (dict): Dictionary with 'code' and 'name' for first stock\n",
    "        stock2 (dict): Dictionary with 'code' and 'name' for second stock\n",
    "        \n",
    "    Returns:\n",
    "        tuple: DataFrames with sentiment analysis results for both stocks\n",
    "    \"\"\"\n",
    "    # Analyze sentiment for both stocks\n",
    "    stock1_df = analyze_stock_sentiment(stock1['code'], stock1['name'])\n",
    "    stock2_df = analyze_stock_sentiment(stock2['code'], stock2['name'])\n",
    "    \n",
    "    # Compare results\n",
    "    if not stock1_df.empty and not stock2_df.empty:\n",
    "        # Generate comparison visualization\n",
    "        plt = plot_sentiment_comparison(\n",
    "            stock1_df, stock2_df, \n",
    "            stock1['name'], stock2['name'],\n",
    "            title=f\"Sentiment Comparison: {stock1['name']} vs {stock2['name']}\"\n",
    "        )\n",
    "        \n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f\"comparison_{stock1['code']}_{stock2['code']}.png\"))\n",
    "        plt.close()\n",
    "        \n",
    "        # Generate comparison report\n",
    "        stock1_stats = calculate_sentiment_stats(stock1_df)\n",
    "        stock2_stats = calculate_sentiment_stats(stock2_df)\n",
    "        \n",
    "        print(\"\\nComparison Results:\")\n",
    "        print(f\"{stock1['name']} Sentiment Score: {stock1_stats['average_sentiment_score']:.4f}\")\n",
    "        print(f\"{stock2['name']} Sentiment Score: {stock2_stats['average_sentiment_score']:.4f}\")\n",
    "        \n",
    "        if stock1_stats['average_sentiment_score'] > stock2_stats['average_sentiment_score']:\n",
    "            print(f\"{stock1['name']} has more positive sentiment\")\n",
    "        else:\n",
    "            print(f\"{stock2['name']} has more positive sentiment\")\n",
    "    \n",
    "    return stock1_df, stock2_df\n",
    "\n",
    "def analyze_market_sentiment(market_name=\"NSE\", days=7):\n",
    "    \"\"\"\n",
    "    Analyze overall market sentiment\n",
    "    \n",
    "    Args:\n",
    "        market_name (str): Market name (e.g., \"NSE\" for Nairobi Stock Exchange)\n",
    "        days (int): Number of days to look back\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Market sentiment analysis results\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing overall {market_name} market sentiment...\")\n",
    "    \n",
    "    # Build search queries\n",
    "    twitter_query = f\"{market_name} stock market OR {market_name} index\"\n",
    "    news_query = f\"{market_name} stock market\"\n",
    "    \n",
    "    # Collect data\n",
    "    twitter_data = get_twitter_data(twitter_query, count=100)\n",
    "    \n",
    "    reddit_dfs = []\n",
    "    for subreddit in [\"investing\", \"stocks\", \"finance\"]:\n",
    "        df = get_reddit_data(subreddit, time_filter=\"week\" if days <= 7 else \"month\")\n",
    "        if not df.empty:\n",
    "            df = df[df['full_text'].str.contains(market_name, case=False)]\n",
    "            reddit_dfs.append(df)\n",
    "    \n",
    "    reddit_data = pd.concat(reddit_dfs, ignore_index=True) if reddit_dfs else pd.DataFrame()\n",
    "    \n",
    "    news_data = get_financial_news(news_query, limit=30)\n",
    "    \n",
    "    # Combine data\n",
    "    dfs = []\n",
    "    if not twitter_data.empty:\n",
    "        twitter_data['text_to_analyze'] = twitter_data['text']\n",
    "        dfs.append(twitter_data)\n",
    "    \n",
    "    if not reddit_data.empty:\n",
    "        reddit_data['text_to_analyze'] = reddit_data['full_text']\n",
    "        dfs.append(reddit_data)\n",
    "    \n",
    "    if not news_data.empty:\n",
    "        news_data['text_to_analyze'] = news_data['text']\n",
    "        dfs.append(news_data)\n",
    "    \n",
    "    if not dfs:\n",
    "        print(f\"No data found for {market_name} market\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Combine all data\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Preprocess text\n",
    "    combined_df = preprocess_dataframe(combined_df, 'text_to_analyze')\n",
    "    \n",
    "    # Analyze sentiment\n",
    "    sentiment_df = analyze_sentiment(combined_df)\n",
    "    \n",
    "    # Filter by date if requested\n",
    "    if days > 0:\n",
    "        cutoff_date = datetime.now() - timedelta(days=days)\n",
    "        sentiment_df = sentiment_df[sentiment_df['created_at'] >= cutoff_date]\n",
    "    \n",
    "    # Add metadata\n",
    "    sentiment_df['market'] = market_name\n",
    "    sentiment_df['analysis_date'] = datetime.now()\n",
    "    \n",
    "    print(f\"Market sentiment analysis complete for {market_name}\")\n",
    "    print(f\"Total items analyzed: {len(sentiment_df)}\")\n",
    "    \n",
    "    return sentiment_df\n",
    "\n",
    "def process_nse_data(stock_codes=None):\n",
    "    \"\"\"\n",
    "    Process Nairobi Stock Exchange data and analyze sentiment for key stocks\n",
    "    \n",
    "    Args:\n",
    "        stock_codes (list, optional): List of stock codes to analyze\n",
    "            If None, analyzes a default set of top NSE stocks\n",
    "            \n",
    "    Returns:\n",
    "        dict: Dictionary of sentiment analysis results\n",
    "    \"\"\"\n",
    "    # Default list of key NSE stocks if none provided\n",
    "    if stock_codes is None:\n",
    "        stock_codes = [\n",
    "            {\"code\": \"SCOM\", \"name\": \"Safaricom\"},\n",
    "            {\"code\": \"EQTY\", \"name\": \"Equity Group\"},\n",
    "            {\"code\": \"KCB\", \"name\": \"KCB Group\"},\n",
    "            {\"code\": \"COOP\", \"name\": \"Co-operative Bank\"},\n",
    "            {\"code\": \"EABL\", \"name\": \"East African Breweries\"}\n",
    "        ]\n",
    "    \n",
    "    # Analyze market sentiment first\n",
    "    market_df = analyze_market_sentiment(\"NSE\")\n",
    "    \n",
    "    # Save market sentiment results\n",
    "    if not market_df.empty:\n",
    "        save_sentiment_data(market_df, filename=\"nse_market_sentiment\")\n",
    "        \n",
    "        # Generate market report\n",
    "        market_report = generate_sentiment_report(market_df, \"NSE Market\")\n",
    "        \n",
    "        # Save market report\n",
    "        report_path = os.path.join(OUTPUT_DIR, f\"nse_market_report_{datetime.now().strftime('%Y%m%d')}.md\")\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(market_report)\n",
    "        \n",
    "        print(f\"Saved market report to {report_path}\")\n",
    "    \n",
    "    # Analyze individual stocks\n",
    "    stock_results = analyze_multiple_stocks(stock_codes)\n",
    "    \n",
    "    # Save results and generate visualizations for each stock\n",
    "    for stock_code, sentiment_df in stock_results.items():\n",
    "        if not sentiment_df.empty:\n",
    "            # Save sentiment data\n",
    "            save_sentiment_data(sentiment_df, filename=f\"{stock_code}_sentiment\")\n",
    "            \n",
    "            # Generate and save visualizations\n",
    "            stock_name = sentiment_df['stock_name'].iloc[0]\n",
    "            \n",
    "            # Sentiment distribution\n",
    "            plt = plot_sentiment_distribution(sentiment_df, title=f\"Sentiment Distribution for {stock_name}\")\n",
    "            plt.savefig(os.path.join(OUTPUT_DIR, f\"{stock_code}_sentiment_distribution.png\"))\n",
    "            plt.close()\n",
    "            \n",
    "            # Sentiment timeline if enough data\n",
    "            if len(sentiment_df['created_at'].dt.date.unique()) > 1:\n",
    "                plt = plot_sentiment_timeline(sentiment_df, title=f\"Sentiment Trend for {stock_name}\")\n",
    "                plt.savefig(os.path.join(OUTPUT_DIR, f\"{stock_code}_sentiment_timeline.png\"))\n",
    "                plt.close()\n",
    "            \n",
    "            # Word cloud\n",
    "            plt = generate_wordcloud(sentiment_df, title=f\"Word Cloud for {stock_name}\")\n",
    "            plt.savefig(os.path.join(OUTPUT_DIR, f\"{stock_code}_wordcloud.png\"))\n",
    "            plt.close()\n",
    "            \n",
    "            # Generate report\n",
    "            stock_report = generate_sentiment_report(sentiment_df, f\"{stock_name} ({stock_code})\")\n",
    "            \n",
    "            # Save stock report\n",
    "            report_path = os.path.join(OUTPUT_DIR, f\"{stock_code}_report_{datetime.now().strftime('%Y%m%d')}.md\")\n",
    "            with open(report_path, 'w') as f:\n",
    "                f.write(stock_report)\n",
    "            \n",
    "            print(f\"Saved stock report for {stock_name} to {report_path}\")\n",
    "    \n",
    "    return {\"market\": market_df, \"stocks\": stock_results}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pesaguru",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
