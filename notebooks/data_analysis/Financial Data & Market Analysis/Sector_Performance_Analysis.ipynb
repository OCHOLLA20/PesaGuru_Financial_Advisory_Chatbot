{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f52042c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\xampp\\htdocs\\PesaGuru\\notebooks\\data_analysis\\Financial Data & Market Analysis\n",
      "# NSE Sector Performance Analysis Notebook\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display current working directory for debugging\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Set styling for visualizations\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_palette('Set2')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"# NSE Sector Performance Analysis Notebook\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f6e14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data directory: C:\\xampp\\htdocs\\PesaGuru\\notebooks\\data\\external\\nse_historical_data\n",
      "Successfully loaded: NSE_data_all_stocks_2024_jan_to_oct.csv\n",
      "Successfully loaded: NSE_data_all_stocks_2023.csv\n",
      "Successfully loaded: NSE_data_all_stocks_2021_upto_31dec2021.csv\n",
      "Successfully loaded: NSE_data_all_stocks_2020.csv\n",
      "Successfully loaded: NSE_data_all_stocks_2019.csv\n",
      "Successfully loaded: NSE_data_all_stocks_2018.csv\n",
      "Successfully loaded: NSE_data_all_stocks_2017.csv\n",
      "Successfully loaded: NSE_data_all_stocks_2016.csv\n",
      "Successfully loaded: NSE_data_all_stocks_2015.csv\n",
      "Successfully loaded: NSE_data_all_stocks_2014.csv\n",
      "Successfully loaded: NSE_data_all_stocks_2013.csv\n",
      "Using sector data from: NSE_data_stock_market_sectors_2023_2024.csv\n",
      "\n",
      "Stock data loaded successfully: 184,720 rows and 13 columns\n",
      "Date range: 2013-01-02 to 2024-10-31\n",
      "\n",
      "Number of unique stocks: 77\n",
      "Number of sectors: 11\n",
      "Sectors: Agricultural, Automobiles and Accessories, Banking, Commercial and Services, Construction and Allied, Insurance, Investment, Investment Services, Manufacturing and Allied, Telecommunication, Indices\n"
     ]
    }
   ],
   "source": [
    "# Function to find the correct data directory\n",
    "def find_data_directory():\n",
    "    \"\"\"Find the directory containing the NSE data files\"\"\"\n",
    "    # Define possible data directory paths in order of preference\n",
    "    possible_paths = [\n",
    "        # Path provided in the error message\n",
    "        r\"C:\\xampp\\htdocs\\PesaGuru\\notebooks\\data\\external\\nse_historical_data\",\n",
    "        # Common relative paths from the notebook location\n",
    "        os.path.join(\"..\", \"data\", \"external\", \"nse_historical_data\"),\n",
    "        os.path.join(\"data\", \"external\", \"nse_historical_data\"),\n",
    "        os.path.join(\"notebooks\", \"data\", \"external\", \"nse_historical_data\"),\n",
    "        # Current directory as fallback\n",
    "        \".\"\n",
    "    ]\n",
    "    \n",
    "    # Check each potential path\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"Found data directory: {path}\")\n",
    "            return path\n",
    "    \n",
    "    # If no valid path found, return current directory with a warning\n",
    "    print(\"WARNING: Could not find the NSE data directory. Using current directory.\")\n",
    "    return \".\"\n",
    "\n",
    "# Function to load and process stock data files\n",
    "def load_stock_data(file_name, data_dir):\n",
    "    \"\"\"Load and process NSE stock data from CSV files\"\"\"\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Standardize column names (adjust based on file examination)\n",
    "        # Different files may have slightly different column names\n",
    "        if 'DATE' in df.columns:\n",
    "            df.rename(columns={'DATE': 'Date'}, inplace=True)\n",
    "        if 'CODE' in df.columns:\n",
    "            df.rename(columns={'CODE': 'Code'}, inplace=True)\n",
    "        if 'NAME' in df.columns:\n",
    "            df.rename(columns={'NAME': 'Name'}, inplace=True)\n",
    "        if 'Adjust' in df.columns:\n",
    "            df.rename(columns={'Adjust': 'Adjusted Price'}, inplace=True)\n",
    "            \n",
    "        # Convert date to datetime format\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        \n",
    "        # Convert price columns to numeric, replacing any non-numeric values with NaN\n",
    "        price_columns = ['Day Price', '12m Low', '12m High', 'Day Low', 'Day High', 'Previous', 'Adjusted Price']\n",
    "        for col in price_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')\n",
    "        \n",
    "        # Convert volume to numeric\n",
    "        if 'Volume' in df.columns:\n",
    "            df['Volume'] = pd.to_numeric(df['Volume'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "            \n",
    "        # Clean percentage change columns if present\n",
    "        if 'Change%' in df.columns:\n",
    "            df['Change%'] = pd.to_numeric(df['Change%'].astype(str).str.replace('%', ''), errors='coerce')\n",
    "            \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Function to load and process sector data\n",
    "def load_sector_data(file_name, data_dir):\n",
    "    \"\"\"Load and process NSE sector classification data\"\"\"\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Sector file not found: {file_path}\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Standardize column names\n",
    "        if 'SECTOR' in df.columns:\n",
    "            df.rename(columns={'SECTOR': 'Sector'}, inplace=True)\n",
    "        if 'CODE' in df.columns:\n",
    "            df.rename(columns={'CODE': 'Code', 'Stock_code': 'Code'}, inplace=True)\n",
    "        if 'NAME' in df.columns:\n",
    "            df.rename(columns={'NAME': 'Name', 'Stock_name': 'Name'}, inplace=True)\n",
    "            \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sector file {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Find the data directory\n",
    "data_dir = find_data_directory()\n",
    "\n",
    "# Load stock data for multiple years\n",
    "stock_files = [\n",
    "    \"NSE_data_all_stocks_2024_jan_to_oct.csv\",\n",
    "    \"NSE_data_all_stocks_2023.csv\",\n",
    "    \"NSE_data_all_stocks_2021_upto_31dec2021.csv\",\n",
    "    \"NSE_data_all_stocks_2020.csv\",\n",
    "    \"NSE_data_all_stocks_2019.csv\",\n",
    "    \"NSE_data_all_stocks_2018.csv\",\n",
    "    \"NSE_data_all_stocks_2017.csv\",\n",
    "    \"NSE_data_all_stocks_2016.csv\",\n",
    "    \"NSE_data_all_stocks_2015.csv\",\n",
    "    \"NSE_data_all_stocks_2014.csv\",\n",
    "    \"NSE_data_all_stocks_2013.csv\"\n",
    "]\n",
    "\n",
    "# Load sector classification data\n",
    "sector_files = [\n",
    "    \"NSE_data_stock_market_sectors_2023_2024.csv\",  # Latest sector data\n",
    "    \"NSE_data_stock_market_sectors_as_at_31dec2021.csv\",\n",
    "    \"NSE_data_stock_market_sectors_2020.csv\"\n",
    "]\n",
    "\n",
    "# Read all stock data files and concatenate them\n",
    "all_stock_data = pd.DataFrame()\n",
    "for file in stock_files:\n",
    "    try:\n",
    "        df = load_stock_data(file, data_dir)\n",
    "        if not df.empty:\n",
    "            all_stock_data = pd.concat([all_stock_data, df], ignore_index=True)\n",
    "            print(f\"Successfully loaded: {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process {file}: {e}\")\n",
    "\n",
    "# Use the most recent sector classification file that can be loaded\n",
    "sector_data = pd.DataFrame()\n",
    "for file in sector_files:\n",
    "    try:\n",
    "        sector_data = load_sector_data(file, data_dir)\n",
    "        if not sector_data.empty:\n",
    "            print(f\"Using sector data from: {file}\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process sector file {file}: {e}\")\n",
    "\n",
    "# Print data summary\n",
    "if not all_stock_data.empty:\n",
    "    print(f\"\\nStock data loaded successfully: {all_stock_data.shape[0]:,} rows and {all_stock_data.shape[1]} columns\")\n",
    "    print(f\"Date range: {all_stock_data['Date'].min().date()} to {all_stock_data['Date'].max().date()}\")\n",
    "    print(f\"\\nNumber of unique stocks: {all_stock_data['Code'].nunique()}\")\n",
    "    if not sector_data.empty:\n",
    "        print(f\"Number of sectors: {sector_data['Sector'].nunique()}\")\n",
    "        print(f\"Sectors: {', '.join(sector_data['Sector'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ffce7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 184,720 rows of stock data...\n",
      "Removing 70 duplicate entries\n",
      "\n",
      "Missing values in key columns:\n",
      "Date         1\n",
      "Code         1\n",
      "Day Price    2\n",
      "dtype: int64\n",
      "WARNING: Removing rows with missing Date values\n",
      "Missing Day Price values: 0.00% of rows\n",
      "Filling missing Day Price values with forward fill method\n",
      "\n",
      "Merging stock data with sector classifications...\n",
      "Error merging sector data: 'Code'\n"
     ]
    }
   ],
   "source": [
    "# Check if we have data to process\n",
    "if all_stock_data.empty:\n",
    "    print(\"ERROR: No stock data was loaded successfully. Cannot proceed with preprocessing.\")\n",
    "    # Create an empty dataframe with required columns to prevent errors in subsequent code\n",
    "    all_stock_data = pd.DataFrame(columns=['Date', 'Code', 'Name', 'Day Price', 'Volume', 'Year'])\n",
    "else:\n",
    "    print(f\"Processing {all_stock_data.shape[0]:,} rows of stock data...\")\n",
    "    \n",
    "    # Sort data by date\n",
    "    all_stock_data = all_stock_data.sort_values('Date')\n",
    "    \n",
    "    # Drop duplicates if any\n",
    "    duplicate_count = all_stock_data.duplicated(subset=['Date', 'Code']).sum()\n",
    "    if duplicate_count > 0:\n",
    "        print(f\"Removing {duplicate_count} duplicate entries\")\n",
    "        all_stock_data = all_stock_data.drop_duplicates(subset=['Date', 'Code'])\n",
    "    \n",
    "    # Ensure we have a Day Price column (use Close or Adjusted Price if Day Price is missing)\n",
    "    if 'Day Price' not in all_stock_data.columns:\n",
    "        if 'Close' in all_stock_data.columns:\n",
    "            print(\"'Day Price' column not found. Using 'Close' column instead.\")\n",
    "            all_stock_data['Day Price'] = all_stock_data['Close']\n",
    "        elif 'Adjusted Price' in all_stock_data.columns:\n",
    "            print(\"'Day Price' column not found. Using 'Adjusted Price' column instead.\")\n",
    "            all_stock_data['Day Price'] = all_stock_data['Adjusted Price']\n",
    "        else:\n",
    "            print(\"WARNING: No price column found in the data. Analysis will be limited.\")\n",
    "            all_stock_data['Day Price'] = np.nan\n",
    "    \n",
    "    # Check for missing values in key columns\n",
    "    key_columns = ['Date', 'Code']\n",
    "    if 'Day Price' in all_stock_data.columns:\n",
    "        key_columns.append('Day Price')\n",
    "    \n",
    "    missing_data = all_stock_data[key_columns].isnull().sum()\n",
    "    print(\"\\nMissing values in key columns:\")\n",
    "    print(missing_data)\n",
    "    \n",
    "    # Handle missing Date or Code values\n",
    "    if all_stock_data['Date'].isnull().sum() > 0:\n",
    "        print(\"WARNING: Removing rows with missing Date values\")\n",
    "        all_stock_data = all_stock_data.dropna(subset=['Date'])\n",
    "    \n",
    "    if all_stock_data['Code'].isnull().sum() > 0:\n",
    "        print(\"WARNING: Removing rows with missing Code values\")\n",
    "        all_stock_data = all_stock_data.dropna(subset=['Code'])\n",
    "    \n",
    "    # Handle any remaining missing values in price data\n",
    "    if 'Day Price' in all_stock_data.columns and all_stock_data['Day Price'].isnull().sum() > 0:\n",
    "        missing_price_pct = (all_stock_data['Day Price'].isnull().sum() / len(all_stock_data)) * 100\n",
    "        print(f\"Missing Day Price values: {missing_price_pct:.2f}% of rows\")\n",
    "        \n",
    "        if missing_price_pct < 30:  # Only fill if less than 30% are missing\n",
    "            print(\"Filling missing Day Price values with forward fill method\")\n",
    "            # Group by stock code and forward fill missing prices\n",
    "            all_stock_data['Day Price'] = all_stock_data.groupby('Code')['Day Price'].transform(\n",
    "                lambda x: x.fillna(method='ffill')\n",
    "            )\n",
    "            # For any remaining missing values, backward fill\n",
    "            all_stock_data['Day Price'] = all_stock_data.groupby('Code')['Day Price'].transform(\n",
    "                lambda x: x.fillna(method='bfill')\n",
    "            )\n",
    "        else:\n",
    "            print(\"WARNING: Too many missing price values. Data quality issues may affect analysis.\")\n",
    "    \n",
    "    # Create a year column for yearly analysis\n",
    "    try:\n",
    "        all_stock_data['Year'] = all_stock_data['Date'].dt.year\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Year column: {e}\")\n",
    "        # Try to fix Date format if possible\n",
    "        if not all_stock_data.empty:\n",
    "            try:\n",
    "                all_stock_data['Date'] = pd.to_datetime(all_stock_data['Date'], errors='coerce')\n",
    "                all_stock_data = all_stock_data.dropna(subset=['Date'])\n",
    "                all_stock_data['Year'] = all_stock_data['Date'].dt.year\n",
    "                print(\"Fixed date format and created Year column\")\n",
    "            except:\n",
    "                print(\"Could not fix date format. Creating dummy Year column\")\n",
    "                all_stock_data['Year'] = 2023  # Use a default value\n",
    "    \n",
    "    # Merge with sector data to add sector information\n",
    "    if not sector_data.empty:\n",
    "        print(\"\\nMerging stock data with sector classifications...\")\n",
    "        \n",
    "        try:\n",
    "            # Standardize the Code column in both dataframes to ensure proper merging\n",
    "            all_stock_data['Code'] = all_stock_data['Code'].astype(str).str.strip()\n",
    "            sector_data['Code'] = sector_data['Code'].astype(str).str.strip()\n",
    "            \n",
    "            # Merge the datasets\n",
    "            merged_data = all_stock_data.merge(sector_data[['Code', 'Sector']], \n",
    "                                            on='Code', \n",
    "                                            how='left')\n",
    "            \n",
    "            # Check how many stocks have sector information\n",
    "            if 'Sector' in merged_data.columns:\n",
    "                sector_coverage = (merged_data['Sector'].notna().sum() / merged_data.shape[0]) * 100\n",
    "                print(f\"Sector information available for {sector_coverage:.2f}% of stock data\")\n",
    "                \n",
    "                # For those without sector info, try to fill based on company name matches\n",
    "                if 'Name' in merged_data.columns:\n",
    "                    missing_sectors = merged_data[merged_data['Sector'].isna()]['Code'].unique()\n",
    "                    if len(missing_sectors) > 0:\n",
    "                        print(f\"Found {len(missing_sectors)} stocks without sector information\")\n",
    "                        \n",
    "                        # Use the most common sector for each stock code where sector is missing\n",
    "                        sectors_assigned = 0\n",
    "                        for code in missing_sectors:\n",
    "                            if merged_data[merged_data['Code'] == code]['Name'].empty:\n",
    "                                continue\n",
    "                                \n",
    "                            stock_name = merged_data[merged_data['Code'] == code]['Name'].iloc[0]\n",
    "                            if stock_name and isinstance(stock_name, str) and len(stock_name.split()) > 0:\n",
    "                                # Look for similar names in sector data\n",
    "                                try:\n",
    "                                    similar_names = sector_data[sector_data['Name'].str.contains(\n",
    "                                        stock_name.split()[0], case=False, na=False)]\n",
    "                                    if not similar_names.empty:\n",
    "                                        most_common_sector = similar_names['Sector'].mode()[0]\n",
    "                                        merged_data.loc[merged_data['Code'] == code, 'Sector'] = most_common_sector\n",
    "                                        sectors_assigned += 1\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error matching sector for {code}: {e}\")\n",
    "                                    continue\n",
    "                        \n",
    "                        print(f\"Assigned sectors to {sectors_assigned} stocks based on name similarity\")\n",
    "                \n",
    "                # Update the dataframe\n",
    "                all_stock_data = merged_data\n",
    "                \n",
    "                # Calculate coverage again after filling\n",
    "                sector_coverage_after = (all_stock_data['Sector'].notna().sum() / all_stock_data.shape[0]) * 100\n",
    "                print(f\"After filling: Sector information available for {sector_coverage_after:.2f}% of stock data\")\n",
    "            else:\n",
    "                print(\"WARNING: Sector column not found in merged data. Check sector data format.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging sector data: {e}\")\n",
    "            # Keep using original all_stock_data if merge fails\n",
    "    else:\n",
    "        print(\"No sector data available. Proceeding without sector classification.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83fec6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating daily returns...\n",
      "Calculating monthly returns...\n",
      "Monthly returns calculated for 8929 stock-months\n",
      "Calculating yearly returns...\n",
      "Yearly returns calculated for 769 stock-years\n",
      "Calculating volatility...\n",
      "Calculating moving averages...\n",
      "Calculating volume trends...\n",
      "\n",
      "Performance metrics calculated:\n",
      "- Daily returns\n",
      "- Monthly returns\n",
      "- Yearly returns\n",
      "- 30-day volatility\n",
      "- 50-day moving average\n",
      "- 200-day moving average\n",
      "- 20-day volume moving average\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Skip performance metrics if we don't have price data\n",
    "    if 'Day Price' not in all_stock_data.columns or all_stock_data['Day Price'].isnull().all():\n",
    "        print(\"WARNING: Cannot calculate performance metrics without price data\")\n",
    "    else:\n",
    "        # Calculate daily returns\n",
    "        print(\"Calculating daily returns...\")\n",
    "        all_stock_data['Daily_Return'] = all_stock_data.groupby('Code')['Day Price'].pct_change() * 100\n",
    "        \n",
    "        # Calculate monthly returns\n",
    "        print(\"Calculating monthly returns...\")\n",
    "        try:\n",
    "            all_stock_data['Year_Month'] = all_stock_data['Date'].dt.to_period('M')\n",
    "            monthly_returns = all_stock_data.groupby(['Code', 'Year_Month']).apply(\n",
    "                lambda x: (x['Day Price'].iloc[-1] / x['Day Price'].iloc[0] - 1) * 100 if len(x) > 1 and x['Day Price'].iloc[0] != 0 else np.nan\n",
    "            ).reset_index(name='Monthly_Return')\n",
    "            \n",
    "            # Join monthly returns back to main dataset\n",
    "            all_stock_data = all_stock_data.merge(\n",
    "                monthly_returns, \n",
    "                on=['Code', 'Year_Month'], \n",
    "                how='left'\n",
    "            )\n",
    "            print(f\"Monthly returns calculated for {monthly_returns.shape[0]} stock-months\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating monthly returns: {e}\")\n",
    "            all_stock_data['Monthly_Return'] = np.nan\n",
    "        \n",
    "        # Calculate yearly returns\n",
    "        print(\"Calculating yearly returns...\")\n",
    "        try:\n",
    "            yearly_returns = all_stock_data.groupby(['Code', 'Year']).apply(\n",
    "                lambda x: (x['Day Price'].iloc[-1] / x['Day Price'].iloc[0] - 1) * 100 \n",
    "                if len(x) > 1 and x['Day Price'].iloc[0] != 0 and not np.isnan(x['Day Price'].iloc[0]) and not np.isnan(x['Day Price'].iloc[-1])\n",
    "                else np.nan\n",
    "            ).reset_index(name='Yearly_Return')\n",
    "            \n",
    "            # Join yearly returns back to main dataset\n",
    "            all_stock_data = all_stock_data.merge(\n",
    "                yearly_returns,\n",
    "                on=['Code', 'Year'],\n",
    "                how='left'\n",
    "            )\n",
    "            print(f\"Yearly returns calculated for {yearly_returns.shape[0]} stock-years\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating yearly returns: {e}\")\n",
    "            all_stock_data['Yearly_Return'] = np.nan\n",
    "        \n",
    "        # Calculate volatility (standard deviation of daily returns) on a rolling 30-day window\n",
    "        print(\"Calculating volatility...\")\n",
    "        try:\n",
    "            if 'Daily_Return' in all_stock_data.columns:\n",
    "                all_stock_data['Volatility_30d'] = all_stock_data.groupby('Code')['Daily_Return'].transform(\n",
    "                    lambda x: x.rolling(window=30, min_periods=15).std()\n",
    "                )\n",
    "            else:\n",
    "                print(\"WARNING: Cannot calculate volatility without daily returns\")\n",
    "                all_stock_data['Volatility_30d'] = np.nan\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating volatility: {e}\")\n",
    "            all_stock_data['Volatility_30d'] = np.nan\n",
    "        \n",
    "        # Calculate 50-day and 200-day moving averages for technical analysis\n",
    "        print(\"Calculating moving averages...\")\n",
    "        try:\n",
    "            all_stock_data['MA_50'] = all_stock_data.groupby('Code')['Day Price'].transform(\n",
    "                lambda x: x.rolling(window=50, min_periods=25).mean()\n",
    "            )\n",
    "            all_stock_data['MA_200'] = all_stock_data.groupby('Code')['Day Price'].transform(\n",
    "                lambda x: x.rolling(window=200, min_periods=100).mean()\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating moving averages: {e}\")\n",
    "            all_stock_data['MA_50'] = np.nan\n",
    "            all_stock_data['MA_200'] = np.nan\n",
    "        \n",
    "        # Calculate trading volume trends\n",
    "        if 'Volume' in all_stock_data.columns and not all_stock_data['Volume'].isnull().all():\n",
    "            print(\"Calculating volume trends...\")\n",
    "            try:\n",
    "                all_stock_data['Volume_MA_20'] = all_stock_data.groupby('Code')['Volume'].transform(\n",
    "                    lambda x: x.rolling(window=20, min_periods=10).mean()\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating volume trends: {e}\")\n",
    "                all_stock_data['Volume_MA_20'] = np.nan\n",
    "        else:\n",
    "            print(\"Volume data not available. Skipping volume trend calculation.\")\n",
    "        \n",
    "        print(\"\\nPerformance metrics calculated:\")\n",
    "        metrics_list = []\n",
    "        if 'Daily_Return' in all_stock_data.columns and not all_stock_data['Daily_Return'].isnull().all():\n",
    "            metrics_list.append(\"- Daily returns\")\n",
    "        if 'Monthly_Return' in all_stock_data.columns and not all_stock_data['Monthly_Return'].isnull().all():\n",
    "            metrics_list.append(\"- Monthly returns\")\n",
    "        if 'Yearly_Return' in all_stock_data.columns and not all_stock_data['Yearly_Return'].isnull().all():\n",
    "            metrics_list.append(\"- Yearly returns\")\n",
    "        if 'Volatility_30d' in all_stock_data.columns and not all_stock_data['Volatility_30d'].isnull().all():\n",
    "            metrics_list.append(\"- 30-day volatility\")\n",
    "        if 'MA_50' in all_stock_data.columns and not all_stock_data['MA_50'].isnull().all():\n",
    "            metrics_list.append(\"- 50-day moving average\")\n",
    "        if 'MA_200' in all_stock_data.columns and not all_stock_data['MA_200'].isnull().all():\n",
    "            metrics_list.append(\"- 200-day moving average\")\n",
    "        if 'Volume_MA_20' in all_stock_data.columns and not all_stock_data['Volume_MA_20'].isnull().all():\n",
    "            metrics_list.append(\"- 20-day volume moving average\")\n",
    "        \n",
    "        if metrics_list:\n",
    "            for metric in metrics_list:\n",
    "                print(metric)\n",
    "        else:\n",
    "            print(\"WARNING: No metrics were successfully calculated\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"ERROR in performance metrics calculation: {e}\")\n",
    "    print(\"Continuing with limited performance data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3103855d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years with sufficient data: [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2023, 2024]\n"
     ]
    }
   ],
   "source": [
    "# Filter for years with complete data\n",
    "valid_years = all_stock_data['Year'].value_counts()\n",
    "valid_years = valid_years[valid_years > 100].index.tolist()\n",
    "valid_years.sort()\n",
    "print(f\"Years with sufficient data: {valid_years}\")\n",
    "\n",
    "# Only analyze sectors that have meaningful data\n",
    "if 'Sector' in all_stock_data.columns:\n",
    "    # Create a sector returns dataframe with yearly performance\n",
    "    sector_yearly_returns = all_stock_data.dropna(subset=['Sector', 'Yearly_Return'])\n",
    "    sector_yearly_returns = sector_yearly_returns.groupby(['Sector', 'Year'])['Yearly_Return'].mean().reset_index()\n",
    "    \n",
    "    # Create a pivot table for easier visualization\n",
    "    sector_yearly_pivot = sector_yearly_returns.pivot(index='Sector', columns='Year', values='Yearly_Return')\n",
    "    \n",
    "    # Create overall metrics table for sectors\n",
    "    print(\"\\nCalculating sector performance metrics...\")\n",
    "    sector_metrics = pd.DataFrame()\n",
    "    \n",
    "    for year in valid_years:\n",
    "        year_data = all_stock_data[all_stock_data['Year'] == year]\n",
    "        \n",
    "        # Skip years with insufficient sector data\n",
    "        if 'Sector' not in year_data.columns or year_data['Sector'].isna().all():\n",
    "            continue\n",
    "            \n",
    "        year_sector_performance = year_data.groupby('Sector').apply(\n",
    "            lambda x: pd.Series({\n",
    "                f'Return_{year}': x['Yearly_Return'].mean(),\n",
    "                f'Volatility_{year}': x['Daily_Return'].std(),\n",
    "                f'Volume_{year}': x['Volume'].mean() if 'Volume' in x.columns else np.nan,\n",
    "                f'Stocks_{year}': x['Code'].nunique()\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        if sector_metrics.empty:\n",
    "            sector_metrics = year_sector_performance\n",
    "        else:\n",
    "            sector_metrics = sector_metrics.join(year_sector_performance, how='outer')\n",
    "    \n",
    "    # Create a column for average return across all years\n",
    "    return_cols = [col for col in sector_metrics.columns if 'Return_' in col]\n",
    "    if return_cols:\n",
    "        sector_metrics['Avg_Return'] = sector_metrics[return_cols].mean(axis=1)\n",
    "        sector_metrics['Cumulative_Return'] = ((1 + sector_metrics[return_cols]/100).prod(axis=1) - 1) * 100\n",
    "        \n",
    "        # Sort by average return\n",
    "        sector_metrics = sector_metrics.sort_values('Avg_Return', ascending=False)\n",
    "        \n",
    "        print(\"\\nTop-performing sectors (by average yearly return):\")\n",
    "        display_cols = ['Avg_Return', 'Cumulative_Return'] + return_cols\n",
    "        print(sector_metrics[display_cols].head().round(2))\n",
    "        \n",
    "        print(\"\\nWorst-performing sectors (by average yearly return):\")\n",
    "        print(sector_metrics[display_cols].tail().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7939749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yearly sector performance heatmap\n",
    "if 'Sector' in all_stock_data.columns and not sector_yearly_pivot.empty:\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    sns.heatmap(sector_yearly_pivot, annot=True, cmap='RdYlGn', center=0, fmt='.1f')\n",
    "    plt.title('Yearly Sector Performance (% Return)', fontsize=16)\n",
    "    plt.xlabel('Year', fontsize=14)\n",
    "    plt.ylabel('Sector', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sector_yearly_performance_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"Sector yearly performance heatmap created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "952640cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative sector performance line chart\n",
    "if 'Sector' in all_stock_data.columns:\n",
    "    # Calculate cumulative returns for each sector over time\n",
    "    pivot_data = sector_yearly_returns.pivot(index='Year', columns='Sector', values='Yearly_Return')\n",
    "    \n",
    "    # Convert to cumulative returns\n",
    "    cumulative_returns = (1 + pivot_data/100).cumprod() * 100 - 100\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    for column in cumulative_returns.columns:\n",
    "        plt.plot(cumulative_returns.index, cumulative_returns[column], marker='o', linewidth=2, label=column)\n",
    "    \n",
    "    plt.title('Cumulative Sector Performance Over Time', fontsize=16)\n",
    "    plt.xlabel('Year', fontsize=14)\n",
    "    plt.ylabel('Cumulative Return (%)', fontsize=14)\n",
    "    plt.legend(title='Sector', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sector_cumulative_performance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"Sector cumulative performance chart created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1527309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sector volatility vs. return scatter plot\n",
    "if 'Sector' in all_stock_data.columns:\n",
    "    # Create a dataframe with sector average returns and volatility\n",
    "    sector_risk_return = all_stock_data.groupby('Sector').apply(\n",
    "        lambda x: pd.Series({\n",
    "            'Average_Return': x['Daily_Return'].mean(),\n",
    "            'Volatility': x['Daily_Return'].std(),\n",
    "            'Sharpe': x['Daily_Return'].mean() / x['Daily_Return'].std() if x['Daily_Return'].std() > 0 else 0,\n",
    "            'Stock_Count': x['Code'].nunique()\n",
    "        })\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Create bubble chart\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(\n",
    "        data=sector_risk_return,\n",
    "        x='Volatility',\n",
    "        y='Average_Return',\n",
    "        size='Stock_Count',\n",
    "        sizes=(50, 500),\n",
    "        alpha=0.7,\n",
    "        hue='Sector'\n",
    "    )\n",
    "    \n",
    "    # Add labels to each point\n",
    "    for i, row in sector_risk_return.iterrows():\n",
    "        plt.annotate(\n",
    "            row['Sector'],\n",
    "            (row['Volatility'], row['Average_Return']),\n",
    "            fontsize=9,\n",
    "            alpha=0.8\n",
    "        )\n",
    "    \n",
    "    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.title('Risk vs. Return by Sector', fontsize=16)\n",
    "    plt.xlabel('Volatility (Risk)', fontsize=14)\n",
    "    plt.ylabel('Average Daily Return (%)', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sector_risk_return.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"Sector risk vs. return chart created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cde4431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-performing stocks within sectors\n",
    "if 'Sector' in all_stock_data.columns:\n",
    "    # Identify most recent year with good data\n",
    "    latest_year = max(valid_years)\n",
    "    latest_data = all_stock_data[all_stock_data['Year'] == latest_year]\n",
    "    \n",
    "    # Calculate average yearly return for each stock in the most recent year\n",
    "    stock_performance = latest_data.groupby(['Sector', 'Code', 'Name'])['Yearly_Return'].mean().reset_index()\n",
    "    \n",
    "    # Get top 3 stocks from each sector\n",
    "    top_stocks_by_sector = stock_performance.groupby('Sector').apply(\n",
    "        lambda x: x.nlargest(3, 'Yearly_Return')\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    g = sns.barplot(\n",
    "        data=top_stocks_by_sector,\n",
    "        y='Code',\n",
    "        x='Yearly_Return',\n",
    "        hue='Sector',\n",
    "        dodge=False\n",
    "    )\n",
    "    \n",
    "    # Enhance the plot\n",
    "    plt.title(f'Top-Performing Stocks by Sector ({latest_year})', fontsize=16)\n",
    "    plt.xlabel('Yearly Return (%)', fontsize=14)\n",
    "    plt.ylabel('Stock Code', fontsize=14)\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.legend(title='Sector', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('top_stocks_by_sector.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"Top-performing stocks by sector chart created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b83d06c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Sharpe Ratio (Return/Risk ratio) for all stocks\n",
    "all_stock_data['Sharpe_Ratio'] = all_stock_data.groupby('Code')['Daily_Return'].transform(\n",
    "    lambda x: x.mean() / x.std() if x.std() > 0 else 0\n",
    ")\n",
    "\n",
    "# Calculate sector momentum (last 6 months performance)\n",
    "if 'Sector' in all_stock_data.columns:\n",
    "    # Get the last 6 months of data\n",
    "    latest_date = all_stock_data['Date'].max()\n",
    "    six_months_ago = latest_date - pd.DateOffset(months=6)\n",
    "    last_6m_data = all_stock_data[all_stock_data['Date'] >= six_months_ago]\n",
    "    \n",
    "    # Calculate sector momentum\n",
    "    sector_momentum = last_6m_data.groupby('Sector').apply(\n",
    "        lambda x: pd.Series({\n",
    "            'Return_6m': ((1 + x.groupby('Code')['Daily_Return'].mean()/100).prod() - 1) * 100,\n",
    "            'Stock_Count': x['Code'].nunique(),\n",
    "            'Avg_Volume': x['Volume'].mean() if 'Volume' in x.columns else np.nan\n",
    "        })\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Sort by 6-month return\n",
    "    sector_momentum = sector_momentum.sort_values('Return_6m', ascending=False)\n",
    "    \n",
    "    print(\"\\nSector momentum (last 6 months):\")\n",
    "    print(sector_momentum.head().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8683e19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 35 stocks with golden cross (50-day MA > 200-day MA)\n"
     ]
    }
   ],
   "source": [
    "latest_year_data = all_stock_data[all_stock_data['Year'] == max(valid_years)]\n",
    "latest_date = latest_year_data['Date'].max()\n",
    "latest_data = latest_year_data[latest_year_data['Date'] == latest_date]\n",
    "\n",
    "# Find stocks where 50-day MA > 200-day MA (golden cross)\n",
    "if 'MA_50' in latest_data.columns and 'MA_200' in latest_data.columns:\n",
    "    golden_cross = latest_data[\n",
    "        (latest_data['MA_50'] > latest_data['MA_200']) &\n",
    "        (latest_data['MA_50'].notna()) &\n",
    "        (latest_data['MA_200'].notna())\n",
    "    ]\n",
    "    \n",
    "    if not golden_cross.empty:\n",
    "        golden_cross = golden_cross.sort_values('Sharpe_Ratio', ascending=False)\n",
    "        print(f\"\\nFound {len(golden_cross)} stocks with golden cross (50-day MA > 200-day MA)\")\n",
    "        if 'Sector' in golden_cross.columns:\n",
    "            golden_cross_by_sector = golden_cross.groupby('Sector').size().reset_index(name='Count')\n",
    "            golden_cross_by_sector = golden_cross_by_sector.sort_values('Count', ascending=False)\n",
    "            print(\"\\nGolden cross stocks by sector:\")\n",
    "            print(golden_cross_by_sector.head())\n",
    "\n",
    "# 6.4 Portfolio Construction - Balance risk/return across sectors\n",
    "if 'Sector' in all_stock_data.columns:\n",
    "    # Combine sector performance with risk metrics for portfolio allocation\n",
    "    portfolio_allocation = sector_risk_return.copy()\n",
    "    \n",
    "    # Calculate a combined score (higher is better)\n",
    "    portfolio_allocation['Score'] = (\n",
    "        portfolio_allocation['Average_Return'] / portfolio_allocation['Average_Return'].max() * 0.5 +\n",
    "        (1 / portfolio_allocation['Volatility']) / (1 / portfolio_allocation['Volatility']).max() * 0.5\n",
    "    )\n",
    "    \n",
    "    # Normalize to get allocation percentages (higher score gets higher allocation)\n",
    "    portfolio_allocation['Allocation'] = portfolio_allocation['Score'] / portfolio_allocation['Score'].sum() * 100\n",
    "    \n",
    "    # Sort by allocation\n",
    "    portfolio_allocation = portfolio_allocation.sort_values('Allocation', ascending=False)\n",
    "    \n",
    "    print(\"\\nRecommended sector allocation for diversified portfolio:\")\n",
    "    print(portfolio_allocation[['Sector', 'Average_Return', 'Volatility', 'Allocation']].head().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e928ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Sector' in all_stock_data.columns and not sector_yearly_pivot.empty:\n",
    "    sector_summary = sector_yearly_pivot.copy()\n",
    "    \n",
    "    # Add average and latest year performance\n",
    "    sector_summary['Average_Return'] = sector_summary.mean(axis=1)\n",
    "    sector_summary = sector_summary.sort_values('Average_Return', ascending=False)\n",
    "    \n",
    "    try:\n",
    "        sector_summary.to_csv('sector_performance_summary.csv')\n",
    "        print(\"Exported sector performance summary to sector_performance_summary.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not export sector summary: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "681ca977",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Sector' in all_stock_data.columns and not portfolio_allocation.empty:\n",
    "    try:\n",
    "        portfolio_allocation.to_csv('recommended_portfolio_allocation.csv', index=False)\n",
    "        print(\"Exported portfolio allocation to recommended_portfolio_allocation.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not export portfolio allocation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f34a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Sector' in all_stock_data.columns and not sector_yearly_pivot.empty:\n",
    "    try:\n",
    "        sector_heatmap = px.imshow(\n",
    "            sector_yearly_pivot,\n",
    "            labels=dict(x=\"Year\", y=\"Sector\", color=\"Return (%)\"),\n",
    "            x=sector_yearly_pivot.columns,\n",
    "            y=sector_yearly_pivot.index,\n",
    "            color_continuous_scale='RdYlGn',\n",
    "            title='Sector Performance by Year (%)',\n",
    "            text_auto='.1f'\n",
    "        )\n",
    "        sector_heatmap.update_layout(width=1000, height=800)\n",
    "        sector_heatmap.write_html('interactive_sector_heatmap.html')\n",
    "        print(\"Created interactive sector heatmap visualization\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create interactive heatmap: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1de34e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Sector' in all_stock_data.columns and not cumulative_returns.empty:\n",
    "    try:\n",
    "        sector_line = px.line(\n",
    "            cumulative_returns,\n",
    "            x=cumulative_returns.index,\n",
    "            y=cumulative_returns.columns,\n",
    "            title='Cumulative Sector Performance Over Time',\n",
    "            labels={'value': 'Cumulative Return (%)', 'variable': 'Sector'}\n",
    "        )\n",
    "        sector_line.update_traces(mode='lines+markers')\n",
    "        sector_line.update_layout(width=1000, height=600)\n",
    "        sector_line.write_html('interactive_sector_performance.html')\n",
    "        print(\"Created interactive sector performance line chart\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create interactive line chart: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fa1dacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Key Investment Insights:\n",
      "\n",
      "Saved investment insights to investment_insights.txt for PesaGuru chatbot integration\n",
      "\n",
      "Analysis complete! All visualizations and data exported successfully.\n"
     ]
    }
   ],
   "source": [
    "insights = []\n",
    "\n",
    "if 'Sector' in all_stock_data.columns:\n",
    "    # Top performing sectors\n",
    "    if not sector_metrics.empty:\n",
    "        top_sector = sector_metrics.iloc[0].name\n",
    "        top_return = sector_metrics['Avg_Return'].iloc[0]\n",
    "        insights.append(f\"The {top_sector} sector has been the best performing with an average yearly return of {top_return:.2f}%\")\n",
    "    \n",
    "    # Sector momentum\n",
    "    if not sector_momentum.empty:\n",
    "        momentum_sector = sector_momentum['Sector'].iloc[0]\n",
    "        momentum_return = sector_momentum['Return_6m'].iloc[0]\n",
    "        insights.append(f\"The {momentum_sector} sector shows the strongest momentum with a {momentum_return:.2f}% return in the last 6 months\")\n",
    "    \n",
    "    # Risk-adjusted performance\n",
    "    if not sector_risk_return.empty:\n",
    "        best_sharpe_idx = sector_risk_return['Sharpe'].idxmax()\n",
    "        best_sharpe_sector = sector_risk_return.loc[best_sharpe_idx, 'Sector']\n",
    "        insights.append(f\"The {best_sharpe_sector} sector offers the best risk-adjusted returns (highest Sharpe ratio)\")\n",
    "    \n",
    "    # Portfolio diversification\n",
    "    if len(portfolio_allocation) > 0:\n",
    "        top_allocation_sectors = portfolio_allocation.head(3)['Sector'].tolist()\n",
    "        insights.append(f\"For optimal portfolio diversification, consider allocating to these sectors: {', '.join(top_allocation_sectors)}\")\n",
    "    \n",
    "    # Technical indicators\n",
    "    if 'golden_cross_by_sector' in locals() and len(golden_cross_by_sector) > 0:\n",
    "        tech_sector = golden_cross_by_sector['Sector'].iloc[0]\n",
    "        tech_count = golden_cross_by_sector['Count'].iloc[0]\n",
    "        insights.append(f\"The {tech_sector} sector has the most stocks ({tech_count}) showing positive technical indicators (golden cross)\")\n",
    "\n",
    "# Print insights\n",
    "print(\"\\nKey Investment Insights:\")\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"{i}. {insight}\")\n",
    "\n",
    "# Save insights to file for PesaGuru chatbot\n",
    "with open('investment_insights.txt', 'w') as f:\n",
    "    f.write(\"NSE SECTOR PERFORMANCE ANALYSIS - KEY INSIGHTS\\n\")\n",
    "    f.write(\"===============================================\\n\\n\")\n",
    "    for i, insight in enumerate(insights, 1):\n",
    "        f.write(f\"{i}. {insight}\\n\")\n",
    "\n",
    "print(\"\\nSaved investment insights to investment_insights.txt for PesaGuru chatbot integration\")\n",
    "print(\"\\nAnalysis complete! All visualizations and data exported successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pesaguru",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
